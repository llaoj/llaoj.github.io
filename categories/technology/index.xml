<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>technology on 老J的博客</title><link>https://blog.llaoj.cn/categories/technology/</link><description>Recent content in technology on 老J的博客</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Tue, 30 Aug 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.llaoj.cn/categories/technology/index.xml" rel="self" type="application/rss+xml"/><item><title>使用rsync在主机之间同步目录</title><link>https://blog.llaoj.cn/posts/2208/rsync-usage/</link><pubDate>Tue, 30 Aug 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2208/rsync-usage/</guid><description>rsync安装 # 在传输双方的服务器上都安装rsync软件. 如果服务器上有rsync可以跳过.
先检查有没有安装rsync:
rsync -v 如果没有安装, 使用下面的命令安装:
# Debian sudo apt-get install rsync # Red Hat sudo yum install rsync # Arch Linux sudo pacman -S rsync 启动rsync守护进程 # rsync使用最多的是ssh模式. 在现代的公司中, 出于安全的原因, 很多ssh是被禁止使用的. 所以, 我们可以使用rsync的守护进程模式. 一起看看怎么用吧.
rsync守护进程部署在传输双方(发送方或者接受方)的任何一端都可以的.
下面的配置和命令中, 我以发送方(10.138.228.201)和接收方(10.206.38.30)为例.
我选择接收方, 先部署配置文件. 配置文件地址: /etc/rsyncd.conf. 配置文件 官方参考手册
以下是一个参考的配置, 每一项配置我都增加了备注说明:
# 指定rsync以什么用户/组传输文件 # 默认nobody,如果使用了系统不存在的用户和组 # 需要先手动创建用户和组 # 它会是生成的文件所属的用户和组 # 也可以把它们配置到模块中 uid = root gid = root # 选择yes可以在操作模块时chroot到同步目录中 # 优势是面对安全威胁能提供额外保护 # 缺点是使用chroot需要root权限, # 以及在传输符号连接或保存用户名/组时会有些问题 use chroot = no # 指定监听端口 # 默认873 port = 873 # 最大连接数 max connections = 200 # 超时时间 timeout = 600 # 进程pid所在的文件 pid file = /var/run/rsyncd.</description></item><item><title>使用etcdctl查看kubernetes存储的内容</title><link>https://blog.llaoj.cn/posts/2208/kubernetes-etcdctl-usage/</link><pubDate>Mon, 29 Aug 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2208/kubernetes-etcdctl-usage/</guid><description>下面这个脚本提供了etcdctl连接etcd所需要的断点、证书相关的信息, 能快速或许并调用命令查看, 这个脚本需要在master节点上执行:
#!/bin/bash ENDPOINTS=$(ps -ef | grep kube-apiserver | grep -P &amp;#39;etcd-servers=(.*?)\s&amp;#39; -o | awk -F= &amp;#39;{print $2}&amp;#39;) CACERT=$(ps -ef | grep kube-apiserver | grep -P &amp;#39;etcd-cafile=(.*?)\s&amp;#39; -o | awk -F= &amp;#39;{print $2}&amp;#39;) CERT=$(ps -ef | grep kube-apiserver | grep -P &amp;#39;etcd-certfile=(.*?)\s&amp;#39; -o | awk -F= &amp;#39;{print $2}&amp;#39;) KEY=$(ps -ef | grep kube-apiserver | grep -P &amp;#39;etcd-keyfile=(.*?)\s&amp;#39; -o | awk -F= &amp;#39;{print $2}&amp;#39;) alias etcdctl=&amp;#39;ETCDCTL_API=3 etcdctl --endpoints=${ENDPOINTS} --cacert=${CACERT} --key=${KEY} --cert=${CERT} -w=json&amp;#39; 好了下面可以直接使用etcdctl命令了, 比如:</description></item><item><title>MetalLB二层模式使用指南</title><link>https://blog.llaoj.cn/posts/2208/metallb-l2-usage/</link><pubDate>Mon, 08 Aug 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2208/metallb-l2-usage/</guid><description>MetalLB概念安装配置和使用请查看
测试组件的版本情况 # kubernetes: v1.22.8 metellb: v0.10.3 nginx: latest 创建测试应用 # 创建一个nginx服务和service资源:
kubectl -n without-istio create deploy nginx --image=nginx 测试分配IP # 创建loadbalancer类型的service:
kubectl -n without-istio create service loadbalancer nginx --tcp=80:80 查看该service详细配置:
apiVersion: v1 kind: Service metadata: labels: app: nginx namespace: without-istio name: nginx ... spec: allocateLoadBalancerNodePorts: true clusterIP: 10.233.15.89 clusterIPs: - 10.233.15.89 externalTrafficPolicy: Cluster internalTrafficPolicy: Cluster ipFamilies: - IPv4 ipFamilyPolicy: SingleStack ports: - name: 80-80 nodePort: 30662 port: 80 protocol: TCP targetPort: 80 selector: app: nginx sessionAffinity: None type: LoadBalancer status: loadBalancer: ingress: - ip: 10.</description></item><item><title>在istio service mesh中使用nginx反向代理</title><link>https://blog.llaoj.cn/posts/2208/istio-with-nginx-reserve-proxy/</link><pubDate>Mon, 08 Aug 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2208/istio-with-nginx-reserve-proxy/</guid><description>nginx反向代理的请求, 和我们直接请求有一定的区别, 比如:
http version # nginx proxy 发出的反向代理请求的http version默认是: 1.0, 但是istio支持1.1 &amp;amp; 2.0, 所以如果不增加http版本限制的话istio就无法进行报文解析, 也就无法应用istio-proxy(sidecar)L7层代理策略, 我们知道istio流量治理是基于L7层的.
http header: Host # 有时候nginx发出的代理请求的http header中host的值, 不能保证是上游服务的host name. 在这种情况下, 是没办法匹配上游服务在istio-proxy中的L7流量治理的配置.
怎么解决? # 所以, 需要在nginx代理配置处增加两项配置:
... location / { proxy_http_version 1.1; &amp;lt;- proxy_set_header Host &amp;lt;upstream-host&amp;gt;; &amp;lt;- proxy_pass http://&amp;lt;upstream-host&amp;gt;:8080; } ... 即可.
参考 # nginx官方文档proxy_http_version介绍 nginx官方文档proxy_set_header介绍</description></item><item><title>Fluentd配置文件最佳实践</title><link>https://blog.llaoj.cn/posts/2207/fluentd-es-config/</link><pubDate>Sun, 31 Jul 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2207/fluentd-es-config/</guid><description>Fluentd负责Kubernetes中容器日志的收集工作, 以Daemonset形式运行在每一个节点上. 下面这个配置是在多个生产集群使用的配置, 经过多次调优的. 有一些关键的配置增加了配置解释说明. 目前使用问题不大. 持续更新配置中&amp;hellip;</description></item><item><title>Kubernetes 服务器配置和规划建设要求</title><link>https://blog.llaoj.cn/posts/2207/kubernetes-requirement/</link><pubDate>Sat, 16 Jul 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2207/kubernetes-requirement/</guid><description>新建集群的第一步就是要规划服务器、网络、操作系统等等, 下面就结合我平时的工作经验总结下相关的要求, 内容根据日常工作持续补充完善:
服务器配置 # kubernetes 集群分为控制节点和数据节点, 它们对于配置的要求有所不同:
控制面 # 节点规模 Master规格 1~5个节点 4核 8Gi（不建议2核 4Gi） 6~20个节点 4核 16Gi 21~100个节点 8核 32Gi 100~200个节点 16核 64Gi 系统盘40+Gi，用于储存 etcd 信息及相关配置文件等
数据面 # 规格：CPU &amp;gt;= 4核, 内存 &amp;gt;= 8Gi 确定整个集群的日常使用的总核数以及可用度的容忍度 例如：集群总的核数有160核, 可以容忍10%的错误. 那么最小选择10台16核VM, 并且高峰运行的负荷不要超过 160*90%=144核. 如果容忍度是20%, 那么最小选择5台32核VM, 并且高峰运行的负荷不要超过160*80%=128核. 这样就算有一台VM出现故障, 剩余VM仍可以支持现有业务正常运行. 确定 CPU:Memory 比例. 对于使用内存比较多的应用, 例如Java类应用, 建议考虑使用1:8的机型 比如: virtual machine 32C 64G 200G系统盘 数据盘可选 什么情况下使用裸金属服务器? # 集群日常规模能够达到1000核。一台服务器至少96核，这样可以通过10台或11台服务器即可构建一个集群。 快速扩大较多容器。例如：电商类大促，为应对流量尖峰，可以考虑使用裸金属来作为新增节点，这样增加一台裸金属服务器就可以支持很多个容器运行。 操作系统 # 建议安装 ubuntu 18.04/debian buster/ubuntu 20.</description></item><item><title>MetalLB概念安装配置和使用</title><link>https://blog.llaoj.cn/posts/2205/metalb/</link><pubDate>Mon, 30 May 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2205/metalb/</guid><description>官方文档
为什么使用? # Kubernetes没有提供适用于裸金属集群的网络负载均衡器实现, 也就是LoadBalancer类型的Service. Kubernetes 附带的网络负载均衡器的实现都是调用各种 IaaS 平台（GCP、AWS、Azure ……）的胶水代码。 如果您没有在受支持的 IaaS 平台（GCP、AWS、Azure&amp;hellip;）上运行，LoadBalancers 在创建时将一直保持在pending状态。
裸金属集群的运维人员只剩下两个方式来将用户流量引入集群内: NodePort和externalIPs. 这两种在生产环境使用有很大的缺点, 这样, 裸金属集群也就成了 Kubernetes 生态中的第二类选择, 并不是首选.
MetalLB 的目的是实现一个网络负载均衡器来与标准的网络设备集成, 这样这些外部服务就能尽可能的正常工作了.
要求 # MetalLB 要求如下:
一个 Kubernetes 集群, Kubernetes 版本 1.13.0+, 没有网络负载均衡器功能. 可以与 MetalLB 共存的集群网络配置。 一些供 MetalLB 分发的 IPv4 地址。 当使用 BGP 操作模式时，您将需要一台或多台能够发布 BGP 的路由器。 使用 L2 操作模式时，节点之间必须允许 7946 端口（TCP 和 UDP，可配置其他端口）上的流量，这是 hashicorp/memberlist 的要求。 功能 # MetalLB 是作为 Kubernetes 中的一个组件, 提供了一个网络负载均衡器的实现. 简单来说, 在非公有云环境搭建的集群上, 不能使用公有云的负载均衡器, 它可以让你在集群中创建 LoadBalancer 类型的 Service.</description></item><item><title>Linux 控制组(cgroups)和进程隔离</title><link>https://blog.llaoj.cn/posts/2205/cgroups-process-isolation/</link><pubDate>Sun, 15 May 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2205/cgroups-process-isolation/</guid><description>控制组(cgroups)是内核的一个特性，它能限制/统计/隔离一个或者多个进程使用CPU、内存、磁盘I/O和网络。cgroups技术最开始是Google开发，最终在2.6.24版本的内核中出现。3.15和3.16版本内核将合并进重新设计的cgroups，它添加来kernfs(拆分一些sysfs逻辑)。cgroups的主要设计目标是提供一个统一的接口，它可以管理进程或者整个操作系统级别的虚拟化，包含Linux容器，或者LXC。</description></item><item><title>分析告警 kubernetes 节点 load 过高问题</title><link>https://blog.llaoj.cn/posts/2204/kubernetes-node-load/</link><pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2204/kubernetes-node-load/</guid><description>负载过高分析 # 通过 linux 提供的几个命令可以从不同的纬度分析系统负载。
vmstat # 这命令能从一个系统的角度反应出服务器情况，报告虚拟内存统计信息，报告有关进程、内存、分页、块的信息 IO、陷阱、磁盘和 CPU 活动。看个例子：
$ vmstat --wide --unit M 5 procs ----------------memory---------------- ---swap--- -----io---- ---system--- ---------cpu-------- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 1 0 127691 1535 73572 0 0 0 3 0 0 2 1 97 0 0 93 0 0 127674 1535 73573 0 0 0 80 49267 67634 5 1 94 1 0 0 2 0 127679 1535 73573 0 0 0 66 38537 56283 3 1 95 1 0 2 2 0 127738 1535 73574 0 0 6 86 41769 61823 5 1 93 2 0 2 0 0 127729 1535 73574 0 0 18 18 41002 59214 4 1 95 0 0 命令以及输出解释：</description></item><item><title>Apisxi Ingress Controller 设计说明</title><link>https://blog.llaoj.cn/posts/2204/apisix-ingress-controller-design/</link><pubDate>Thu, 14 Apr 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2204/apisix-ingress-controller-design/</guid><description>apisix-ingress-controller 要求 kubernetes 版本 1.16+. 因为使用了 CustomResourceDefinition v1 stable 版本的 API. 从 1.0.0 版本开始，APISIX-ingress-controller 要求 Apache APISIX 版本 2.7+.</description></item><item><title>Grafana Mimir 发布 目前最具扩展性的开源时序数据库</title><link>https://blog.llaoj.cn/posts/2204/announcing-grafana-mimir/</link><pubDate>Fri, 01 Apr 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2204/announcing-grafana-mimir/</guid><description>Grafana Mimir 是目前最具扩展性、性能最好的开源时序数据库，Mimir 允许你将指标扩展到 1 亿。它部署简单、高可用、多租户支持、持久存储、查询性能超高，比 Cortex 快 40 倍。 Mimir 托管在
&lt;a href="https://github.com/grafana/mimir">https://github.com/grafana/mimir&lt;/a> 并在 AGPLv3 下获得许可。</description></item><item><title>在 kubernetes 中找出使用 jdk9 及以上版本的应用</title><link>https://blog.llaoj.cn/posts/2203/execjava-versioninpod/</link><pubDate>Wed, 30 Mar 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2203/execjava-versioninpod/</guid><description>近日, Spring Cloud (SPEL) 中发现 RCE 0-day 漏洞, 为了排查 kubernetes 中所有存在安全威胁的应用. 特地开发了一个小工具来寻找。该工具基于 golang&amp;amp;client-go 开发, 程序会找出当前集群中所有 Running 的 pods, 然后逐个进入容器，执行 &lt;code>java -version&lt;/code> 命令，将命令输出打印到文件中，使用编辑器进行查找检索即可。</description></item><item><title>Harbor 双主复制解决方案实践</title><link>https://blog.llaoj.cn/posts/2203/harbor-dual-master-replication-ha/</link><pubDate>Tue, 29 Mar 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2203/harbor-dual-master-replication-ha/</guid><description>既然使用了外部的服务, 那么高可用的压力自然而然的转移到了外部服务上. 我们一开始采用的外部的 NFS 共享存储服务, 由于我们团队实际情况, 我们暂时还不能保证外部存储的高可用. 同时, 鉴于我们对镜像服务高可用的迫切需求, 决定调研新的 Harbor 的高可用方案.</description></item><item><title>在 kubernetes 中找出过度使用资源的 namespaces</title><link>https://blog.llaoj.cn/posts/2203/find-ns-that-exceed-resource-limits/</link><pubDate>Mon, 28 Mar 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2203/find-ns-that-exceed-resource-limits/</guid><description>我们知道, 在 kubernetes 中, namespace 的资源限制在 ResourceQuota 中定义, 比如我们控制 default 名称空间使用 1核1G 的资源. 通常来讲, 由于 kubernetes 的资源控制机制, &lt;code>.status.used&lt;/code> 中资源的值会小于 &lt;code>.status.hard&lt;/code> 中相应资源的值. 但是也有特例. 当我们开始定义了一个较大的资源限制, 待应用部署完毕, 资源占用了很多之后, 这时调低资源限制. 此时就会出现 &lt;code>.status.used&lt;/code> 中的值超过 &lt;code>.status.hard&lt;/code> 中相应值的情况, 尤其是内存的限制.</description></item><item><title>[译]什么是 eBPF?</title><link>https://blog.llaoj.cn/posts/2203/what-is-ebpf/</link><pubDate>Wed, 23 Mar 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2203/what-is-ebpf/</guid><description>eBPF 程序是事件驱动的, 能在内核或应用程序执行到一个特定的 hook 点时执行. 预定义的 hooks 包含系统调用, 函数出/入口, 内核追踪点, 网络事件等等. 如果预定义 hook 不能满足需求, 也可以创建内核探针(kprobe)或者用户探针(uprobe), 在内核/用户应用程序的任何位置, 把探针附加到 eBPF 程序上.</description></item><item><title>比较冷门但有用的 kubectl 命令</title><link>https://blog.llaoj.cn/posts/2203/kubectl-usefull-command/</link><pubDate>Tue, 22 Mar 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2203/kubectl-usefull-command/</guid><description>以下冷门命令能实现某种具体的功能, 都是在实际工作中摸索总结的经验, 获取到相关的资源名称之后, 就可以配合常用的 kubectl 命令获取其他详细信息.</description></item><item><title>[解决] FailedScheduling pod/&lt;pod-name> pod is &lt;uid> in the cache so can't be assumed</title><link>https://blog.llaoj.cn/posts/2203/pod-cannot-be-assumed/</link><pubDate>Mon, 21 Mar 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2203/pod-cannot-be-assumed/</guid><description>pod is in the cache, so can&amp;rsquo;t be assumed, 这是调度器 scheduler 缓存失效导致的异常事件, 大致原因是 pod 已经调度, 并绑定到指定节点, 由于该节点异常导致启动失败, 重新启动 prometheus statefulset, 让集群重新调度, 其实就是将现有到 prometheus pod 副本数将至 0, 再恢复正常即可.</description></item><item><title>[解决] Warning pod/calico-node-&lt;hash> Readiness probe failed</title><link>https://blog.llaoj.cn/posts/2203/calico-node-readiness-probe-failed/</link><pubDate>Mon, 21 Mar 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2203/calico-node-readiness-probe-failed/</guid><description>calico-node-4fpgp Readiness probe failed, orphaned pod &lt;!-- raw HTML omitted --> found, but volume paths are still present on disk : There were a total of N errors similar to this. Turn up verbosity to see them.</description></item><item><title>bcc 之 opensnoop 工具的使用</title><link>https://blog.llaoj.cn/posts/2203/bcc-opensnoop-usage/</link><pubDate>Wed, 16 Mar 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2203/bcc-opensnoop-usage/</guid><description>这篇文档主要演示了 opensnoop(Linux eBPF/bcc) 工具的使用. opensnoop 在系统范围内跟踪 open() 系统调用，并打印各种详细信息.</description></item><item><title>bcc 之 tcplife 工具的使用</title><link>https://blog.llaoj.cn/posts/2203/bcc-tcplife-usage/</link><pubDate>Wed, 16 Mar 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2203/bcc-tcplife-usage/</guid><description>这篇文档主要演示了 tcplife(Linux eBPF/bcc) 工具的使用. tcplife 总结了在跟踪期间打开和关闭的 TCP 会话. 比如</description></item><item><title>Linux 常见错误码</title><link>https://blog.llaoj.cn/posts/2203/linux-error-code/</link><pubDate>Tue, 15 Mar 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2203/linux-error-code/</guid><description>下表是 Linux 操作系统一些常见的错误代码和对应的错误描述 1 EPERM Operation not permitted 2 ENOENT No such file or directory</description></item><item><title>load average 过高, mount nfs 问题处理</title><link>https://blog.llaoj.cn/posts/2203/nfs-options/</link><pubDate>Mon, 14 Mar 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2203/nfs-options/</guid><description>周末, 有一台服务器告警: 系统负载过高, 最高的时候都已经到 100 +, 以下是排查&amp;amp;处理的具体过程.
发现的问题/现象 # uptime 显示 load average 都在70+ # 因为服务器是40核心, 原则上负载40是满负荷, 现在明显存在大量等待的任务. 继续往下分析进程, 看具体那个进程一直在堵塞.
ps -ef 执行到某一个进程就卡住了 # 命令执行如下:
$ ps -ef ... root 40004 2912 0 Mar08 ? 00:00:33 containerd-shim -namespace moby -workdir /data/docker/containerd/daemon/ io.containerd.runtime.v1.linux/moby/&amp;lt;container-hash&amp;gt; 卡住了 根据命令中的 找到对应的 pod, 将其从当前节点移除. 移除之后, ps 命令以及其他系统命令可以成功执行. 被移除的 pod 分别是: 2个 prometheus、 1个 mysql.
无法执行 umount 卸载 # 测试 mount 挂载正常, 但是 umount 失败, 解决办法:
先强制 umount $ umount -f -l /mount-point # 命令解释 $ umount [options] &amp;lt;source&amp;gt; | &amp;lt;directory&amp;gt; Options: -f Force unmount (in case of an unreachable NFS system).</description></item><item><title>容器化部署 openldap</title><link>https://blog.llaoj.cn/posts/2203/openldap-docker/</link><pubDate>Thu, 03 Mar 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2203/openldap-docker/</guid><description>使用容器化安装非常便捷, 参考 osixia/openldap仓库使用说明安装即可, 如下:
docker stop openldap &amp;amp;&amp;amp; docker rm openldap &amp;amp;&amp;amp; \ docker run --name openldap --detach \ -p 389:389 \ -p 636:636 \ --env LDAP_ORGANISATION=&amp;#34;Rutron Net&amp;#34; \ --env LDAP_DOMAIN=&amp;#34;rutron.net&amp;#34; \ --env LDAP_ADMIN_PASSWORD=&amp;#34;your-password&amp;#34; \ --env LDAP_READONLY_USER=true \ --env LDAP_TLS_VERIFY_CLIENT=try \ --volume /data/openldap/data:/var/lib/ldap \ --volume /data/openldap/slapd.d:/etc/ldap/slapd.d \ --hostname ldap.rutron.net \ osixia/openldap:1.5.0 好了, 现在该服务同时支持 ldap 和 ldaps 协议, 有一个初始化的账号 readonly/readonly, 可以使用了~</description></item><item><title>解决 kubelet cannot allocate memory 错误</title><link>https://blog.llaoj.cn/posts/2202/cannot-allocate-memory/</link><pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2202/cannot-allocate-memory/</guid><description>问题描述 # 查看 pod 相关 events 如下：
Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 18m default-scheduler Successfully assigned container-186002196200947712/itms-5f6d7798-wrpjj to 10.206.65.144 Warning FailedCreatePodContainer 3m31s (x71 over 18m) kubelet unable to ensure pod container exists: failed to create container for [kubepods burstable pod31f4c93c-c3a1-49ad-b091-0802c5f1d396] : mkdir /sys/fs/cgroup/memory/kubepods/burstable/pod31f4c93c-c3a1-49ad-b091-0802c5f1d396: cannot allocate memory 这是内核bug，建议升级内核</description></item><item><title>使用Kubespray安装kubernetes的教程</title><link>https://blog.llaoj.cn/posts/2202/Kubespray-kubernetes-setup/</link><pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2202/Kubespray-kubernetes-setup/</guid><description>本文使用 kubespray 容器部署 kubernetes v1.22, 提供了从国外搬运的离线软件包/容器镜像. 仅需要几步即可部署高可用集群. 所有离线文件都来自官方下载 kubespray 安装过程会进行软件包验证, 放心使用.
前提 # 禁用防火墙 重要: 本文使用 kubespray 的容器环境部署, 为避免影响节点部署(特别是 Runtime 部署), 所以需要一台独立于集群外的服务器执行下面的命令, 这台服务器安装 docker 19.03+ 并到所有节点SSH免密进入. 目标服务器要允许 IPv4 转发, 如果要给 pods 和 services 用 IPv6, 目标服务器要允许 IPv6 转发. 注意: 下面配置是适合 kubespray 的配置, 实际配置取决于集群规模.
Master Memory: 1500 MB Node Memory: 1024 MB 文件和镜像搬运 # 由于国内网络的限制, 直接使用Kubespray是无法成功安装集群的. 所以我写了一个脚本, 将文件上传至阿里云OSS, 镜像上传至阿里云ACR. 下面是脚本的内容, 请在国外服务器上运行, 比如GitHub Actions、Google云控制台等.
#!/bin/bash set -x KUBSPRAY_VERSION=v2.18.1 OSS_ENDPOINT=&amp;lt;OSS ENDPOINT&amp;gt; OSS_ACCESS_KEY_ID=&amp;lt;OSS ACCESS KEY ID&amp;gt; OSS_ACCESS_KEY=&amp;lt;OSS ACCESS KEY&amp;gt; OSS_CLOUD_URL=&amp;lt;Bucket和文件路径&amp;gt; ACR_REPO=&amp;lt;ACR地址&amp;gt; ACR_USERNAME=&amp;lt;ACR用户名&amp;gt; ACR_PASSWORD=&amp;lt;ACR密码&amp;gt; MY_IMAGE_REPO=${ACR_REPO}/&amp;lt;ACR命名空间&amp;gt; wget -O kubespray-src.</description></item><item><title>[PPT] 实践中总结 Kubernetes 必须了解的核心内容</title><link>https://blog.llaoj.cn/posts/2202/intro-kubernetes/</link><pubDate>Tue, 08 Feb 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2202/intro-kubernetes/</guid><description>PPT 分享 # 以下是 &amp;lt;实践中总结 Kubernetes 必须了解的核心内容&amp;gt; 主题分享 PPT
完~</description></item><item><title>kubernetes 中的 pod 究竟是什么</title><link>https://blog.llaoj.cn/posts/2202/what-are-kubernetes-pods-anyway/</link><pubDate>Tue, 08 Feb 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2202/what-are-kubernetes-pods-anyway/</guid><description>前言 # kubernetes 中 pod 的设计是一个伟大的发明, 今天我很有必要去聊一下 pod 和 container, 探究一下它们究竟是什么? kubernetes 官方文档中关于 pod 概念介绍提供了一个完整的解释, 但写的不够详细, 表达过于专业, 但还是很推荐大家阅读一下. 当然这篇文档应该更接地气.
容器真的存在吗? # linux 中是没有容器这个概念的, 容器就是 linux 中的普通进程, 它使用了 linux 内核提供的两个重要的特性: namespace &amp;amp; cgroups.
namespace 提供了一种隔离的特性, 让它之外的内容隐藏, 给它下面的进程一个不被干扰的运行环境(其实不完全,下面说) .
namespace 包含:
hostname Process IDs File System Network Interface Inter-Process Communication(IPC) 接上面, 其实 namespace 内部的进程并不是完全不和外面的进程产生影响的. 进程可以不受限制的使用物理机上的所有资源, 这样就会导致其他进程无资源可用. 所以, 为了限制进程资源使用, linux 提供了另一种特性 cgroups. 进程可以像在 namespace 中运行, 但是 cgroups 限制了进程的可以使用的资源. 这些资源包括:
CPU RAM block I/O network I/O etc.</description></item><item><title>Prometheus Operator 设计思路</title><link>https://blog.llaoj.cn/posts/2202/prometheus-operator-design/</link><pubDate>Mon, 07 Feb 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2202/prometheus-operator-design/</guid><description>设计 # 这篇文章介绍了 Prometheus Operator 的几种自定义资源 (CRD):
Prometheus Alertmanager ThanosRuler ServiceMonitor PodMonitor Probe PrometheusRule AlertmanagerConfig Prometheus # 它定义了在 Kubernetes 集群中安装 Prometheus 的方式. 它提供了一些配置项, 比如副本数、持久卷还有接收告警的 Alertmanagers.
对于每一个 Prometheus 资源, Operator 会在同一 namespace 中部署一个经过正确配置的 StatefulSet. 其配置文件 secret/prometheus-name 会被挂载到它的 pod 上.
它明确了 Prometheus 实例选择 ServiceMonitors 所用的标签. 任何时候, 一旦 ServiceMonitors 和 Prometheus 资源有更改, Operator 会根据他们生成重新配置文件, 然后更新到上述 Secret 配置文件中.
如果没有提供选择 ServiceMonitors 所用标签, 用户也可以自己管理 Secret, 同时该 Secret 也享受 Operator 配置安装 Prometheus 的能力.
Alertmanager # 该资源定义了在 Kubernetes 中配置安装 Alertmanager 的方式.</description></item><item><title>使用 velero 备份 kubernetes 指引</title><link>https://blog.llaoj.cn/posts/2202/velero-backup-k8s/</link><pubDate>Wed, 02 Feb 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2202/velero-backup-k8s/</guid><description>要求 # kubernetes 版本 1.7+，velero 的每个主版本对 kuberetes 的版本要求不同，详情请参考官方文档说明。 官方文档通道 velero 所在服务器有 kubectl 命令, 且能连上集群 我们先从最简单的体验开始
1. 安装 velero 客户端 # 下载二进制安装包, 点击 latest release, 下载 velero-v1.7.0-linux-amd64.tag.gz (以 release 页面为准), 解压
tar -xvf &amp;lt;RELEASE-TARBALL-NAME&amp;gt;.tar.gz 然后将二进制文件 velero 移动到 $PATH 中的一个目录, 如 /usr/local/bin
2. 创建 credentials # 备份文件保存在对象存储中, 在当前目录下创建 credentials-velero 文件, 声明连接对象存储所用的账号密码
[default] aws_access_key_id = &amp;lt;your key_id&amp;gt; aws_secret_access_key = &amp;lt;your secret&amp;gt; 3. 安装 velero 服务端 # 通过 velero 客户端在 kubernetes 中安装 deployment/velero，velero 提供了很多 stroage provider, 能将备份文件存储到比如 aws, aliyun-oss 中, 他们大都是支持 s3 接口的.</description></item></channel></rss>