<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>kubernetes on 老J的博客</title><link>https://blog.llaoj.cn/tags/kubernetes/</link><description>Recent content in kubernetes on 老J的博客</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Wed, 08 Jan 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.llaoj.cn/tags/kubernetes/index.xml" rel="self" type="application/rss+xml"/><item><title>解决执行kubectl命令没有权限</title><link>https://blog.llaoj.cn/posts/2025/fix-kubectl-no-authorization/</link><pubDate>Wed, 08 Jan 2025 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2025/fix-kubectl-no-authorization/</guid><description>问题描述 # 反应执行kubectl命令没有权限:
$ kubectl get pod -A Error from server (Forbidden): pods is forbidden: User &amp;#34;kubernetes-admin&amp;#34; cannot list resource &amp;#34;pods&amp;#34; in API group &amp;#34;&amp;#34; at the cluster scope 解决思路 # 首先要了解几个文件夹的作用:</description></item><item><title>解决Velero报错: failed to list daemonset pods: client rate limiter Wait returned an error: context deadline exceeded</title><link>https://blog.llaoj.cn/posts/2309/velero-failed-to-list-daemonset-pods/</link><pubDate>Mon, 04 Sep 2023 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2309/velero-failed-to-list-daemonset-pods/</guid><description>使用VeleroFSB备份集群的时候, 遇到了一些错误, 导致整个备份任务没有成功, 状态: &lt;code>PartiallyFailed&lt;/code>: failed to list daemonset pods: client rate limiter Wait returned an error: context deadline exceeded</description></item><item><title>通过shell脚本扫描从Kubernetes节点往外的tcp请求</title><link>https://blog.llaoj.cn/posts/2304/container-tcp-conn/</link><pubDate>Tue, 04 Apr 2023 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2304/container-tcp-conn/</guid><description>由于Kubernetes中部署的服务队外发起的tcp请求很难监控, 最近数据库运维在排查来自集群的大量数据库请求, 网络层只能看到来自哪个Kubernetes节点主机. 所以写了下面这个脚本来定时扫描.</description></item><item><title>Fluentd配置文件最佳实践</title><link>https://blog.llaoj.cn/posts/2207/fluentd-es-config/</link><pubDate>Sun, 31 Jul 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2207/fluentd-es-config/</guid><description>Fluentd负责Kubernetes中容器日志的收集工作, 以Daemonset形式运行在每一个节点上. 下面这个配置是在多个生产集群使用的配置, 经过多次调优的. 有一些关键的配置增加了配置解释说明. 目前使用问题不大. 持续更新配置中&amp;hellip;</description></item><item><title>Kubernetes 服务器配置和规划建设要求</title><link>https://blog.llaoj.cn/posts/2207/kubernetes-requirement/</link><pubDate>Sat, 16 Jul 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2207/kubernetes-requirement/</guid><description>新建集群的第一步就是要规划服务器、网络、操作系统等等, 下面就结合我平时的工作经验总结下相关的要求, 内容根据日常工作持续补充完善:
服务器配置 # kubernetes 集群分为控制节点和数据节点, 它们对于配置的要求有所不同:
控制面 # 节点规模 Master规格 1~5个节点 4核 8Gi（不建议2核 4Gi） 6~20个节点 4核 16Gi 21~100个节点 8核 32Gi 100~200个节点 16核 64Gi 系统盘40+Gi，用于储存 etcd 信息及相关配置文件等</description></item><item><title>Linux 控制组(cgroups)和进程隔离</title><link>https://blog.llaoj.cn/posts/2205/cgroups-process-isolation/</link><pubDate>Sun, 15 May 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2205/cgroups-process-isolation/</guid><description>控制组(cgroups)是内核的一个特性，它能限制/统计/隔离一个或者多个进程使用CPU、内存、磁盘I/O和网络。cgroups技术最开始是Google开发，最终在2.6.24版本的内核中出现。3.15和3.16版本内核将合并进重新设计的cgroups，它添加来kernfs(拆分一些sysfs逻辑)。cgroups的主要设计目标是提供一个统一的接口，它可以管理进程或者整个操作系统级别的虚拟化，包含Linux容器，或者LXC。</description></item><item><title>分析告警 kubernetes 节点 load 过高问题</title><link>https://blog.llaoj.cn/posts/2204/kubernetes-node-load/</link><pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2204/kubernetes-node-load/</guid><description>负载过高分析 # 通过 linux 提供的几个命令可以从不同的纬度分析系统负载。
vmstat # 这命令能从一个系统的角度反应出服务器情况，报告虚拟内存统计信息，报告有关进程、内存、分页、块的信息 IO、陷阱、磁盘和 CPU 活动。看个例子：
$ vmstat --wide --unit M 5 procs ----------------memory---------------- ---swap--- -----io---- ---system--- ---------cpu-------- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 1 0 127691 1535 73572 0 0 0 3 0 0 2 1 97 0 0 93 0 0 127674 1535 73573 0 0 0 80 49267 67634 5 1 94 1 0 0 2 0 127679 1535 73573 0 0 0 66 38537 56283 3 1 95 1 0 2 2 0 127738 1535 73574 0 0 6 86 41769 61823 5 1 93 2 0 2 0 0 127729 1535 73574 0 0 18 18 41002 59214 4 1 95 0 0 命令以及输出解释：</description></item><item><title>Apisxi Ingress Controller 设计说明</title><link>https://blog.llaoj.cn/posts/2204/apisix-ingress-controller-design/</link><pubDate>Thu, 14 Apr 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2204/apisix-ingress-controller-design/</guid><description>apisix-ingress-controller 要求 kubernetes 版本 1.16+. 因为使用了 CustomResourceDefinition v1 stable 版本的 API. 从 1.0.0 版本开始，APISIX-ingress-controller 要求 Apache APISIX 版本 2.7+.</description></item><item><title>在 kubernetes 中找出使用 jdk9 及以上版本的应用</title><link>https://blog.llaoj.cn/posts/2203/execjava-versioninpod/</link><pubDate>Wed, 30 Mar 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2203/execjava-versioninpod/</guid><description>近日, Spring Cloud (SPEL) 中发现 RCE 0-day 漏洞, 为了排查 kubernetes 中所有存在安全威胁的应用. 特地开发了一个小工具来寻找。该工具基于 golang&amp;amp;client-go 开发, 程序会找出当前集群中所有 Running 的 pods, 然后逐个进入容器，执行 &lt;code>java -version&lt;/code> 命令，将命令输出打印到文件中，使用编辑器进行查找检索即可。</description></item><item><title>在 kubernetes 中找出过度使用资源的 namespaces</title><link>https://blog.llaoj.cn/posts/2203/find-ns-that-exceed-resource-limits/</link><pubDate>Mon, 28 Mar 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2203/find-ns-that-exceed-resource-limits/</guid><description>我们知道, 在 kubernetes 中, namespace 的资源限制在 ResourceQuota 中定义, 比如我们控制 default 名称空间使用 1核1G 的资源. 通常来讲, 由于 kubernetes 的资源控制机制, &lt;code>.status.used&lt;/code> 中资源的值会小于 &lt;code>.status.hard&lt;/code> 中相应资源的值. 但是也有特例. 当我们开始定义了一个较大的资源限制, 待应用部署完毕, 资源占用了很多之后, 这时调低资源限制. 此时就会出现 &lt;code>.status.used&lt;/code> 中的值超过 &lt;code>.status.hard&lt;/code> 中相应值的情况, 尤其是内存的限制.</description></item><item><title>比较冷门但有用的 kubectl 命令</title><link>https://blog.llaoj.cn/posts/2203/kubectl-usefull-command/</link><pubDate>Tue, 22 Mar 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2203/kubectl-usefull-command/</guid><description>以下冷门命令能实现某种具体的功能, 都是在实际工作中摸索总结的经验, 获取到相关的资源名称之后, 就可以配合常用的 kubectl 命令获取其他详细信息.</description></item><item><title>[解决] FailedScheduling pod/&lt;pod-name> pod is &lt;uid> in the cache so can't be assumed</title><link>https://blog.llaoj.cn/posts/2203/pod-cannot-be-assumed/</link><pubDate>Mon, 21 Mar 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2203/pod-cannot-be-assumed/</guid><description>pod is in the cache, so can&amp;rsquo;t be assumed, 这是调度器 scheduler 缓存失效导致的异常事件, 大致原因是 pod 已经调度, 并绑定到指定节点, 由于该节点异常导致启动失败, 重新启动 prometheus statefulset, 让集群重新调度, 其实就是将现有到 prometheus pod 副本数将至 0, 再恢复正常即可.</description></item><item><title>[解决] Warning pod/calico-node-&lt;hash> Readiness probe failed</title><link>https://blog.llaoj.cn/posts/2203/calico-node-readiness-probe-failed/</link><pubDate>Mon, 21 Mar 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2203/calico-node-readiness-probe-failed/</guid><description>&lt;p>calico-node-4fpgp Readiness probe failed, orphaned pod &lt;pod-hash> found, but volume paths are still present on disk : There were a total of N errors similar to this. Turn up verbosity to see them.&lt;/p></description></item><item><title>load average 过高, mount nfs 问题处理</title><link>https://blog.llaoj.cn/posts/2203/nfs-options/</link><pubDate>Mon, 14 Mar 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2203/nfs-options/</guid><description>周末, 有一台服务器告警: 系统负载过高, 最高的时候都已经到 100 +, 以下是排查&amp;amp;处理的具体过程.
发现的问题/现象 # uptime 显示 load average 都在70+ # 因为服务器是40核心, 原则上负载40是满负荷, 现在明显存在大量等待的任务. 继续往下分析进程, 看具体那个进程一直在堵塞.
ps -ef 执行到某一个进程就卡住了 # 命令执行如下:</description></item><item><title>解决 kubelet cannot allocate memory 错误</title><link>https://blog.llaoj.cn/posts/2202/cannot-allocate-memory/</link><pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2202/cannot-allocate-memory/</guid><description>问题描述 # 查看 pod 相关 events 如下：
Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 18m default-scheduler Successfully assigned container-186002196200947712/itms-5f6d7798-wrpjj to 10.</description></item><item><title>使用Kubespray安装kubernetes的教程</title><link>https://blog.llaoj.cn/posts/2202/Kubespray-kubernetes-setup/</link><pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2202/Kubespray-kubernetes-setup/</guid><description>本文使用 kubespray 容器部署 kubernetes v1.22, 提供了从国外搬运的离线软件包/容器镜像. 仅需要几步即可部署高可用集群. 所有离线文件都来自官方下载 kubespray 安装过程会进行软件包验证, 放心使用.
前提 # 禁用防火墙 重要: 本文使用 kubespray 的容器环境部署, 为避免影响节点部署(特别是 Runtime 部署), 所以需要一台独立于集群外的服务器执行下面的命令, 这台服务器安装 docker 19.</description></item><item><title>[PPT] 实践中总结 Kubernetes 必须了解的核心内容</title><link>https://blog.llaoj.cn/posts/2202/intro-kubernetes/</link><pubDate>Tue, 08 Feb 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2202/intro-kubernetes/</guid><description>PPT 分享 # 以下是 &amp;lt;实践中总结 Kubernetes 必须了解的核心内容&amp;gt; 主题分享 PPT
完~</description></item><item><title>kubernetes 中的 pod 究竟是什么</title><link>https://blog.llaoj.cn/posts/2202/what-are-kubernetes-pods-anyway/</link><pubDate>Tue, 08 Feb 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2202/what-are-kubernetes-pods-anyway/</guid><description>前言 # kubernetes 中 pod 的设计是一个伟大的发明, 今天我很有必要去聊一下 pod 和 container, 探究一下它们究竟是什么? kubernetes 官方文档中关于 pod 概念介绍提供了一个完整的解释, 但写的不够详细, 表达过于专业, 但还是很推荐大家阅读一下. 当然这篇文档应该更接地气.
容器真的存在吗? # linux 中是没有容器这个概念的, 容器就是 linux 中的普通进程, 它使用了 linux 内核提供的两个重要的特性: namespace &amp;amp; cgroups.</description></item><item><title>Prometheus Operator 设计思路</title><link>https://blog.llaoj.cn/posts/2202/prometheus-operator-design/</link><pubDate>Mon, 07 Feb 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2202/prometheus-operator-design/</guid><description>设计 # 这篇文章介绍了 Prometheus Operator 的几种自定义资源 (CRD):
Prometheus Alertmanager ThanosRuler ServiceMonitor PodMonitor Probe PrometheusRule AlertmanagerConfig Prometheus # 它定义了在 Kubernetes 集群中安装 Prometheus 的方式.</description></item><item><title>使用 velero 备份 kubernetes 指引</title><link>https://blog.llaoj.cn/posts/2202/velero-backup-k8s/</link><pubDate>Wed, 02 Feb 2022 00:00:00 +0000</pubDate><guid>https://blog.llaoj.cn/posts/2202/velero-backup-k8s/</guid><description>要求 # kubernetes 版本 1.7+，velero 的每个主版本对 kuberetes 的版本要求不同，详情请参考官方文档说明。 官方文档通道 velero 所在服务器有 kubectl 命令, 且能连上集群 我们先从最简单的体验开始
1. 安装 velero 客户端 # 下载二进制安装包, 点击 latest release, 下载 velero-v1.</description></item></channel></rss>