[{"id":0,"href":"/docs/mywork/gcopy/","title":"GCopy","section":"我的项目","content":" 项目地址: https://github.com/llaoj/gcopy\n一个剪切板同步的工具, 支持文字、截图和文件.\n使用 Golang 和 Nextjs 开发. 完全开源.\nGCopy重视您的数据隐私, 不持久化存储您的数据, 它们都在内存中. 如果你24h内不使用, 数据就会被删除.\n背景\n在日常的办公中, 我们通常需要操作两台以上的电脑, 尤其是软件开发工程师.\n假如, 你需要同时操作Windows电脑和MacOS电脑, 在这两个设备之间传递信息变得非常麻烦. 因为很多原因, 我找不到一个非常好的工具能共享两个不同操作系统的设备的剪切板. 当我拷贝了一段文字, 我需要在另外一台电脑上粘贴, 这通常非常困难.\n现在的工具通常只支持文字, 但是截图, 文件不支持. 而且, 他们通常要求设备必须要在同一个局域网, 可以互相访问. 他们往往收费.\n这很不好!\n所以, 我开发了GCopy, 它解决了这些问题. 目前, 你可以在PC、Mac和移动端之间共享剪切板, 支持文字, 截图和文件. 它对网络没有太高的要求, 不同的设备可以在同一个局域网, 也可以不在.\n最开始我使用Git作为后端存储, 使用powershell、osascript这样的脚本在不同的设备之间同步剪切板. 但是因为它依赖Git, 注定不能让更多的非技术朋友使用. 所以, 我使用Golang替换了Git, 来作为不同设备之间的数据中转服务, 但是它仍然需要您在设备上下载运行GCopy客户端, 给使用带来了门槛. 所以, 我做了现在的GCopy v1.0, 它是一个Web服务, 您可以直接打开网站 https://gcopy.rutron.net 使用, 同时不用担心您的数据泄露.\n"},{"id":1,"href":"/docs/mywork/oauth2nsso/","title":"OAuth2\u0026SSO","section":"我的项目","content":"项目地址: https://github.com/llaoj/oauth2nsso\nllaoj/oauth2nsso 项目是基于 go-oauth2 打造的独立的 OAuth2.0 和 SSO 服务，提供了开箱即用的 OAuth2.0服务和单点登录SSO服务。开源一年多，获得了社区很多用户的关注，该项目多公司线上在用，其中包含上市公司。轻又好用，稳的一P。\n"},{"id":2,"href":"/docs/mywork/kubefinder/","title":"KubeFinder","section":"我的项目","content":" 项目地址: https://github.com/llaoj/kube-finder\n一个独立的容器文件服务器, 可以通过该项目查看容器内(namespace/pod/container)的文件目录结构/列表, 下载文件或者上传文件到指定目录中. 使用golang开发, 直接操作主机 /proc/\u0026lt;pid\u0026gt;/root 目录, 速度很快.\n这是一个后端项目, 仅提供API, 前端对接之后可以是这样的\n 也可以是这样的：\n 项目名取自MacOS Finder, 希望它像Finder一样好用.\n"},{"id":3,"href":"/docs/about/","title":"关于","section":"Docs","content":"上善若水, 水善利万物而不争.\n专注云原生领域, 包括容器, 微服务, Service Mesh, Serverless, Devops, Gitops等技术, 热衷于参与开源软件和开源社区. 主导微服务化体系建设，推动传统运维方式向云原生方向转型.\n喜欢技术, 喜欢云原生. 我们的工作/人生需要不断总结和复盘, 归纳整理. 这里是我记录学习过程, 积累经验, 总结工作的地方. 如果恰好对你也有所帮助, 我真的会很高兴. 水平一般, 能力有限, 希望我们能共同进步, 多创造点价值, 多帮助到别人, 不枉此生!\n我的Github地址: https://github.com/llaoj\n我在Bilibili也发了一些视频: https://space.bilibili.com/67920653\n你可以通过我的邮箱联系到我: qustwwy@163.com\n或者, 你可以添加我的微信\n "},{"id":4,"href":"/posts/2309/velero-failed-to-list-daemonset-pods/","title":"解决Velero报错: failed to list daemonset pods: client rate limiter Wait returned an error: context deadline exceeded","section":"文章","content":"使用VeleroFSB备份集群的时候, 遇到了一些错误, 导致整个备份任务没有成功, 状态: PartiallyFailed.\n以下是报错日志:\n$ velero backup describe backup-20230903090401 Name: backup-20230903090401 Namespace: velero Labels: velero.io/schedule-name=backup velero.io/storage-location=default Annotations: velero.io/source-cluster-k8s-gitversion=v1.22.17 velero.io/source-cluster-k8s-major-version=1 velero.io/source-cluster-k8s-minor-version=22 Phase: PartiallyFailed (run `velero backup logs backup-20230903090401` for more information) Errors: Velero: name: /resource-nginx-8559c56b49-7w5w8 error: /timed out waiting for all PodVolumeBackups to complete name: /yjwz-server-55bbc5b5f4-rz9wz error: /failed to list daemonset pods: client rate limiter Wait returned an error: context deadline exceeded name: /yjwz-web-67d9f496c9-9v45p error: /failed to list daemonset pods: client rate limiter Wait returned an error: context deadline exceeded name: /fs-erp-dev-5b549f6788-trtr7 error: /failed to list daemonset pods: client rate limiter Wait returned an error: context deadline exceeded name: /fs-erp-test-6c9c64b495-6twpr error: /failed to list daemonset pods: client rate limiter Wait returned an error: context deadline exceeded ... 通过google并没有找到答案, 好像大家没有遇到这个问题, 翻遍官方文档也没有. 于是我拿着报错日志, 去搜索了源代码, 找到了该报错对应的逻辑.\n 可以看到, 这是在对node-agent进行健康检查, 通过kubernetes接口查找daemonset管理的pods列表的方式来对node-agent进行健康检查. http客户端报超过请求次数限制. 那么应该会有请求次数限制的相关定义.\n通过定位podClient的来源, 找到了关于速率的定义参数:\n Github现在做的越来越好了, 检索分析代码也很方便. 代码中定义的\u0026ndash;client-qps默认值为20.0, \u0026ndash;client-burst默认值为30, 将其提高配置如下:\ncontainers:- args:- server- --features=- --default-volumes-to-fs-backup=true- --uploader-type=restic+ - --client-qps=2000.0+ - --client-burst=3000command:- /velero再执行一遍备份试试:\nvelero backup create backup-cluster-20230904 "},{"id":5,"href":"/posts/2304/container-tcp-conn/","title":"通过shell脚本扫描从Kubernetes节点往外的tcp请求","section":"文章","content":"由于Kubernetes中部署的服务队外发起的tcp请求很难监控, 最近数据库运维在排查来自集群的大量数据库请求, 网络层只能看到来自哪个Kubernetes节点主机. 所以写了下面这个脚本来定时扫描.\n#! /bin/bash  set -ex filter=$1 test -n \u0026#34;$filter\u0026#34; echo \u0026#34;过滤字符串: $filter\u0026#34; resultDir=\u0026#34;/tmp/container_tcp_conn\u0026#34; test ! -d \u0026#34;$resultDir\u0026#34; \u0026amp;\u0026amp; mkdir $resultDir cd $resultDir || return if command -v docker \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then whichPid=\u0026#34;docker inspect -f {{.State.Pid}} {}\u0026#34; else whichPid=\u0026#34;crictl inspect {} | jq .info.pid\u0026#34; fi cantainer_tcp_conn() { containers=$(crictl ps | awk \u0026#39;{print $1}\u0026#39; | grep -v CONTAINER) for container in $containers; do pid=$(echo \u0026#34;$container\u0026#34; | xargs -I {} /bin/sh -c \u0026#34;$whichPid\u0026#34;) output=$(crictl inspect \u0026#34;$container\u0026#34; | grep \u0026#34;logPath\u0026#34; | awk -F \u0026#34;/\u0026#34; \u0026#39;{print $5\u0026#34;_\u0026#34;$6}\u0026#39;) { printf \u0026#34;[%s] start scanning...\\n\u0026#34; \u0026#34;$(date +\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;)\u0026#34; nsenter -t \u0026#34;$pid\u0026#34; -n ss -natup | grep \u0026#34;$filter\u0026#34; || true } \u0026gt;\u0026gt;\u0026#34;$output\u0026#34; done } cantainer_tcp_conn 解释一下:\n  为了避免疯狂输出文件, 必须添加过滤字符串 根据节点是否有docker命令来选择获取pid的方式 每次扫描都会记录时间 进入容器的网络namespace使用ss命令获取tcp连接信息   你可以这样执行上面的脚本:\n./container_tcp_conn.sh ip地址:端口号 把它部署到cron中每分钟/几分钟定期扫描, 应该就能发现请求的容器. 比如:\n*/2 20 * * * /root/container_tcp_conn.sh 10.206.97.239:3317 \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 最后, 通过执行下面的命令, 你应该就能看到具体是哪个pod哪个容器请求的你关注的主机和端口号:\ngrep -rn \u0026#34;关键词\u0026#34; /tmp/container_tcp_conn "},{"id":6,"href":"/posts/2210/docker-oracle-xe-11g/","title":"使用docker运行orcale xe 11g","section":"文章","content":"注意: 根据自己实际情况, 替换下面名利中的\u0026lt;var\u0026gt;变量.\n启动orcale xe 11g容器 #  Oracle Database XE是人人都可免费使用的 Oracle 数据库. Oracle Database XE 支持最高:\n 最多 12 GB 的用户磁盘数据 最大 2 GB 的数据库 RAM 最多 2 个 CPU 线程  产品介绍地址: https://www.oracle.com/cn/database/technologies/appdev/xe.html\nOracle Database XE支持容器化部署, 镜像项目地址(这里面也有详细使用文档): https://hub.docker.com/r/oracleinanutshell/oracle-xe-11g\ndocker run -d \\  --name=oracle-xe-11g \\  --restart=always \\  -p 31521:1521 \\  -p 38080:8080 \\  -v /data/oracle_data:/opt/oracle/oracle_data:rw \\  oracleinanutshell/oracle-xe-11g 该镜像的默认登录信息是:\nhostname: localhost port: 31521 sid: xe username: system 或者 sys password: oracle 使用了主机的/data/oracle_data目录作为数据持久化目录.\n创建表空间和用户 #   进入容器  docker exec -it oracle-xe-11g bash 创建表空间  切换到oracle用户\nsu oracle 以管理员身份登录数据库, 两个系统账号 system/sys 密码默认都是 oracle\nsqlplus system/oracle as sysdba 创建表空间\ncreate tablespace \u0026lt;tablespace-name\u0026gt; logging datafile \u0026#39;/opt/oracle/oracle_data/\u0026lt;tablespace-name\u0026gt;.dbf\u0026#39; size 200m autoextend on next 100m maxsize unlimited; 创建用户并分配权限  创建和业务相关的用户, 并赋予相关权限.\ncreate user \u0026lt;user-name\u0026gt; identified by \u0026lt;user-password\u0026gt; default tablespace \u0026lt;tablespace-name\u0026gt;; 角色授权\ngrant connect,resource,dba to \u0026lt;user-name\u0026gt;; 修改字符集 #  一般来说,初装之后数据库字符集都是AMERICAN_AMERICA.AL32UTF8, 有些数据库要求使用特定字符集, 比如需要修改成ZHS16GBK, 先查询现在的字符集:\nselect userenv(\u0026#39;language\u0026#39;) from dual; 参考上述步骤, 先以管理员身份登录oracle数据库, 执行变更:\nshutdown immediate; startup mount; alter system enable restricted session; alter system set job_queue_processes=0; alter system set aq_tm_processes=0; alter database open; alter database character set internal_use ZHS16GBK; shutdown immediate; startup; 最后执行命令检查字符集是否修改完成.\n参考文档: https://www.cnblogs.com/geekdc/p/5817306.html\n导入dmp文件 #  很多使用, 我们需要从dmp文件导入oracle数据库, 参考上面步骤, 进入oracle-xe-11g容器, 切换到 oracle 用户执行下面命令, 把数据和结构一起导入. 先将dmp文件放到主机目录/data/oracle_data/imp/example.dmp中, 这样映射到容器中需要导入的数据目录为: /opt/oracle/oracle_data/imp/example.dmp, 执行导入:\nimp \u0026lt;user-name\u0026gt;/\u0026lt;user-password\u0026gt; file=/opt/oracle/oracle_data/dmp/example.dmp full=y; 修改管理员账号密码 #  为了安全, 我们可能需要修改管理员账号的密码, 可以使用管理员免密登录:\nsqlplus /nolog; conn /as sysdba; 查看用户列表:\nselect username from dba_users; 修改密码:\nalter user sys identified by \u0026lt;new-password\u0026gt;; alter user system identified by \u0026lt;new-password\u0026gt;; system是数据库内置的一个普通管理员, 手工创建的任何用户在被授予dba角色后都跟这个用户差不多. sys是数据库的超级用户, 数据库内很多重要的东西(数据字典表、内置包、静态数据字典视图等)都属于这个用户, sys用户必须以sysdba身份登录.\n登录信息 #  好了! 现在可以登录了, 登录信息:\nhost: \u0026lt;host-ip\u0026gt; port: 31521 user: \u0026lt;user-name\u0026gt; password: \u0026lt;user-password\u0026gt; sid: xe "},{"id":7,"href":"/posts/2211/fluentd-kafka/","title":"使用fluentd收集kubernetes日志并推送给kafka","section":"文章","content":"这篇文章使用fluentd官方提供的kubernetes部署方案daemonset来部署日志收集, 参考项目地址:\n  https://github.com/fluent/fluentd-kubernetes-daemonset  本文使用的kubernetes版本为: 1.22.8\n使用fluentd镜像为: fluent/fluentd-kubernetes-daemonset:v1.15.2-debian-kafka2-1.0\n请注意下文配置中\u0026lt;var\u0026gt;标记, 需要根据需求自行替换.\n创建命名空间 #  本项目所有的资源创建在logging下, 先创建它:\nkubectl create ns fluentd-kafka 先创建服务账号 #  创建服务账号并赋予集群查看的权限, 使用下面的命令:\nkubectl -n fluentd-kafka create -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: ServiceAccount metadata: name: fluentd EOF 创建绑定关系:\nkubectl create -f - \u0026lt;\u0026lt;EOF kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: fluentd-kafka roleRef: kind: ClusterRole name: view apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: fluentd namespace: fluentd-kafka EOF 创建配置文件 #  执行下面到命令创建configmap:\ncat \u0026gt; /tmp/fluentd.conf \u0026lt;\u0026lt;EOF # 粘贴下面的配置 EOF kubectl -n fluentd-kafka create configmap fluentd-kafka-conf --from-file=fluent.conf=/tmp/fluentd.conf 配置文件内容如下, 它只收集容器日志:\n@include \u0026#39;prometheus.conf\u0026#39; \u0026lt;label @FLUENT_LOG\u0026gt; \u0026lt;match fluent.**\u0026gt; @type null @id ignore_fluent_logs \u0026lt;/match\u0026gt; \u0026lt;/label\u0026gt; \u0026lt;source\u0026gt; @type tail @id in_tail_container_logs path \u0026#39;/var/log/containers/*.log\u0026#39; pos_file /var/log/fluentd-kafka-containers.log.pos tag \u0026#39;kubernetes.*\u0026#39; exclude_path use_default read_from_head true follow_inodes true \u0026lt;parse\u0026gt; @type \u0026#34;json\u0026#34; time_key \u0026#34;read_time\u0026#34; \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt; \u0026lt;filter kubernetes.**\u0026gt; @type kubernetes_metadata @id filter_kube_metadata kubernetes_url \u0026#34;#{\u0026#39;https://\u0026#39; + ENV.fetch(\u0026#39;KUBERNETES_SERVICE_HOST\u0026#39;) + \u0026#39;:\u0026#39; + ENV.fetch(\u0026#39;KUBERNETES_SERVICE_PORT\u0026#39;) + \u0026#39;/api\u0026#39;}\u0026#34; verify_ssl \u0026#34;#{ENV[\u0026#39;KUBERNETES_VERIFY_SSL\u0026#39;] || true}\u0026#34; ca_file \u0026#34;#{ENV[\u0026#39;KUBERNETES_CA_FILE\u0026#39;]}\u0026#34; skip_labels false skip_container_metadata true skip_master_url true skip_namespace_metadata false watch true \u0026lt;/filter\u0026gt; \u0026lt;filter kubernetes.**\u0026gt; @type record_transformer \u0026lt;record\u0026gt; cluster_id \u0026#39;CLUSTER_ID\u0026#39; \u0026lt;/record\u0026gt; \u0026lt;/filter\u0026gt; \u0026lt;match **\u0026gt; @type kafka2 @id out_kafka2 brokers \u0026#34;#{ENV[\u0026#39;FLUENT_KAFKA2_BROKERS\u0026#39;]}\u0026#34; # username \u0026#34;#{ENV[\u0026#39;FLUENT_KAFKA2_USERNAME\u0026#39;] || nil}\u0026#34; # password \u0026#34;#{ENV[\u0026#39;FLUENT_KAFKA2_PASSWORD\u0026#39;] || nil}\u0026#34; # scram_mechanism \u0026#39;sha256\u0026#39; # sasl_over_ssl false default_topic \u0026#34;#{ENV[\u0026#39;FLUENT_KAFKA2_DEFAULT_TOPIC\u0026#39;] || nil}\u0026#34; partition_key_key \u0026#39;kubernetes.host\u0026#39; use_event_time true get_kafka_client_log true \u0026lt;format\u0026gt; @type \u0026#39;json\u0026#39; \u0026lt;/format\u0026gt; \u0026lt;inject\u0026gt; time_key \u0026#34;read_time\u0026#34; \u0026lt;/inject\u0026gt; \u0026lt;buffer\u0026gt; @type file path /var/log/fluentd/kafka-buffers flush_thread_count 8 flush_interval \u0026#39;5s\u0026#39; chunk_limit_size \u0026#39;2M\u0026#39; retry_max_interval 30 retry_forever true overflow_action \u0026#39;block\u0026#39; \u0026lt;/buffer\u0026gt; # ruby-kafka producer options max_send_retries 10000 required_acks 1 ack_timeout 20 compression_codec \u0026#39;gzip\u0026#39; discard_kafka_delivery_failed false \u0026lt;/match\u0026gt; 注意: 因为CPU调度原因, 日志在日志文件中的排列顺序和time的顺序不一致. 所以, 使用read_time作为日志的envent time, 表示日志的采集时间. 这样就能确保日志的顺序和日志源文件中保持一致. 源日志中的time字段保留, 作为日志生成时间.\n创建daemonset部署 #  该镜像配置都是通过环境变量, 请根据自己实际情况修改环境变量配置.\n先创建deployment部署文件:\nkubectl -n logging-kafka apply -f - \u0026lt;\u0026lt;EOF # 粘贴下面的内容 EOF 将下面的内容拷贝进去:\napiVersion:apps/v1kind:DaemonSetmetadata:name:fluentdlabels:k8s-app:fluentd-loggingversion:v1spec:selector:matchLabels:k8s-app:fluentd-loggingversion:v1template:metadata:labels:k8s-app:fluentd-loggingversion:v1spec:serviceAccount:fluentdserviceAccountName:fluentdtolerations:- key:node-role.kubernetes.io/control-planeeffect:NoSchedule- key:node-role.kubernetes.io/mastereffect:NoSchedulecontainers:- name:fluentdimage:fluent/fluentd-kubernetes-daemonset:v1.15.2-debian-kafka2-1.0env:- name:K8S_NODE_NAMEvalueFrom:fieldRef:fieldPath:spec.nodeName- name:FLUENT_KAFKA2_BROKERSvalue:\u0026#34;10.206.96.26:9092,10.206.96.27:9092,10.206.96.28:9092\u0026#34;- name:FLUENT_KAFKA2_DEFAULT_TOPICvalue:\u0026#34;container-log\u0026#34;# when log formt is not json, unconmment- name:FLUENT_CONTAINER_TAIL_PARSER_TYPEvalue:\u0026#34;/^(?\u0026lt;time\u0026gt;.+) (?\u0026lt;stream\u0026gt;stdout|stderr) [^ ]* (?\u0026lt;log\u0026gt;.*)$/\u0026#34;resources:limits:memory:600Mirequests:cpu:100mmemory:200MivolumeMounts:- name:varlogmountPath:/var/log# When actual pod logs in /var/lib/docker/containers, the following lines should be used.# - name: dockercontainerlogdirectory# mountPath: /var/lib/docker/containers# readOnly: true- name:config-volumemountPath:/fluentd/etc/fluent.confsubPath:fluent.confterminationGracePeriodSeconds:30volumes:- name:varloghostPath:path:/var/log# When actual pod logs in /var/lib/docker/containers, the following lines should be used.# - name: dockercontainerlogdirectory# hostPath:# path: /var/lib/docker/containers- name:config-volumeconfigMap:name:fluentd-kafka-conf"},{"id":8,"href":"/posts/2209/fluentd-kubernetes-daemonset/","title":"使用fluentd收集kubernetes日志并推送到ES","section":"文章","content":"这篇文章使用fluentd官方提供的kubernetes部署方案daemonset来部署日志收集, 参考项目地址:\n  https://github.com/fluent/fluentd-kubernetes-daemonset  本文使用的kubernetes版本为: 1.22.8\n使用fluentd镜像为: fluent/fluentd-kubernetes-daemonset:v1.15.2-debian-elasticsearch7-1.0\n请注意下文配置中\u0026lt;var\u0026gt;标记, 需要根据需求自行替换.\n创建命名空间 #  本项目所有的资源创建在logging下, 先创建它:\nNAMESPACE=logging kubectl create ns $NAMESPACE 先创建服务账号 #  创建服务账号并赋予集群查看的权限, 使用下面的命令:\nkubectl -n $NAMESPACE create -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: ServiceAccount metadata: name: fluentd EOF 创建绑定关系:\nkubectl create -f - \u0026lt;\u0026lt;EOF kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: fluentd roleRef: kind: ClusterRole name: view apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: fluentd namespace: ${NAMESPACE} EOF 创建配置文件 #  配置文件使用configmap挂在在容器内, 覆盖容器内现有的配置文件. 根据实际需要创建配置文件. 官方镜像提供个默认配置之外, 我有以下额外的需求,\n 根据用户为pod配置的标签collect-logs: true来判断是否收集该容器日志, 没有配置该标签不会收集. 需要将日志输出到elasticsearch中, 同时要保证日志的唯一, 不能同一条日志在elasticsearch中存在多条. 推送到ES中的日志都带有集群标识, 方便检索.  好, 开始创建, 先创建一个文件:\nvi /tmp/fluent.conf 写入下面的内容, 这里的内容大部分是官方镜像中拷贝过来的, 我在其基础之上做了一些修改以满足我的需求:\n注意: CLUSTER_ID表示为集群的唯一标识,请替换!\n# AUTOMATICALLY GENERATED # DO NOT EDIT THIS FILE DIRECTLY, USE /templates/conf/fluent.conf.erb @include \u0026#34;#{ENV[\u0026#39;FLUENTD_SYSTEMD_CONF\u0026#39;] || \u0026#39;systemd\u0026#39;}.conf\u0026#34; @include \u0026#34;#{ENV[\u0026#39;FLUENTD_PROMETHEUS_CONF\u0026#39;] || \u0026#39;prometheus\u0026#39;}.conf\u0026#34; @include kubernetes.conf @include conf.d/*.conf \u0026lt;match kubernetes.**\u0026gt; @type rewrite_tag_filter \u0026lt;rule\u0026gt; key $.kubernetes.labels.collect-logs pattern /^true$/ tag collect-logs.CLUSTER_ID \u0026lt;/rule\u0026gt; \u0026lt;/match\u0026gt; \u0026lt;filter collect-logs.CLUSTER_ID\u0026gt; @type elasticsearch_genid hash_id_key _hash \u0026lt;/filter\u0026gt; \u0026lt;match collect-logs.CLUSTER_ID\u0026gt; @type elasticsearch @id out_es @log_level info include_tag_key true hosts \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_HOSTS\u0026#39;]}\u0026#34; host \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_HOST\u0026#39;]}\u0026#34; port \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_PORT\u0026#39;]}\u0026#34; path \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_PATH\u0026#39;]}\u0026#34; scheme \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_SCHEME\u0026#39;] || \u0026#39;http\u0026#39;}\u0026#34; ssl_verify \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_SSL_VERIFY\u0026#39;] || \u0026#39;true\u0026#39;}\u0026#34; ssl_version \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_SSL_VERSION\u0026#39;] || \u0026#39;TLSv1_2\u0026#39;}\u0026#34; user \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_USER\u0026#39;] || use_default}\u0026#34; password \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_PASSWORD\u0026#39;] || use_default}\u0026#34; reload_connections \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_RELOAD_CONNECTIONS\u0026#39;] || \u0026#39;false\u0026#39;}\u0026#34; reconnect_on_error \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_RECONNECT_ON_ERROR\u0026#39;] || \u0026#39;true\u0026#39;}\u0026#34; reload_on_failure \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_RELOAD_ON_FAILURE\u0026#39;] || \u0026#39;true\u0026#39;}\u0026#34; log_es_400_reason \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_LOG_ES_400_REASON\u0026#39;] || \u0026#39;false\u0026#39;}\u0026#34; logstash_prefix \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX\u0026#39;] || \u0026#39;logstash\u0026#39;}\u0026#34; logstash_dateformat \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_LOGSTASH_DATEFORMAT\u0026#39;] || \u0026#39;%Y.%m.%d\u0026#39;}\u0026#34; logstash_format \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_LOGSTASH_FORMAT\u0026#39;] || \u0026#39;true\u0026#39;}\u0026#34; index_name \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_LOGSTASH_INDEX_NAME\u0026#39;] || \u0026#39;logstash\u0026#39;}\u0026#34; target_index_key \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_TARGET_INDEX_KEY\u0026#39;] || use_nil}\u0026#34; type_name \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_LOGSTASH_TYPE_NAME\u0026#39;] || \u0026#39;fluentd\u0026#39;}\u0026#34; include_timestamp \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_INCLUDE_TIMESTAMP\u0026#39;] || \u0026#39;false\u0026#39;}\u0026#34; template_name \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_TEMPLATE_NAME\u0026#39;] || use_nil}\u0026#34; template_file \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_TEMPLATE_FILE\u0026#39;] || use_nil}\u0026#34; template_overwrite \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_TEMPLATE_OVERWRITE\u0026#39;] || use_default}\u0026#34; sniffer_class_name \u0026#34;#{ENV[\u0026#39;FLUENT_SNIFFER_CLASS_NAME\u0026#39;] || \u0026#39;Fluent::Plugin::ElasticsearchSimpleSniffer\u0026#39;}\u0026#34; request_timeout \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_REQUEST_TIMEOUT\u0026#39;] || \u0026#39;5s\u0026#39;}\u0026#34; application_name \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_APPLICATION_NAME\u0026#39;] || use_default}\u0026#34; suppress_type_name \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_SUPPRESS_TYPE_NAME\u0026#39;] || \u0026#39;true\u0026#39;}\u0026#34; enable_ilm \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_ENABLE_ILM\u0026#39;] || \u0026#39;false\u0026#39;}\u0026#34; ilm_policy_id \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_ILM_POLICY_ID\u0026#39;] || use_default}\u0026#34; ilm_policy \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_ILM_POLICY\u0026#39;] || use_default}\u0026#34; ilm_policy_overwrite \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_ILM_POLICY_OVERWRITE\u0026#39;] || \u0026#39;false\u0026#39;}\u0026#34; id_key _hash remove_keys _hash \u0026lt;buffer\u0026gt; @type file path /var/log/fluentd/buffers-es flush_thread_count \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_BUFFER_FLUSH_THREAD_COUNT\u0026#39;] || \u0026#39;8\u0026#39;}\u0026#34; flush_interval \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_BUFFER_FLUSH_INTERVAL\u0026#39;] || \u0026#39;5s\u0026#39;}\u0026#34; chunk_limit_size \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_BUFFER_CHUNK_LIMIT_SIZE\u0026#39;] || \u0026#39;2M\u0026#39;}\u0026#34; retry_max_interval \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_BUFFER_RETRY_MAX_INTERVAL\u0026#39;] || \u0026#39;30\u0026#39;}\u0026#34; retry_forever true overflow_action \u0026#34;#{ENV[\u0026#39;FLUENT_ELASTICSEARCH_BUFFER_OVERFLOW_ACTION\u0026#39;] || \u0026#39;block\u0026#39;}\u0026#34; \u0026lt;/buffer\u0026gt; \u0026lt;/match\u0026gt; 生成configmap:\nkubectl -n $NAMESPACE create configmap fluentd-conf --from-file=fluent.conf=/tmp/fluent.conf 创建daemonset部署 #  先创建deployment部署文件:\nvi /tmp/deployment.yaml 将下面的内容拷贝进去,之后:wq:\napiVersion:apps/v1kind:DaemonSetmetadata:name:fluentdlabels:k8s-app:fluentd-loggingversion:v1spec:selector:matchLabels:k8s-app:fluentd-loggingversion:v1template:metadata:labels:k8s-app:fluentd-loggingversion:v1spec:serviceAccount:fluentdserviceAccountName:fluentdtolerations:- key:node-role.kubernetes.io/control-planeeffect:NoSchedule- key:node-role.kubernetes.io/mastereffect:NoSchedulecontainers:- name:fluentdimage:fluent/fluentd-kubernetes-daemonset:v1.15.2-debian-elasticsearch7-1.0env:- name:K8S_NODE_NAMEvalueFrom:fieldRef:fieldPath:spec.nodeName- name:FLUENT_ELASTICSEARCH_HOSTSvalue:\u0026#34;\u0026lt;es1-host\u0026gt;:\u0026lt;es1-port\u0026gt;,\u0026lt;es2-host\u0026gt;:\u0026lt;es2-port\u0026gt;,\u0026lt;es3-host\u0026gt;:\u0026lt;es3-port\u0026gt;\u0026#34;# or use# - name: FLUENT_ELASTICSEARCH_HOST# value: \u0026#34;\u0026lt;es-host\u0026gt;\u0026#34;# - name: FLUENT_ELASTICSEARCH_PORT# value: \u0026#34;\u0026lt;es-port\u0026gt;\u0026#34;- name:FLUENT_ELASTICSEARCH_USERvalue:\u0026#34;\u0026lt;es-user\u0026gt;\u0026#34;- name:FLUENT_ELASTICSEARCH_PASSWORDvalue:\u0026#34;\u0026lt;es-password\u0026gt;\u0026#34;- name:FLUENT_ELASTICSEARCH_LOG_ES_400_REASONvalue:\u0026#34;true\u0026#34;# when log formt is not json, unconmment# - name: FLUENT_CONTAINER_TAIL_PARSER_TYPE# value: \u0026#34;/^(?\u0026lt;time\u0026gt;.+) (?\u0026lt;stream\u0026gt;stdout|stderr) [^ ]* (?\u0026lt;log\u0026gt;.*)$/\u0026#34;# - name: FLUENT_CONTAINER_TAIL_PARSER_TIME_FORMAT# value: \u0026#34;%Y-%m-%dT%H:%M:%S.%N%:z\u0026#34;resources:limits:memory:600Mirequests:cpu:100mmemory:200MivolumeMounts:- name:varlogmountPath:/var/log# When actual pod logs in /var/lib/docker/containers, the following lines should be used.# - name: dockercontainerlogdirectory# mountPath: /var/lib/docker/containers# readOnly: true- name:config-volumemountPath:/fluentd/etc/fluent.confsubPath:fluent.confterminationGracePeriodSeconds:30volumes:- name:varloghostPath:path:/var/log# When actual pod logs in /var/lib/docker/containers, the following lines should be used.# - name: dockercontainerlogdirectory# hostPath:# path: /var/lib/docker/containers- name:config-volumeconfigMap:name:fluentd-confkubectl -n $NAMESPACE apply -f /tmp/deployment.yaml "},{"id":9,"href":"/posts/2209/envoy-usage/","title":"Envoy的静态配置使用方法","section":"文章","content":"Envoy静态配置 #  L4转发 #  下面的例子是配置4层转发, 将443端口的流量都代理到www.example.com对应的后端的443端口上, 如下:\nstatic_resources:listeners:- name:listener_0address:socket_address:protocol:TCPaddress:0.0.0.0port_value:443filter_chains:- filters:- name:envoy.filters.network.tcp_proxytyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxystat_prefix:tcp_443cluster:cluster_0clusters:- name:cluster_0type:LOGICAL_DNSdns_lookup_family:V4_ONLYload_assignment:cluster_name:cluster_0endpoints:- lb_endpoints:- endpoint:address:socket_address:address:www.example.comport_value:443启动Envoy #  将创建的静态配置文件envoy-custom.yaml映射到容器内部, 启动:\ndocker run -d --name=envoy --restart=always \\  -p 443:443 -v /root/envoy-custom.yaml:/etc/envoy/envoy.yaml \\  envoyproxy/envoy:v1.22.2 "},{"id":10,"href":"/posts/2208/rsync-usage/","title":"使用rsync在主机之间同步目录","section":"文章","content":"rsync安装 #  在传输双方的服务器上都安装rsync软件. 如果服务器上有rsync可以跳过.\n先检查有没有安装rsync:\nrsync -h 如果没有安装, 使用下面的命令安装:\n# Debian sudo apt-get install rsync # Red Hat sudo yum install rsync # Arch Linux sudo pacman -S rsync 启动rsync守护进程 #  rsync使用最多的是ssh模式. 在现代的公司中, 出于安全的原因, 很多ssh是被禁止使用的. 所以, 我们可以使用rsync的守护进程模式. 一起看看怎么用吧.\nrsync守护进程部署在传输双方(发送方或者接受方)的任何一端都可以的.\n下面的配置和命令中, 我以发送方(10.138.228.201)和接收方(10.206.38.30)为例.\n我选择接收方, 先部署配置文件. 配置文件地址: /etc/rsyncd.conf. 配置文件 官方参考手册\n以下是一个参考的配置, 每一项配置我都增加了备注说明:\n# 指定rsync以什么用户/组传输文件 # 默认nobody,如果使用了系统不存在的用户和组 # 需要先手动创建用户和组 # 它会是生成的文件所属的用户和组 # 也可以把它们配置到模块中 uid = root gid = root # 选择yes可以在操作模块时chroot到同步目录中 # 优势是面对安全威胁能提供额外保护 # 缺点是使用chroot需要root权限, # 以及在传输符号连接或保存用户名/组时会有些问题 use chroot = no # 指定监听端口 # 默认873 port = 873 # 最大连接数 max connections = 200 # 超时时间 timeout = 600 # 进程pid所在的文件 pid file = /var/run/rsyncd.pid # 多连接时所用的锁 lock file = /var/run/rsyncd.lock # 出错的日志文件 log file = /var/log/rsyncd.log # 忽略错误 ignore errors = true read only = false # 是否允许查看module列表 list = false # 允许的客户端IP hosts allow = 10.138.228.201 hosts deny = 0.0.0.0/32 # rsync认证用的用户 auth users = rsync # 认证用户对应的密码文件 secrets file = /etc/rsyncd.secrets # 排除目录,空格分隔 # 可以配置到特定的模块上 # 如果没有需要排除的目录可以不用写 # exclude=tmp etc # 模块的定义,它是暴露的一个目录 [test] # 需要同步的目录 # 该目录必须要是存在的,如果没有请创建 path = /opt/test/ # 模块的备注说明,展示用 comment = test 然后配置认证用户密码文件, 文件路径在上述配置文件中指定/etc/rsyncd.secrets:\necho \u0026#34;rsync:6j_ioU1xA\u0026#34; \u0026gt; /etc/rsyncd.secrets # rsync对密钥文件的权限有要求 # 仅文件拥有者可以读写 chmod 600 /etc/rsyncd.secrets 通过shell运行以下命令启动守护进程:\nrsync --daemon 查看是否启动成功, 且已经开始监听端口:\n$ ss -apnl | grep 873 tcp LISTEN 0 5 *:873 *:* users:((\u0026#34;rsync\u0026#34;,pid=20945,fd=4)) tcp LISTEN 0 5 [::]:873 [::]:* users:((\u0026#34;rsync\u0026#34;,pid=20945,fd=5)) 启动成功~\n开始传输 #  登录到发送方, 使用下面的命令开始传输目录文件:\n# 创建密码文件避免手输密码 echo \u0026#34;6j_ioU1xA\u0026#34; \u0026gt; /etc/rsync.password chmod 600 /etc/rsync.password # 开始同步文件到10.206.38.30 rsync -avzPh /opt/test/ rsync@10.206.38.30::test --password-file=/etc/rsync.password 参数说明:\n-a, --archive 存档模式,等同于 -rlptgoD (no -H,-A,-X) -r, --recursive 递归进入文件夹 -l, --links 拷贝符号连接到符号连接 -p, --perms 文件权限保持一致 -t, --times 文件修改时间保持一致 -g, --group 组保持一致 -o, --owner 用户保持一致 -D 等同于 --devices --specials --devices 设备文件保持一致 --specials 特殊文件保持一致 -v, --verbose 显示更多的输出信息 -z, --compress 传输期间压缩文件 -P 等同于 --partial --progress --progress 显示传输进度 --partial 保留部分传输(没传输完成)的文件 -h, --human-readable 以易读的格式输出数字 执行完成之后, 文件就会从当前机器拷贝到10.206.38.30上了.\n创建一个定时同步的任务, 每隔2小时同步一次:\ncat \u0026gt;\u0026gt; /var/spool/cron/root \u0026lt;\u0026lt;EOF 0 */2 * * * /bin/rsync -avzPh /opt/test/ rsync@10.206.38.30::storage --password-file=/etc/rsync.password \u0026gt; /tmp/rsync.log 2\u0026gt;\u0026amp;1 EOF "},{"id":11,"href":"/posts/2208/kubernetes-etcdctl-usage/","title":"使用etcdctl查看kubernetes存储的内容","section":"文章","content":"下面这个脚本提供了etcdctl连接etcd所需要的断点、证书相关的信息, 能快速或许并调用命令查看, 这个脚本需要在master节点上执行:\n#!/bin/bash  ENDPOINTS=$(pgrep kube-apiserver | grep -P \u0026#39;etcd-servers=(.*?)\\s\u0026#39; -o | awk -F= \u0026#39;{print $2}\u0026#39;) CACERT=$(pgrep kube-apiserver | grep -P \u0026#39;etcd-cafile=(.*?)\\s\u0026#39; -o | awk -F= \u0026#39;{print $2}\u0026#39;) CERT=$(pgrep kube-apiserver | grep -P \u0026#39;etcd-certfile=(.*?)\\s\u0026#39; -o | awk -F= \u0026#39;{print $2}\u0026#39;) KEY=$(pgrep kube-apiserver | grep -P \u0026#39;etcd-keyfile=(.*?)\\s\u0026#39; -o | awk -F= \u0026#39;{print $2}\u0026#39;) ETCDCTL_API=3 etcdctl --endpoints=\u0026#34;$ENDPOINTS\u0026#34; --cacert=\u0026#34;$CACERT\u0026#34; --key=\u0026#34;$KEY\u0026#34; --cert=\u0026#34;$CERT\u0026#34; \u0026#34;$@\u0026#34; 或者, 使用这个命令一键下载脚本:\ncurl -o ./etcdctl.sh https://blog.llaoj.cn/posts/2208/kubernetes-etcdctl-usage/etcdctl.sh / \u0026amp;\u0026amp; chmod +x ./etcdctl.sh 好了, 我们已经把证书都提前配置好了. 下面可以直接使用etcdctl.sh命令了, 比如:\n./etcdctl.sh --prefix=true get /... "},{"id":12,"href":"/posts/2208/metallb-l2-usage/","title":"MetalLB二层模式使用指南","section":"文章","content":" MetalLB概念安装配置和使用请查看\n测试组件的版本情况 #   kubernetes: v1.22.8 metellb: v0.10.3 nginx: latest  创建测试应用 #  创建一个nginx服务和service资源:\nkubectl -n without-istio create deploy nginx --image=nginx 测试分配IP #  创建loadbalancer类型的service:\nkubectl -n without-istio create service loadbalancer nginx --tcp=80:80 查看该service详细配置:\napiVersion:v1kind:Servicemetadata:labels:app:nginxnamespace:without-istioname:nginx...spec:allocateLoadBalancerNodePorts:trueclusterIP:10.233.15.89clusterIPs:- 10.233.15.89externalTrafficPolicy:ClusterinternalTrafficPolicy:ClusteripFamilies:- IPv4ipFamilyPolicy:SingleStackports:- name:80-80nodePort:30662port:80protocol:TCPtargetPort:80selector:app:nginxsessionAffinity:Nonetype:LoadBalancerstatus:loadBalancer:ingress:- ip:10.206.65.234可以发现external-ip已经完成分配.\n在同一个局域网内, 使用curl命令测试联通情况:\n 可以看到, 是可以正常访问的.\n手动指定地址池 #  默认, metallb会从所有的可用地址池中分配IP, 除非我们关闭某一个地址池的自动分配auto-assign: false.\nmetallb v0.12之前都是用configmap进行配置, 而不是用CRD. 这里是 配置相关文档.\n下面我们让集群中有两个地址池, 其中一个关闭自动分配. 修改metallb的配置文件, 增加一个address-pools(expensive), 地址范围10.206.65.224-10.206.65.233, 如下:\nkubectl -n metallb-system edit cm config 可以看到目前有两个地址池, 同时配置auto-assign: false来关闭对expensive地址池的自动分配. 配置完毕之后, metallb会重新加载配置文件, 不需要重启它.\n 下面创建一个指定地址池(loadbalanced-manual)的service, 我们看能否正确分配地址:\nkubectl create -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Service metadata: name: nginx-manual namespace: without-istio annotations: metallb.universe.tf/address-pool: expensive spec: ports: - port: 80 targetPort: 80 selector: app: nginx type: LoadBalancer EOF 继续查看刚创建的service, 发现IP地址已经成功分配, 而且地址范围也符合预期:\n 经测试访问正常:\n 手动指定IP #  现在我们看nginx-manual的external-ip是:\n 下面我手动将其ip指定为10.206.65.225, 使用如下命令:\nkubectl -n without-istio patch service nginx-manual -p \u0026#39; { \u0026#34;spec\u0026#34;: { \u0026#34;loadBalancerIP\u0026#34;: \u0026#34;10.206.65.225\u0026#34; } }\u0026#39; 请看发生的变化:\n 可见external-ip地址按照我们的要求发生了变化. 下面测试下请求连通性:\n 访问正常.\n当地址池IP不够用时 #  当前expensive地址池只有两个IP地址:\n 这两个地址已经被service占用:\n 下面指定该地址池, 创建第三个service, 我们看看会发生什么\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Service metadata: annotations: metallb.universe.tf/address-pool: expensive name: nginx-manual-2 namespace: without-istio spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: app: nginx type: LoadBalancer EOF  我们发现, 该service的EXTERNAL-IP处于pending状态, 并且会看到一条Warning级别的事件, 提示地址池中没有可分配的IP了.\n因此, 当地址池中IP数量不够用的时候, Service的EXTERNAL-IP会处于挂起状态, 并发送事件提示地址池中无可分配IP地址.\n"},{"id":13,"href":"/posts/2208/istio-with-nginx-reserve-proxy/","title":"在istio service mesh中使用nginx反向代理","section":"文章","content":"nginx反向代理的请求, 和我们直接请求有一定的区别, 比如:\nhttp version #  nginx proxy 发出的反向代理请求的http version默认是: 1.0, 但是istio支持1.1 \u0026amp; 2.0, 所以如果不增加http版本限制的话istio就无法进行报文解析, 也就无法应用istio-proxy(sidecar)L7层代理策略, 我们知道istio流量治理是基于L7层的.\nhttp header: Host #  有时候nginx发出的代理请求的http header中host的值, 不能保证是上游服务的host name. 在这种情况下, 是没办法匹配上游服务在istio-proxy中的L7流量治理的配置.\n怎么解决? #  所以, 需要在nginx代理配置处增加两项配置:\n... location / { proxy_http_version 1.1; \u0026lt;- proxy_set_header Host \u0026lt;upstream-host\u0026gt;; \u0026lt;- proxy_pass http://\u0026lt;upstream-host\u0026gt;:8080; } ... 即可.\n参考 #    nginx官方文档proxy_http_version介绍  nginx官方文档proxy_set_header介绍  "},{"id":14,"href":"/posts/2207/fluentd-es-config/","title":"Fluentd配置文件最佳实践","section":"文章","content":"Fluentd负责Kubernetes中容器日志的收集工作, 以Daemonset形式运行在每一个节点上. 下面这个配置是在多个生产集群使用的配置, 经过多次调优的. 有一些关键的配置增加了配置解释说明. 目前使用问题不大. 持续更新配置中\u0026hellip;\nkind:ConfigMapapiVersion:v1metadata:name:fluentd-es-confignamespace:logginglabels:addonmanager.kubernetes.io/mode:Reconciledata:system.conf:|-\u0026lt;system\u0026gt; root_dir /tmp/fluentd-buffers/ \u0026lt;/system\u0026gt;containers.input.conf:|-# Json Log Example: # {\u0026#34;log\u0026#34;:\u0026#34;[info:2016-02-16T16:04:05.930-08:00] Some log text here\\n\u0026#34;,\u0026#34;stream\u0026#34;:\u0026#34;stdout\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2016-02-17T00:04:05.931087621Z\u0026#34;} # CRI Log Example: # 2016-02-17T00:04:05.931087621Z stdout F [info:2016-02-16T16:04:05.930-08:00] Some log text here \u0026lt;source\u0026gt; @id fluentd-containers.log @type tail path /var/log/containers/*.log pos_file /var/log/es-containers.log.pos tag raw.kubernetes.* read_from_head true \u0026lt;parse\u0026gt; @type multi_format \u0026lt;pattern\u0026gt; format json time_key time time_format %Y-%m-%dT%H:%M:%S.%NZ \u0026lt;/pattern\u0026gt; \u0026lt;pattern\u0026gt; format /^(?\u0026lt;time\u0026gt;.+) (?\u0026lt;stream\u0026gt;stdout|stderr) [^ ]* (?\u0026lt;log\u0026gt;.*)$/ time_format %Y-%m-%dT%H:%M:%S.%N%:z \u0026lt;/pattern\u0026gt; \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt; # Detect exceptions in the log output and forward them as one log entry. \u0026lt;match raw.kubernetes.**\u0026gt; @id raw.kubernetes @type detect_exceptions remove_tag_prefix raw message log stream stream multiline_flush_interval 5 max_bytes 500000 max_lines 1000 \u0026lt;/match\u0026gt; # Concatenate multi-line logs # \u0026lt;filter kubernetes.**\u0026gt; # @id filter_concat # @type concat # key log # multiline_end_regexp /\\n$/ # separator \u0026#34;\u0026#34; # \u0026lt;/filter\u0026gt; # Enriches records with Kubernetes metadata \u0026lt;filter kubernetes.**\u0026gt; @id filter_kubernetes_metadata @type kubernetes_metadata \u0026lt;/filter\u0026gt; # 防止ES中出现重复数据 \u0026lt;filter collect-logs.**\u0026gt; @type elasticsearch_genid hash_id_key _hash # storing generated hash id key (default is _hash) \u0026lt;/filter\u0026gt;output.conf:|-# 根据pod.metadata.labels来判断是否收集日志 # collect-logs: true # 添加标识集群ID的tag: collect-logs.\u0026lt;clustername\u0026gt; \u0026lt;match kubernetes.**\u0026gt; @type rewrite_tag_filter \u0026lt;rule\u0026gt; key $.kubernetes.labels.collect-logs pattern /^true$/ tag collect-logs.\u0026lt;clustername\u0026gt; \u0026lt;/rule\u0026gt; \u0026lt;/match\u0026gt; \u0026lt;match collect-logs.**\u0026gt; @type elasticsearch @log_level info hosts 10.138.1.51:9200,10.138.1.52:9200,10.138.1.53:9200 user \u0026lt;user\u0026gt; password \u0026lt;password\u0026gt; reload_connections false reconnect_on_error true reload_on_failure true request_timeout 30s suppress_type_name true id_key _hash # specify same key name which is specified in hash_id_key remove_keys _hash # Elasticsearch doesn\u0026#39;t like keys that start with _ include_tag_key true logstash_format true \u0026lt;buffer\u0026gt; @type file path /var/log/fluentd-buffers/elasticsearch01.buffer flush_mode interval retry_type exponential_backoff flush_thread_count 2 flush_interval 5s retry_forever retry_max_interval 30 chunk_limit_size 2M # total_limit_size 500M # 默认64G, 太小会因为buffer灌满卡住并丢失数据 overflow_action block \u0026lt;/buffer\u0026gt; \u0026lt;/match\u0026gt;monitoring.conf:|-# Prometheus Exporter Plugin # input plugin that exports metrics \u0026lt;source\u0026gt; @id prometheus @type prometheus \u0026lt;/source\u0026gt; \u0026lt;source\u0026gt; @id monitor_agent @type monitor_agent \u0026lt;/source\u0026gt; # input plugin that collects metrics from MonitorAgent \u0026lt;source\u0026gt; @id prometheus_monitor @type prometheus_monitor \u0026lt;labels\u0026gt; host ${hostname} \u0026lt;/labels\u0026gt; \u0026lt;/source\u0026gt; # input plugin that collects metrics for output plugin \u0026lt;source\u0026gt; @id prometheus_output_monitor @type prometheus_output_monitor \u0026lt;labels\u0026gt; host ${hostname} \u0026lt;/labels\u0026gt; \u0026lt;/source\u0026gt; # input plugin that collects metrics for in_tail plugin \u0026lt;source\u0026gt; @id prometheus_tail_monitor @type prometheus_tail_monitor \u0026lt;labels\u0026gt; host ${hostname} \u0026lt;/labels\u0026gt; \u0026lt;/source\u0026gt;"},{"id":15,"href":"/posts/2207/kubernetes-requirement/","title":"Kubernetes 服务器配置和规划建设要求","section":"文章","content":"新建集群的第一步就是要规划服务器、网络、操作系统等等, 下面就结合我平时的工作经验总结下相关的要求, 内容根据日常工作持续补充完善:\n服务器配置 #  kubernetes 集群分为控制节点和数据节点, 它们对于配置的要求有所不同:\n控制面 #     节点规模 Master规格     1~5个节点 4核 8Gi（不建议2核 4Gi）   6~20个节点 4核 16Gi   21~100个节点 8核 32Gi   100~200个节点 16核 64Gi    系统盘40+Gi，用于储存 etcd 信息及相关配置文件等\n数据面 #   规格：CPU \u0026gt;= 4核, 内存 \u0026gt;= 8Gi 确定整个集群的日常使用的总核数以及可用度的容忍度  例如：集群总的核数有160核, 可以容忍10%的错误. 那么最小选择10台16核VM, 并且高峰运行的负荷不要超过 160*90%=144核. 如果容忍度是20%, 那么最小选择5台32核VM, 并且高峰运行的负荷不要超过160*80%=128核. 这样就算有一台VM出现故障, 剩余VM仍可以支持现有业务正常运行.   确定 CPU:Memory 比例. 对于使用内存比较多的应用, 例如Java类应用, 建议考虑使用1:8的机型 比如: virtual machine 32C 64G 200G系统盘 数据盘可选  什么情况下使用裸金属服务器? #   集群日常规模能够达到1000核。一台服务器至少96核，这样可以通过10台或11台服务器即可构建一个集群。 快速扩大较多容器。例如：电商类大促，为应对流量尖峰，可以考虑使用裸金属来作为新增节点，这样增加一台裸金属服务器就可以支持很多个容器运行。  操作系统 #   建议安装 ubuntu 18.04/debian buster/ubuntu 20.04/centos 7.9 优先级从高到低 linux kenerl 4.17+ 安装 ansible v2.9 \u0026amp; python-netaddr 以运行 ansible 命令 安装 jinja 2.11+ 以运行 ansible playbooks 允许 IPv4 forwarding 部署节点的 ssh key 拷贝到所有节点 禁用防火墙  网络 #   单一集群采用统一的网卡命名比如:eth0等, 保证名称唯一 没有特殊要求, 服务器要求可访问外网  集群规模限制 #   每节点不超过 110 pods 不超过 5k nodes 总计不超过 15w pods 总计不超过 30w containers  参考 #    优刻得容器云UK8S集群节点配置推荐  阿里云ACK容器服务之ECS选型  "},{"id":16,"href":"/posts/2205/metalb/","title":"MetalLB概念安装配置和使用","section":"文章","content":" 官方文档\n为什么使用? #  Kubernetes没有提供适用于裸金属集群的网络负载均衡器实现, 也就是LoadBalancer类型的Service. Kubernetes 附带的网络负载均衡器的实现都是调用各种 IaaS 平台（GCP、AWS、Azure ……）的胶水代码。 如果您没有在受支持的 IaaS 平台（GCP、AWS、Azure\u0026hellip;）上运行，LoadBalancers 在创建时将一直保持在pending状态。\n裸金属集群的运维人员只剩下两个方式来将用户流量引入集群内: NodePort和externalIPs. 这两种在生产环境使用有很大的缺点, 这样, 裸金属集群也就成了 Kubernetes 生态中的第二类选择, 并不是首选.\nMetalLB 的目的是实现一个网络负载均衡器来与标准的网络设备集成, 这样这些外部服务就能尽可能的正常工作了.\n要求 #  MetalLB 要求如下:\n 一个 Kubernetes 集群, Kubernetes 版本 1.13.0+, 没有网络负载均衡器功能. 可以与 MetalLB 共存的集群网络配置。 一些供 MetalLB 分发的 IPv4 地址。 当使用 BGP 操作模式时，您将需要一台或多台能够发布 BGP 的路由器。 使用 L2 操作模式时，节点之间必须允许 7946 端口（TCP 和 UDP，可配置其他端口）上的流量，这是 hashicorp/memberlist 的要求。  功能 #  MetalLB 是作为 Kubernetes 中的一个组件, 提供了一个网络负载均衡器的实现. 简单来说, 在非公有云环境搭建的集群上, 不能使用公有云的负载均衡器, 它可以让你在集群中创建 LoadBalancer 类型的 Service.\n为了提供这样的服务, 它具备两个功能: 地址分配、对外发布\n地址分配 #  在公有云的 Kubernetes 集群中, 你申请一个负载均衡器, 云平台会给你分配一个 IP 地址. 在裸金属集群中, MetalLB 来做地址分配.\nMetalLB 不能凭空造 IP, 所以你需要提供供它使用的 IP 地址池. 在 Service 的创建和删除过程中, MetalLB 会对 Service 分配和回收 IP, 这些都是你配置的地址池中的 IP.\n如何获取 MetalLB 的 IP 地址池取决于您的环境。 如果您在托管设施中运行裸机集群，您的托管服务提供商可能会提供 IP 地址供出租。 在这种情况下，您将租用例如 /26 的 IP 空间（64 个地址），并将该范围提供给 MetalLB 以用于集群服务。\n同样, 如果你的集群是纯私有的, 可以提供一个没有暴露到网络中的相邻的 LAN 网段. 这种情况下, 你可以抽取私有地址空间中的一段 IP 地址, 分配给 MetalLB. 这种地址是免费的, 只要你把它提供给当前 LAN 内的集群服务, 它们就能正常工作.\n或者你可以两者都用! MetalLB 可以让你定义多个地址池, 它很方便.\n对外发布 #  当 MetalLB 给 Service 分配一个对外使用的IP之后, 它需要让集群所在的网络知道这个 IP 的存在. MetalLB 使用标准的网络/路由协议来实现, 这要看具体的工作模式: ARP、NDP 或者 BGP.\n2层模式 (ARP/NDP) #  在2层模式下, 集群中的机器使用标准地址发现协议把这些 IPs 通知本地网络. 从 LAN 的角度看, 这台服务器有多个 IP 地址. 这个模式的详细行为还有局限性下面会介绍.\nBGP #  在 BGP 模式下, 集群中的所有机器与临近的路由器建立 BGP 对等会话, 告诉他们如何路由 Service IPs. 得益于 BGP 的策略机制, 使用 BGP 能实现真正的多节点负载均衡和细粒度的流量控制. 下面会介绍更多操作和局限性方面的细节.\n工作模式 #  2层模式 #  在2层工作模式下, 一个节点负责向本地网络发布服务. 从网络的角度看, 更像是这台服务器的网卡有多个 IP 地址. 在底层, MetalLB 会响应ARP请求(IPv4)和NDP请求(IPv6). 这个工作模式的最大的优势就是适应性强: 它能工作在任何以太网内, 没有特殊的硬件要求, 不需要花哨的路由器.\n负载均衡行为 #  在2层模式下, 发给 Service IP 的流量都会到一个节点上. 在该节点上, kube-proxy 将流量分发到服务具体的 pods 上. L2 并没有实现负载均衡.\n但是, 它实现了一个错误转移机制, 当领袖节点因为某些原因故障了, 另一个节点就会接管这些 IP 地址. 故障转移是自动的: 使用 hashicorp/memberlist 检测到节点发生故障, 同时新的节点会从故障节点上接管这些 IP.\n局限性 #  L2模式有两个主要局限性: 单点的瓶颈、潜在的缓慢故障转移.\n如上面说的, 在 L2 模式下, 选举产生的单一的领袖节点会接收所有的服务流量. 这意味着, 你服务的入口带宽受限与这个单一节点的带宽. 如果使用 ARP/NDP 引导流量, 这是一个基本限制.\n当前的实现, 节点之间的故障转移依赖客户端之间的配合. 当故障发生时, MetalLB 会不经请求的发送出一些2层的数据包, 来通知其他客户端 Service IP 所对应的 MAC 地址已经更改.\n大多数操作系统能正确处理这种数据包, 同时更新“邻居”的地址缓存. 这种情况下, 故障转移也就几秒钟. 但是, 还是有个别系统要么没有实现这种报文的处理, 要么实现了, 但是更新缓存很慢.\n好在所有现代版本的操作系统都正确实现了 L2 故障转移, 比如 Windows、Mac、Linux. 所以, 出问题的仅仅是很老或者不常见的操作系统.\n为了最大限度地减少计划内的故障转移对客户端的影响，应该让旧的领袖节点多运行几分钟, 以便它可以继续为旧客户端转发流量，直到它们的缓存刷新。\n当一个计划之外的故障出现时, 在访问出错的客户端刷新它们的缓存之前, 这些 Service IPs 将不可达.\n和 keepalive 比较 #  MetalLB 的2层模式和 keepalived 有很多相似之处. 所以, 如果你熟悉 keepalived, 我说的很多你应该很熟悉. 但是和它也有一些不同的地方需要说一下.\nKeepalived 使用虚拟路由器冗余协议(VRRP). 为了选举领袖并监控该领导者何时离开, keepalived 的各实例之间不断地相互交换 VRRP 消息。\n不一样的是, MetalLB 通过 memberlist 来知道什么时候集群中的节点不可达, 什么时候这个节点的 Service IPs 需要移动到别处.\nKeepalived 和 MetalLB 从客户端的角度看起来是一样的: 当发生故障转移时，Service IP 地址从一台机器迁移到另一台机器，之后该机器就会有多个 IP 地址。\n因为它不用 VRRP, MetalLB 并不会有这个协议本身的局限性. 比如: VRRP协议限制每个网络只能有255个负载均衡器实例, 但是 MetalLB 就没有这个限制. 只要你网络中有空闲IP, 你可以有很多负载均衡器实例.\n还有, 配置上 MetalLB 比 Keepalived 少, 比如它不需要 Virtual Router IDs.\n另一方面，由于 MetalLB 依赖于 memberlist 来获取集群成员信息，它无法与第三方 VRRP 感知路由器和基础设施进行互操作。 这是MetalLB的定位: MetalLB 专门设计用于在 Kubernetes 集群内提供负载平衡和故障转移.\nBGP模式 #  在 BGP 模式下，集群中的每个节点都会与您的网络路由器建立 BGP 对等会话，并使用该对等会话来通告外部集群服务的 IP.\n假设您的路由器配置为支持多路径，这将实现真正的负载平衡: MetalLB 发布的路由彼此等效。这意味着路由器将使用所有下一跳，并在它们之间进行负载平衡.\n数据包到达节点后，kube-proxy 负责流量路由的最后一跳，将数据包送到服务中的特定 pod.\n负载均衡行为 #  负载均衡的确切行为取决于您的特定路由器型号和配置，但常见的行为是基于 packet-hash 方法在连接层面 per-connection 进行平衡。这是什么意思?\n这个per-connection意味着单个 TCP 或 UDP 会话的所有数据包将被定向到集群中的单个机器。流量传播只发生在不同的连接之间，而不是一个连接内的数据包. 这是一件好事，因为在多个集群节点上传播数据包会导致一些问题：\n 跨多个传输路径传播单个连接会导致数据包(packet)在线路上重新排序，这会极大地影响终端主机的性能。 在 Kubernetes 中, 不能保证节点之间流量的路由保持一致。这意味着两个不同的节点可以将同一连接的数据包(packet)路由到不同的 Pod，这将导致连接失败。  高性能路由器能够以一种无状态的方式在多个后端之间使用数据包哈希的方法分发数据包. 对于每一个数据包, 它们拥有一些属性, 并能用它作为 “种子” 来决定选择哪一个后端. 如果, 所有的属性都一样, 它们就会选择同一个后端.\n具体使用哪种哈希方法取决于路由器的硬件和软件. 典型的方法是: 3-tuple 和 5-tuple. 3-tuple 哈希法使用数据包中的协议、源IP、目的IP作为哈希键, 这意味着来自不同ip的数据包会进入同一个后端. 5-tuple 哈希法又在其中加入了源端口和目的端口, 所有来自相同客户端的不同连接将会在集群中均衡分布.\n通常, 我们推荐加入尽可能多的属性来参与数据包哈希, 也就是说使用更多的属性是非常好的. 因为这样会更加接近理想的负载均衡状态, 每一个节点都会收到相同数量的数据包. 但是我们永远不会达到这种理想状态, 因为上述原因, 但是我们能做的就是尽可能的均匀的传播连接, 以防止出现主机热点.\n# 一个连接(connection)由多个连续的数据包(packet)构成 # 比如:  connection 1 : source[ip:port] -packet N-\u0026gt;...-packet 1-\u0026gt; target[ip:port] connection 2 : source[ip:port] -packet N-\u0026gt;...-packet 1-\u0026gt; target[ip:port] 局限性 #  使用 BGP 作为负载均衡机制可以让你使用标准的路由器硬件, 而不是定制的负载均衡器. 但是, 这也带来的一些缺点:\n最大的缺点是基于 BGP 的负载平衡不能优雅地响应后端设置的地址更改. 也就是说, 当集群的一个节点下线了, 到你服务的所有成功的连接都会损坏(用户会看到报错:Connection reset by peer)\n基于 BGP 的路由器实现无状态负载均衡。 他们通过哈希数据包头中的一些字段并将该哈希用作可用后端数组的索引，将给定数据包分配给特定的下一跳。\n问题是路由器中使用的哈希值通常不稳定，所以每当后端集的大小发生变化时（例如，当一个节点的 BGP 会话关闭时），现有的连接将被有效地随机重新哈希，这意味着大多数现有的 连接最终会突然被转发到不同的后端，一个不知道相关连接的后端。\n结果是每当你的服务的 IP \u0026gt; Node 映射发生变化时, 你希望看到一次性干净的切换出现, 到该服务的大部分所有可用连接中断. 没有持续的丢包或者黑洞, 只是一次很干净的中断而已.\n根据您的服务的用途，您可以采用几种缓解策略：\n 你的 BGP 路由器可能有更加稳定的ECMP哈希算法. 有时候可能叫: 弹性ECMP 或 弹性LAG. 使用这样的算法, 在后端集合发生变化的时候, 能有效的减少受影响的连接数量. 把这些需要外部访问的服务部署到特定的节点上, 来减小节点池的大小, 平时看好这些节点. 把服务的变更安排在流量的低谷时候, 此时你的用户在睡觉流量很低. 把每一个逻辑上的服务拆分成两个有着不同 IP 的 kubernetes 服务, 使用DNS服务优雅的将用户流量从将要中断的服务迁移到另一个服务上. 在客户端添加重试逻辑, 以优雅地从突然断开连接中恢复. 如果您的客户是移动应用程序或丰富的单页网络应用程序, 这尤其适用. 将您的服务放在 ingress 控制器后面. ingress 控制器本身可以使用 MetalLB 来接收流量, 但是在 BGP 和您的服务之间有一个状态层意味着您可以毫无顾虑地更改您的服务。 您只需在更改 ingress 控制器本身的部署时小心(例如, 在添加更多 NGINX pod 时). 接受偶尔会出现重置连接的情况. 对于低可用性的内部服务, 这可能是可以接受的.  FRR 模式 #  MetalLB 提供了一个实验模式: 使用 FRR 作为 BGP 层的后端.\n开启 FRR 模式之后, 会获得以下额外的特性:\n 由BFD支持的BGP会话 BGP和BFD都支持IPv6 多协议支持的BGP  FRR 模式的局限性 #  相比与原生实现, FRR 模式有以下局限性:\n BGPAdvertisement 的 RouterID 字段可以被覆盖，但它必须对所有的 advertisements 都相同（不能有不同的 advertisements 具有不同的 RouterID）。 BGPAdvertisement 的 myAsn 字段可以被覆盖，但它必须对所有 advertisements 都相同(不能有不同的 advertisements 具有不同的 myAsn) 如果 eBGP Peer 是距离节点多跳的, 则 ebgp-multihop 标志必须设置为 true  安装 #  安装之前, 确保满足所有 要求. 尤其是, 你要注意 网络附加组件的兼容性\n如果你在云平台环境运行 MetalLB, 你最好先看看 云环境兼容性页面, 确保你选择的云平台可以和 MetalLB 一起正常工作(大多数情况下都不好用).\nMetalLB 支持4种安装方式:\n 使用 Kubernetes 部署清单安装 使用 Kustomize 安装 使用 Helm 安装 使用 MetalLB Operator 安装  准备 #  如果您在 IPVS 模式下使用 kube-proxy，则从 Kubernetes v1.14.2 开始，您必须启用严格的 ARP 模式。\n 请注意，如果您使用 kube-router 作为服务代理，则不需要这个，因为它默认启用严格的 ARP。\n 你可以在集群中修改 kube-proxy 的配置文件:\nkubectl edit configmap -n kube-system kube-proxy 这样配置:\napiVersion:kubeproxy.config.k8s.io/v1alpha1kind:KubeProxyConfigurationmode:\u0026#34;ipvs\u0026#34;ipvs:strictARP:true你也可以把这个配置片段加入到你的 kubeadm-config 中, 在主配置后面使用 --- 附加上它就行.\n如果你想自动更改配置, 下面的 shell 脚本可以用:\n# 查看将会发生什么样的配置变化, 如果存在不同则返回非零返回值 kubectl get configmap kube-proxy -n kube-system -o yaml | \\ sed -e \u0026#34;s/strictARP: false/strictARP: true/\u0026#34; | \\ kubectl diff -f - -n kube-system # 实际应用变更, 仅在遇到错误的时候返回非零返回值 kubectl get configmap kube-proxy -n kube-system -o yaml | \\ sed -e \u0026#34;s/strictARP: false/strictARP: true/\u0026#34; | \\ kubectl apply -f - -n kube-system 使用部署清单安装 #  使用下面的部署清单, 来安装MetalLB:\nkubectl apply -f https://raw.githubusercontent.com/ metallb/metallb/v0.13.4/config/manifests/metallb-native.yaml  如果你想开启使用实验性的FRR模式, 使用下面的部署清单 kubectl apply -f https://raw.githubusercontent.com /metallb/metallb/v0.13.4/config/manifests/metallb-frr.yaml 注意: 这些部署清单来自开发分支. 如果应用在生产环境, 强烈推荐你使用稳定的发布版本.\n 这样, 在 metallb-system 名称空间下就部署了 MetalLB. 主要组件是:\n Deployment: metallb-system/controller, 这是集群级别的控制器, 负责处理 IP 分配. Daemonset: metallb-system/speaker, 这个组件使用你选择的协议对外发送信息, 使你的 Service 可以被访问. ServiceAccount: controller 和 speaker 所使用的, 同时配置好了组件所需要的 RBAC 权限.  该安装清单并不包含配置文件, 但是 MetalLB 组件仍会启动, 在你 部署相关配置资源之前, 它保持空闲状态.\n其他安装方式 #  另外的三种安装方式(Kustomize, Helm, MetalLB Operator)请查看 官方文档\n网络附加组件的兼容性 #  通常来说, MetalLB 并不关心你集群用什么网络附件组件, 只要它能满足 Kubernetes 要求的标准就可以.\n下面是一些网络附加组件与 MetalLB 一起测试的结果, 可以供你参考. 列表中没有的附加组件可能也可以正常工作, 只是我们没有测试.\n   网络附加组件 兼容性 备注     Antrea 可以 1.4和1.5版本测试通过   Calico 大部分可以 Calico也提供了使用BGP公布LoadBalancer IP的能力 详细   Canal 可以    Cilium 可以    Flannel 可以    Kube-ovn 可以    Kube-router 大部分可以  已知问题   Weave Net 大部分可以  已知问题    配置 #  在配置之前, MetalLB一直保持空闲状态. 想要配置 MetalLB, 需要在 MetalLB 所在的名称空间(通常是 metallb-system)部署很多和配置相关的资源.\n当然, 如果你没有把 MetalLB 部署到 metallb-system 名称空间下, 你可能需要修改下面的配置清单.\n为LoadBalancer类型服务定义可分配的IP地址 #  为了能给 Services 分配 IPs, MetalLB 通过 IPAddressPool 自定义资源来定义.\n通过 IPAddressPools 定义好 IPs 地址池之后, MetalLB 就会使用这些地址给 Services 分配 IP.\napiVersion:metallb.io/v1beta1kind:IPAddressPoolmetadata:name:first-poolnamespace:metallb-systemspec:addresses:- 192.168.10.0/24- 192.168.9.1-192.168.9.5- fc00:f853:0ccd:e799::/124可以同时定义多个 IPAddressPools 资源. 可以使用 CIDR 定义地址, 也可以使用范围区间定义, 也可以定义 IPv4 和 IPv6 地址用于分配.\n对外公布Service IPs #  一旦给 Service 分配IPs之后, 它们必须要公布出去. 具体的配置要根据你想使用的协议来定, 下面会一一介绍:\n注意: 也可以同时将一个 service 同时通过 L2 和 BGP 对外公布.\n使用 L2 模式的配置 #  配置2层模式最简单, 在多数情况下, 你不需要任何关于协议的配置, 只配置IP地址就行.\n使用2层模式不需要将IP地址绑定到工作节点的网络接口上. 它直接响应本地网络上的ARP请求，将机器的MAC地址提供给客户端。\n为了公布来自 IPAddressPool 的 IP, L2Advertisement 资源必须与 IPAddressPool 资源一起使用.\n比如, 下面的例子配置了 MetalLB 可以分配从 192.168.1.240 到 192.168.1.250 之间的地址, 并配置它使用2层模式:\napiVersion:metallb.io/v1beta1kind:IPAddressPoolmetadata:name:first-poolnamespace:metallb-systemspec:addresses:- 192.168.1.240-192.168.1.250---apiVersion:metallb.io/v1beta1kind:L2Advertisementmetadata:name:examplenamespace:metallb-system上面的例子, 在 L2Advertisement 中没有配置 IPAddressPool 选择器, 这样它能使用所有的 IP 地址.\n所以, 为了只将一部分 IPAddressPools 使用 L2 对外公布, 我们就需要声明一下(或者, 使用标签选择器).\napiVersion:metallb.io/v1beta1kind:L2Advertisementmetadata:name:examplenamespace:metallb-systemspec:ipAddressPools:- first-pool使用 BGP 模式的配置 #  需要告诉 MetalLB 如何与外部一个或多个 BGP 路由器建立会话.\n所以, 需要给每一个 MetalLB 需要连接到路由器建一个 BGPPeer 资源.\n为了能提供一个基础的 BGP 路由器配置和一个 IP 地址范围, 你需要定义4段信息.\n MetalLB 需要连接的路由器地址 路由器的AS编号 MetalLB 使用的AS编号 使用CIDR前缀表示IP地址范围  比如给MetalLB分配的AS编号为64500, 并且将它连接到地址为 10.0.0.1 且AS编号为 64501 的路由器，您的配置将如下所示：\napiVersion:metallb.io/v1beta2kind:BGPPeermetadata:name:samplenamespace:metallb-systemspec:myASN:64500peerASN:64501peerAddress:10.0.0.1提供一个IP地址池 IPAddressPool 像这样:\napiVersion:metallb.io/v1beta1kind:IPAddressPoolmetadata:name:first-poolnamespace:metallb-systemspec:addresses:- 192.168.1.240-192.168.1.250使用自定义资源BGPAdvertisement来配置MetalLB使用BGP来公布IPs:\napiVersion:metallb.io/v1beta1kind:BGPAdvertisementmetadata:name:examplenamespace:metallb-system在BGPAdvertisement的定义中, 如果没有配置IPAddressPool选择器, 默认使用所有可用的IP地址池IPAddressPools\n如果仅需要使用特定的IP地址池通过BGP进行地址公布, 则需要声明一个IP地址池列表ipAddressPools(或者使用标签选择器):\napiVersion:metallb.io/v1beta1kind:BGPAdvertisementmetadata:name:examplenamespace:metallb-systemspec:ipAddressPools:- first-pool为 BGP 会话开启 BFD 支持 #  在实验的FRR模式下, BGP会话可以由BFD会话取代，从而能实现比单纯使用BGP更快的路径故障检测的能力.\n想要开启BFD, 你必须定义个BFD配置BFDProfile, 并且和BGP会话对等方配置BGPPeer关联起来:\napiVersion:metallb.io/v1beta1kind:BFDProfilemetadata:name:testbfdprofilenamespace:metallb-systemspec:receiveInterval:380transmitInterval:270apiVersion:metallb.io/v1beta2kind:BGPPeermetadata:name:peersamplenamespace:metallb-systemspec:myASN:64512peerASN:64512peerAddress:172.30.0.3bfdProfile:testbfdprofile配置验证 #  部署时 MetalLB 会安装配置验证的钩子程序 validatingwebhook, 用来检查用户部署的自定义资源的有效性.\n但是, 因为 MetalLB 的整体配置被分隔成很多分片, 不是所有的无效配置都能被钩子程序避免. 所以, 一旦无效的 MetalLB 配置资源被成功部署了, MetalLB 会忽略它, 并使用最新的有效配置.\n未来的版本中, MetalLB 会把错误的配置暴露到 Kubernetes 资源中. 但是, 目前来说, 如果需要知道为什么配置没有生效, 就需要去查看一下控制器的日志.\n更多高级配置 #  关于地址分配、BGP、L2、calico等相关的高级配置, 请参考 官方文档\n如何使用 #  当安装并配置完 MetalLB 之后, 为了对外暴露 Service, 非常简单, 将 Service 的 spec.type 配置为 LoadBalancer, 剩下的就交给 MetalLB 就好了.\nMetalLB 会给它控制的 Service 添加一些事件, 如果你的 LoadBalancer 类型的 service 表现的不符合预期, 可以执行kubectl describe service \u0026lt;service name\u0026gt;查看事件日志.\n请求特定的IPs #  MetalLB 尊重spec.loadBalancerIP参数, 所以, 如果你想要使用一个特定的IP地址, 你可以通过配置这个参数来指定. 如果MetalLB没有你申请的IP地址, 或者如果你申请的IP地址已经被其他的服务占用了, 地址分配就会失败, MetalLB会记录一个Warning级别的事件, 你通过kubectl describe service \u0026lt;service name\u0026gt;就能看到.\nMetalLB不仅支持spec.loadBalancerIP参数, 还支持一个自定义 annotation 参数: metallb.universe.tf/loadBalancerIPs. 对于有些双栈的 Service 需要多个IPs, 这个 annotation 也支持使用逗号分隔指定多个IPs\n请注意: 在 kubernetes 的API中, spec.LoadBalancerIP参数未来计划会被废弃. 请看这\n如果你想使用特定的类型的IP, 但是不在乎具体是什么地址, MetalLB同样也支持请求一个特定的IP地址池. 为能能使用特定的地址池, 你需要在 Service 中添加一个 annotation: metallb.universe.tf/address-pool, 来指定IP地址池的名称. 比如:\napiVersion:v1kind:Servicemetadata:name:nginxannotations:metallb.universe.tf/address-pool:production-public-ipsspec:ports:- port:80targetPort:80selector:app:nginxtype:LoadBalancer流量策略 #  MetalLB 理解并尊重服务的 externalTrafficPolicy 选项，并根据您选择的策略和公告协议实现不同的公告模式。\nLayer2 #  当使用2层模式公布的时候, 集群中的一个节点会接收给 Service IP 的流量. 从那开始, 行为就取决于选择的流量策略.\nCluster流量策略 #  使用默认的Cluster流量策略, 节点上的kube-proxy接收流量并负载均衡, 并且将流量分发给 Service 对应的 Pod.\n这种策略下 pods 之间的流量是均匀分布的. 但是kube-proxy在进行负载均衡的时候会隐藏真实的源IP地址, 因此, 在 pod 的日志中会看到外部流量来自 MetalLB 的领袖节点.\nLocal流量策略 #  使用Local流量策略, 节点上的kube-proxy接收流量, 同时将流量发送给当前节点的 pod. 因为 kube-proxy 不会跨集群节点分发流量, 你的 pod 可以看到真实的源IP地址.\n这个策略的缺点是, 流量仅能流向 Service 对应的某些 pod. 那些不在领袖节点上的 Pod 是无法接收到任何流量的, 他们可以暂时当作副本存在, 当 MetalLB 发生故障转移的时候他们或许可以接收流量.\nBGP #  当通过 BGP 发布时, MetalLB尊重Service的externalTrafficPolicy选项, 按照用户选择的策略实现了两种不同的发布模式.\nCluster流量策略 #  使用默认的Cluster流量策略, 集群中的每个节点都会接收服务 IP 的流量。 在每个节点上，流量都经过第二次负载均衡（由 kube-proxy 提供），它将流量引导到各个 pod。\n此策略会在集群中的所有节点以及服务中的所有 Pod 之间实现统一的流量分布。 但是，因为存在两次负载均衡（一次在 BGP 路由器上，一次在节点上的 kube-proxy 上），这会导致流量低效。 例如，特定用户的连接可能由 BGP 路由器发送到节点 A，但随后节点 A 决定将该连接发送到运行在节点 B 上的 pod。\nCluster策略的另一个缺点是 kube-proxy 在进行负载平衡时会隐藏连接的源 IP 地址，因此在 pod 日志中会看到外部流量来自集群的节点。\nLocal流量策略 #  使用Local流量策略，只有在本地运行了一个或多个 Services 的 Pod 时, 该节点才会吸引流量。 同时 BGP 路由器仅在托管服务的那些节点之间, 对传入流量进行负载平衡。在每个节点上，流量仅通过 kube-proxy 转发到本地 Pod，节点之间没有“水平”流量。\n此策略为你的服务提供最有效的流量。此外，由于 kube-proxy 不需要在集群节点之间发送流量，因此您的 pod 可以看到传入连接的真实源 IP 地址。\n该策略的缺点是: 节点作为负载均衡的一个单元，它不管该节点上运行了多少服务的 pod。所以, 这可能会导致你的 pod 流量不平衡。\n比如，如果你有一个服务, 它在节点 A 上运行 2 个 pod，在节点 B 上运行1个 pod，则Local流量策略会将到该服务的流量平分到这两个节点(A\u0026amp;B节点各50%)。在节点A上, 又会将到达A的流量平分给2个 pod, 因此节点A上的每个 pod 的负载分配为 25%，节点B的 pod 为 50%。相反，如果您使用Cluster流量策略，每个 pod 将接收到总流量的 33%。\n一般来说，在使用 Local 流量策略时，建议对 Pod 在节点上的调度进行精细控制，例如使用节点反亲和性，从而实现 Pod 之间的流量均匀.\n将来，MetalLB 或许会解决这种流量策略的缺点，那时, 它无疑会成为 BGP 模式一起使用的最佳模式。\nIPv6和双协议栈Services #  在 L2 模式下同时支持 IPv6 和双协议栈Services，但在 BGP 模式下仅通过实验性 FRR 模式来提供支持.\n为了让 MetalLB 将 IP 分配给双栈服务，必须至少有一个IP地址池同时具有 v4 和 v6 版本的地址。\n请注意，在双协议栈Services的情况下，不能使用spec.loadBalancerIP，因为它不允许请求多个IP，因此必须使用注解 metallb.universe.tf/loadBalancerIPs。\nIP地址共享 #  默认, Services 之间不能共享IP地址. 如果你希望多个Service使用一个IP地址. 你可以在 service 上配置 annotation metallb.universe.tf/allow-shared-ip 来开启优选择的IP地址共享.\n这个 annotation 的值是一个共享 key. 下面几种情况下, Services 可以共享IP:\n 他们都有一个相同的共享 key. 他们使用的端口不一样(比如一个是 tcp/80 另一个是 tcp/443) 他们都使用Cluster外部流量策略, 或者它们都指向相同的一组 pods(比如 pod 选择器是完全相同的)  如果这些条件不满足, MetalLB可能会给两个 service 分配同一个IP, 但是也不一定. 如果你想确保多个 service 共享一个特定的IP, 使用上面提到的spec.loadBalancerIP来定义.\n以这样的方式来管理 Service 有两个主要原因: 1. 规避 Kubernetes 的限制; 2. 可使用的IP地址有限.\n下面是两个 services 共享一个IP地址的例子:\napiVersion:v1kind:Servicemetadata:name:dns-service-tcpnamespace:defaultannotations:metallb.universe.tf/allow-shared-ip:\u0026#34;key-to-share-1.2.3.4\u0026#34;spec:type:LoadBalancerloadBalancerIP:1.2.3.4ports:- name:dnstcpprotocol:TCPport:53targetPort:53selector:app:dns---apiVersion:v1kind:Servicemetadata:name:dns-service-udpnamespace:defaultannotations:metallb.universe.tf/allow-shared-ip:\u0026#34;key-to-share-1.2.3.4\u0026#34;spec:type:LoadBalancerloadBalancerIP:1.2.3.4ports:- name:dnsudpprotocol:UDPport:53targetPort:53selector:app:dns 目前 kubernetes 不支持多协议的 LoadBalancer Service. 通常, 像DNS这样的服务, 会同时监听TCP和UDP. 为了规避这个限制. 创建两个 Service(一个使用TCP, 一个使用UDP), 它们使用同样的pod选择器. 然后给他们配置相同的共享Key和spec.loadBalancerIP, 这样就可以在同一个IP地址上同时使用TCP和UDP.\n第二个原因很简单, 如果你的Service数量比IP地址数量多, 并且也搞不来更多的IP地址. 那么只能共享IP地址了.\n一些例子 #  假如, 一个电子商务平台由一个生产环境和很多沙箱环境. 生产环境需要公网IP地址, 但是沙箱环境使用私有的IP地址, 开发者通过VPN可以访问沙箱环境.\n另外, 生产的IP已经在很多地方写死了(比如, DNS、安全扫描等), 所以在生产环境中, 我们希望特定的服务使用特定的IP地址. 因为沙箱环境是由开发者开启和关闭的, 所以我们不想手动管理.\n我们可以使用 MetalLB 来满足上面的要求, 我们定义两个IP地址池, 通过定义BGP属性来控制每一个IP地址池的可见性.\napiVersion:metallb.io/v1beta1kind:IPAddressPoolmetadata:name:productionnamespace:metallb-systemspec:# 生产使用,因为公网的IP很贵,我们只有4个addresses:- 42.176.25.64/30apiVersion:metallb.io/v1beta1kind:IPAddressPoolmetadata:name:sandboxnamespace:metallb-systemspec:addresses:# 相反, 沙箱环境使用私有IP空间# 免费的而且很多, 所以我们给这个地址池分配了大量的IP# 这样开发者可以根据需要启动很多沙箱环境- 192.168.144.0/20然后我们需要公布他们, 可以通过设置BGP的一些属性来控制每一个地址集合的可见性.\napiVersion:metallb.io/v1beta1kind:BGPAdvertisementmetadata:name:externalnamespace:metallb-systemspec:ipAddressPools:- productionapiVersion:metallb.io/v1beta1kind:BGPAdvertisementmetadata:name:localnamespace:metallb-systemspec:ipAddressPools:- sandboxcommunities:- vpn-only# 我们数据中心路由器知道“vpn-only”的BGP社区。# 带有此社区标签的公告将仅通过公司VPN隧道传播回开发人员办公室。apiVersion:metallb.io/v1beta1kind:Communitymetadata:name:communitiesnamespace:metallb-systemspec:communities:- name:vpn-onlyvalue:1234:1在我们沙箱环境的 Helm 定义 charts 中, 我们给每一个 service 都打上了annotation metallb.universe.tf/address-pool: sandbox. 这样, 不管开发者什么时候创建沙箱环境, 都会从192.168.144.0/20获得一个IP地址.\n对于生产环境, 我们使用spec.loadBalancerIP参数来给 service 定义特定的IP地址.\n Copyright © The MetalLB Contributors. Copyright © 2021 The Linux Foundation ®. All rights reserved. Linux 基金会已注册商标并使用商标. "},{"id":17,"href":"/posts/2205/cgroups-process-isolation/","title":"Linux 控制组(cgroups)和进程隔离","section":"文章","content":"每个人都听过容器，但它究竟是什么？\n支持这项技术的软件有很多，其中 Docker 最为流行。因为它的可移植性和环境隔离的能力，它在数据中心内部特别流行。为了能理解这个技术，需要理解很多方面。\n注意：很多人拿容器和虚拟机比较，他们有不同的设计目标，不是替代关系，重叠度很小。容器旨在成为一个轻量级环境，您可以裸机上启动容器，托管一个或几个独立的应用程序。当您想要托管整个操作系统或生态系统或者可能运行与底层环境不兼容的应用程序时，您应该选择虚拟机。\nLinux 控制组 #  说实话，零信任环境下有些软件的确需要被控制或被限制 - 至少为了稳定，或是为了安全。很多时候一个Bug或不良代码可能会摧毁整个机器并削弱整个生态系统。还好，有办法来控制这些应用程序，控制组(cgroups)是内核的一个特性，它能限制/计量/隔离一个或者多个进程使用CPU、内存、磁盘I/O和网络。\ncgroup技术最开始是Google开发，最终在2.6.24版本（2008年1月）的内核中出现。3.15和3.16版本内核将合并进重新设计的cgroups，它添加了kernfs(拆分一些sysfs逻辑)。\ncgroups的主要设计目标是提供一个统一的接口，它可以管理进程或者整个操作系统级别的虚拟化，包含Linux容器，或者LXC。cgroups主要提供了以下能力：\n 资源限制：一个组，可以通过配置使其不能使用超过特定内存限制，或者使用超过指定数量的处理器，或者被限制使用特定的外围设备。 优先级：可以配置一个或多个组比别的组使用更少/更多的CPU或者I/O吞吐。 计量：组的资源使用是被监控和计量的。 控制：进程组可以被冻结、停止或重启。  一个 cgroup 可以由一个或多个进程组成，这些进程都绑定到同一组限制。这些组也可以是分层的，这意味着子组继承了对其父组管理的限制。\nLinux内核为cgroups提供了一系列控制器或者子系统，控制器负责给一个或者一组进程分配指定的系统资源。比如，memory控制器限制内存使用，cpuacct控制器限制cpu使用。\n您可以直接或间接访问和管理 cgroup（使用 LXC、libvirt 或 Docker），首先，我在这里通过 sysfs 和 libcgroups 库介绍。下面的例子中，需要安装必要的软件包。在Red Hat Enterprise Linux或者CentOS上，执行下面命令：\nsudo yum install libcgroup libcgroup-tools 在Ubuntu或Debian上这样安装：\nsudo apt-get install libcgroup1 cgroup-tools 这个例子中，我用一个简单的脚本(test.sh)，里面会执行一个无限循环。\n$ cat test.sh #!/bin/sh while [ 1 ]; do echo \u0026#34;hello world\u0026#34; sleep 60 done 手动方式 #  需要的软件包安装完毕之后，您可以通过 sysfs 层次结构直接配置您的 cgroup。比如，要在memory子系统下创建一个名为 foo 的 cgroup，请在 /sys/fs/cgroup/memory 中创建一个名为 foo 的目录：\nsudo mkdir /sys/fs/cgroup/memory/foo 默认情况下，每个新创建的 cgroup 都将继承对系统整个内存池的访问权限。但是，对于那些不断分配内存却不释放的应用来说，这样并不好。要将应用程序限制在合理的范围内，您需要更新 memory.limit_in_bytes 文件。\n$ echo 50000000 | sudo tee ↪/sys/fs/cgroup/memory/foo/memory.limit_in_bytes 验证配置：\n$ sudo cat memory.limit_in_bytes 50003968 注意，读到的值通常是内核页大小的倍数(page size, 4096bytes 或 4KB)。\n执行应用程序：\n$ sh ~/test.sh \u0026amp; 使用该进程PID，将其添加到memory控制器管理下，\n$ echo 2845 \u0026gt; /sys/fs/cgroup/memory/foo/cgroup.procs 使用相同的 PID 号，列出正在运行的进程，并验证它是否在期望的 cgroup 中运行：\n$ ps -o cgroup 2845 CGROUP 8:memory:/foo,1:name=systemd:/user.slice/user-0.slice/ ↪session-4.scope 您还可以通过读取指定的文件来监控该 cgroup 当前使用的资源。在这个例子中，你可能想看一下当前进程(以及派生的子进程)的内存使用量。\n$ cat /sys/fs/cgroup/memory/foo/memory.usage_in_bytes 253952 当程序不良运行 #  还是上面的例子，我们将cgroup/foo内存限制调整为 500 bytes。\n$ echo 500 | sudo tee /sys/fs/cgroup/memory/foo/ ↪memory.limit_in_bytes 注意：如果一个任务超出了其定义的限制，内核将进行干预，在某些情况下，会终止该任务。\n同样，再读这个值，因为它要是内核页大小的倍数。所以尽管你配置的是500字节，但实际上设置的是4KB。\n$ cat /sys/fs/cgroup/memory/foo/memory.limit_in_bytes 4096 启动应用，将其移动到cgroup中，并监控系统日志。\n$ sudo tail -f /var/log/messages Oct 14 10:22:40 localhost kernel: sh invoked oom-killer: ↪gfp_mask=0xd0, order=0, oom_score_adj=0 Oct 14 10:22:40 localhost kernel: sh cpuset=/ mems_allowed=0 Oct 14 10:22:40 localhost kernel: CPU: 0 PID: 2687 Comm: ↪sh Tainted: G OE ------------ 3.10.0-327.36.3.el7.x86_64 #1 Oct 14 10:22:40 localhost kernel: Hardware name: innotek GmbH VirtualBox/VirtualBox, BIOS VirtualBox 12/01/2006 Oct 14 10:22:40 localhost kernel: ffff880036ea5c00 ↪0000000093314010 ffff88000002bcd0 ffffffff81636431 Oct 14 10:22:40 localhost kernel: ffff88000002bd60 ↪ffffffff816313cc 01018800000000d0 ffff88000002bd68 Oct 14 10:22:40 localhost kernel: ffffffffbc35e040 ↪fffeefff00000000 0000000000000001 ffff880036ea6103 Oct 14 10:22:40 localhost kernel: Call Trace: Oct 14 10:22:40 localhost kernel: [\u0026lt;ffffffff81636431\u0026gt;] ↪dump_stack+0x19/0x1b Oct 14 10:22:40 localhost kernel: [\u0026lt;ffffffff816313cc\u0026gt;] ↪dump_header+0x8e/0x214 Oct 14 10:22:40 localhost kernel: [\u0026lt;ffffffff8116d21e\u0026gt;] ↪oom_kill_process+0x24e/0x3b0 Oct 14 10:22:40 localhost kernel: [\u0026lt;ffffffff81088e4e\u0026gt;] ? ↪has_capability_noaudit+0x1e/0x30 Oct 14 10:22:40 localhost kernel: [\u0026lt;ffffffff811d4285\u0026gt;] ↪mem_cgroup_oom_synchronize+0x575/0x5a0 Oct 14 10:22:40 localhost kernel: [\u0026lt;ffffffff811d3650\u0026gt;] ? ↪mem_cgroup_charge_common+0xc0/0xc0 Oct 14 10:22:40 localhost kernel: [\u0026lt;ffffffff8116da94\u0026gt;] ↪pagefault_out_of_memory+0x14/0x90 Oct 14 10:22:40 localhost kernel: [\u0026lt;ffffffff8162f815\u0026gt;] ↪mm_fault_error+0x68/0x12b Oct 14 10:22:40 localhost kernel: [\u0026lt;ffffffff816422d2\u0026gt;] ↪__do_page_fault+0x3e2/0x450 Oct 14 10:22:40 localhost kernel: [\u0026lt;ffffffff81642363\u0026gt;] ↪do_page_fault+0x23/0x80 Oct 14 10:22:40 localhost kernel: [\u0026lt;ffffffff8163e648\u0026gt;] ↪page_fault+0x28/0x30 Oct 14 10:22:40 localhost kernel: Task in /foo killed as ↪a result of limit of /foo Oct 14 10:22:40 localhost kernel: memory: usage 4kB, limit ↪4kB, failcnt 8 Oct 14 10:22:40 localhost kernel: memory+swap: usage 4kB, ↪limit 9007199254740991kB, failcnt 0 Oct 14 10:22:40 localhost kernel: kmem: usage 0kB, limit ↪9007199254740991kB, failcnt 0 Oct 14 10:22:40 localhost kernel: Memory cgroup stats for /foo: ↪cache:0KB rss:4KB rss_huge:0KB mapped_file:0KB swap:0KB ↪inactive_anon:0KB active_anon:0KB inactive_file:0KB ↪active_file:0KB unevictable:0KB Oct 14 10:22:40 localhost kernel: [ pid ] uid tgid total_vm ↪rss nr_ptes swapents oom_score_adj name Oct 14 10:22:40 localhost kernel: [ 2687] 0 2687 28281 ↪347 12 0 0 sh Oct 14 10:22:40 localhost kernel: [ 2702] 0 2702 28281 ↪50 7 0 0 sh Oct 14 10:22:40 localhost kernel: Memory cgroup out of memory: ↪Kill process 2687 (sh) score 0 or sacrifice child Oct 14 10:22:40 localhost kernel: Killed process 2702 (sh) ↪total-vm:113124kB, anon-rss:200kB, file-rss:0kB Oct 14 10:22:41 localhost kernel: sh invoked oom-killer: ↪gfp_mask=0xd0, order=0, oom_score_adj=0 [ ... ] 注意，一旦应用程序使用内存达到 4KB 限制，内核的 Out-Of-Memory Killer（或 oom-killer）就会介入。它杀死了应用程序。您可以下面的方式来验证这一点：\n$ ps -o cgroup 2687 CGROUP 使用 libcgroup #  libcgroup软件包提供了简单的管理工具，上面很多操作步骤都可以用它实现。例如，使用cgcreate命令可以创建sysfs条目和文件。\n在memory子系统下创建名字为foo的组，使用下面命令：\n$ sudo cgcreate -g memory:foo 注意：libcgroup 提供了一种用于管理控制组中的任务的机制。\n使用与之前相同的方法，设置阈值：\n$ echo 50000000 | sudo tee ↪/sys/fs/cgroup/memory/foo/memory.limit_in_bytes 验证配置：\n$ sudo cat memory.limit_in_bytes 50003968 使用 cgexec 命令在 cgroup/foo 下运行应用程序：\n$ sudo cgexec -g memory:foo ~/test.sh 使用它的 PID，验证应用程序是否在 cgroup 和定义的memory管理器下运行：\n$ ps -o cgroup 2945 CGROUP 6:memory:/foo,1:name=systemd:/user.slice/user-0.slice/ ↪session-1.scope 如果您的应用程序不再运行，并且您想要清理并删除 cgroup，您可以使用 cgdelete。要从memory控制器下删除组 foo，请键入：\n$ sudo cgdelete memory:foo 持久组 #  通过简单的配置文件来启动服务，也可以完成上面的工作。你可以在/etc/cgconfig.conf文件中定义所有cgroup名字和属性。下面的例子中配置了foo组和它的一些属性。\n$ cat /etc/cgconfig.conf # # Copyright IBM Corporation. 2007 # # Authors: Balbir Singh \u0026lt;balbir@linux.vnet.ibm.com\u0026gt; # This program is free software; you can redistribute it # and/or modify it under the terms of version 2.1 of the GNU # Lesser General Public License as published by the Free # Software Foundation. # # This program is distributed in the hope that it would be # useful, but WITHOUT ANY WARRANTY; without even the implied # warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR # PURPOSE. # # 默认，我们希望 systemd 默认加载所有内容 # 所以没啥可做的 # 详细内容查看 man cgconfig.conf # 了解如何在系统启动时使用该文件创建 cgroup group foo { cpu { cpu.shares = 100; } memory { memory.limit_in_bytes = 5000000; } } cpu.shares定义了cgroup的CPU优先级。默认，所有的组继承 1024 shares 或者说 100% CPU使用时间。降低该值，比如 100，该组将被限制在大约 10% CPU使用时间。\n如前所述，cgroup 中的进程也可以被限制使用CPUs(core)数量，把下面的内容添加到cgconfig.conf文件相应的cgroup下：\ncpuset { cpuset.cpus=\u0026#34;0-5\u0026#34;; } 它将限制该cgroup使用索引为0到5的核心(core)，即仅能使用前6个CPU核心。\n下面，需要使用cgconfig服务加载该配置文件。首先，配置cgconfig开机自启动加载上面的配置文件。\n$ sudo systemctl enable cgconfig Create symlink from /etc/systemd/system/sysinit.target.wants/ ↪cgconfig.service to /usr/lib/systemd/system/cgconfig.service. 现在，手动启动服务加载配置文件（或者直接重启操作系统）\n$ sudo systemctl start cgconfig 在cgroup/foo下，启动应用，并将其和它的memory、cpuset和cpu限制进行绑定：\n$ sudo cgexec -g memory,cpu,cpuset:foo ~/test.sh \u0026amp; 除了将应用启动到预定义的cgroup中之外，剩下的操作系统重启后会一直存在。但是，你可以通过写一个依赖cgconfig服务的开机初始化脚本来启动应用。\n"},{"id":18,"href":"/posts/2204/kubernetes-node-load/","title":"分析告警 kubernetes 节点 load 过高问题","section":"文章","content":"负载过高分析 #  通过 linux 提供的几个命令可以从不同的纬度分析系统负载。\nvmstat #  这命令能从一个系统的角度反应出服务器情况，报告虚拟内存统计信息，报告有关进程、内存、分页、块的信息 IO、陷阱、磁盘和 CPU 活动。看个例子：\n$ vmstat --wide --unit M 5 procs ----------------memory---------------- ---swap--- -----io---- ---system--- ---------cpu-------- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 1 0 127691 1535 73572 0 0 0 3 0 0 2 1 97 0 0 93 0 0 127674 1535 73573 0 0 0 80 49267 67634 5 1 94 1 0 0 2 0 127679 1535 73573 0 0 0 66 38537 56283 3 1 95 1 0 2 2 0 127738 1535 73574 0 0 6 86 41769 61823 5 1 93 2 0 2 0 0 127729 1535 73574 0 0 18 18 41002 59214 4 1 95 0 0 命令以及输出解释：\nvmstat - --wide 宽幅展示 比较易读 - --unit 输出单位，可以是 1000(k)、1024(K)、1000000(m) 或 1048576(M) 个字节 procs - r: 可运行 runnable 进程数量（包括运行中 running 或者等待中 waiting 的进程） - b: 等待 i/o 完成的阻塞 blocked 进程数 memory 显示单位受 --unit 影响 - swpd: 交换 swap 内存使用量 - free: 空闲 idle 内存数 - buff: 用作缓冲区 buffers 的内存量 - cache: 用作缓存 cache 的内存量 swap 显示单位受 --unit 影响 - si: 每秒从磁盘换入 swapped 的内存量 - so: 每秒交换 swapped 到磁盘的内存量 io - bi: 从块设备接收的块 block (blocks/s) - bo: 发送到块设备到块 block (blocks/s) system - in：每秒的中断数，包括时钟 clock - cs：每秒上下文 context 切换的次数。 cpu 以下都是 cpu 总时间的百分比 - us: 非内核代码运行耗时 (user time \u0026amp; nice time) - sy: 内核代码运行耗时 (系统耗时) - id: 空闲 idle 时间，从 Linux 2.5.41开始它包含 i/o 等待时间 - *wa: io 等待时间，从 Linux 2.5.41开始它包含在 idle 中 - *st: 从虚拟机中窃取时间，未知 这个例子中，可见 cpu 空闲 idle 时间占比 90% 以上，说明 i/o 等待时间很高。\niostat #  cpu 统计报告 \u0026amp; 设备/分区 input/output 统计报告。\n$ iostat -m 2 3 Linux 5.4.108-1.el7.elrepo.x86_64 (node27) 04/28/2022 _x86_64_ (80 CPU) avg-cpu: %user %nice %system %iowait %steal %idle 2.35 0.05 0.64 0.21 0.00 96.75 Device: tps MB_read/s MB_wrtn/s MB_read MB_wrtn sdb 5.13 0.00 0.19 3477 5627354 sda 3.10 0.00 0.03 1890 738773 avg-cpu: %user %nice %system %iowait %steal %idle 1.78 0.00 0.57 1.30 0.00 96.34 Device: tps MB_read/s MB_wrtn/s MB_read MB_wrtn sdb 0.00 0.00 0.00 0 0 sda 0.50 0.00 0.00 0 0 avg-cpu: %user %nice %system %iowait %steal %idle 1.71 0.01 0.61 0.94 0.00 96.72 Device: tps MB_read/s MB_wrtn/s MB_read MB_wrtn sdb 0.00 0.00 0.00 0 0 sda 0.00 0.00 0.00 0 0 报告解读：\niostat -m 2 3 - -m: 以兆为单位显示 - -x: 展示扩展信息 - 每隔2秒输出一次报告，总共输出3次。 - 第一份报告是系统自启动以来的统计信息，每个后续报告都是上次报告以来的统计信息。 avg-cpu cpu 使用率报告，对于多核心系统，这里的值是全局平均值。下面每一项都是对比 cpu 总时间的使用率。 - %user: 执行用户级别应用所用的时间 - %nice: 执行具有 nice 优先级的用户级别程序所用的时间 - %system: 执行系统级别（内核）程序所用时间 - %iowait: cpu/cpus 空闲等待磁盘 i/o 请求所用的时间 - %steal: 当虚拟机管理器正服务另一个虚拟处理器时，虚拟 cpu/cpus 的被迫等待时间（被偷走的时间） - %idle: 系统没有 i/o 请求时的 cpu/cpus 空闲时间 下面是设备使用率报告 展示每一个物理设备/分区的统计信息 - tps: 每秒向设备发出的传输次数。一次传输就是一个 i/o 请求，多个逻辑请求可以合并成一次 i/o 请求。 - MB_read/s, MB_wrtn/s: 每秒读取/写入设备的数据量，单位 M - MB_read, MB_wrtn: 读取/写入设备的总数据量 从例子中，可见 cpu %idle 占比 90% 以上，说明 cpu 花了绝大多数时间在等待 i/o。设备的读写数据几乎没有，说明这些 i/o 并不是来自系统物理设备/分区。有可能来自挂载的网络文件存储设备。\nps #  当前进程信息快照，通过下面的命令找出存在大量 io 的进程\n$ ps -e -L o state,pid,cmd | grep \u0026#34;^[R|D]\u0026#34; | sort | uniq -c | sort -k 1nr 41 R 75319 /bin/node_exporter... 命令解释：\n-e every 输出所有进程 o 自定义输出列 逗号分割 -L 展示线程 进程状态码介绍 - R 在执行队列中的，正在运行中或者可以运行的 running or runnable - D 不可中断的睡眠 (通常是 i/o) - S 可以中断的睡眠 (正在等待某个事件完成) - I 空闲 Idle 的内核线程 - s session leader - \u0026lt; 高优先级 - + 在前台进程组中 - l 是多线程的 使用下面的命令找出占用 io 的 pod uid\n$ cat /proc/75319/cgroup | awk -F \u0026#34;/\u0026#34; \u0026#39;{print $4}\u0026#39; pode0c67fad-9fab-4f35-87b3-d918b5f09882 ... $ kubectl get pods --all-namespaces \\  -ocustom-columns=NS:metadata.namespace,Name:metadata.name,UID:metadata.uid \\  | grep e0c67fad-9fab-4f35-87b3-d918b5f09882 "},{"id":19,"href":"/posts/2204/apisix-ingress-controller-design/","title":"Apisxi Ingress Controller 设计说明","section":"文章","content":"Apache APISIX - 专门为 kubernetes 研发的入口控制器。\n状态 #  该项目目前是 general availability 级别\n先决条件 #  apisix-ingress-controller 要求 kubernetes 版本 1.16+. 因为使用了 CustomResourceDefinition v1 stable 版本的 API. 从 1.0.0 版本开始，APISIX-ingress-controller 要求 Apache APISIX 版本 2.7+.\n功能特性 #   使用 Custom Resource Definitions(CRDs) 对 Apache APISIX 进行声明式配置，使用 kubernetes yaml 结构最小化学习成本。 Yaml 配置热加载 支持原生 Kubernetes Ingress (v1 和 v1beta1) 资源 Kubernetes endpoint 自动注册到 Apache APISIX 上游节点 支持基于 POD（上游节点） 的负载均衡 开箱支持上游节点健康检查 扩展插件支持热配置并且立即生效 支持路由的 SSL 和 mTLS 支持流量切割和金丝雀发布 支持 TCP 4层代理 Ingress 控制器本身也是一个可插拔的热加载组件 多集群配置分发   这里有一份在线竞品分析表格\nApache APISIX Ingress vs. Kubernetes Nginx Ingress #   yaml 配置热加载 更方便的金丝雀发布 配置验证，安全可靠 丰富的插件和生态, 插件列表 支持 APISIX 自定义资源和原生 kubernetes ingress 资源 更活跃的社区  设计原理 #  架构 #  apisix-ingress-controller 需要的所有配置都是通过 Kubernetes CRDs (Custom Resource Definitions) 定义的。支持在 Apache APISIX 中配置插件、上游的服务注册发现机制、负载均衡等。\napisix-ingress-controller 是 Apache APISIX 的控制面组件. 当前服务于 Kubernetes 集群。 未来, 计划分离出子模块以适配更多的部署模式，比如虚拟机集群部署。\n整体架构图如下：\n 这是一张内部架构图：\n 时序/流程图 #  apisix-ingress-controller 负责和 Kubernetes Apiserver 交互, 申请可访问资源权限（RBAC），监控变化，在 Ingress 控制器中实现对象转换，比较变化，然后同步到 Apache APISIX。\n 这是一张流程图，介绍了ApisixRoute和其他CRD在同步过程中的主要逻辑\n 结构转换 #  apisix-ingress-controller 给 CRDs 提供了外部配置方法。它旨在务于需要日常操作和维护的运维人员，他们需要经常处理大量路由配置，希望在一个配置文件中处理所有相关的服务，同时还希望能具备便捷和易于理解的管理能力。但是，Apache APISIX 则是从网关的角度设计的，并且所有的路由都是独立的。这就导致了两者在数据结构上存在差异。一个注重批量定义，一个注重离散实现。\n考虑到不同人群的使用习惯，CRDs 的数据结构借鉴了 Kubernetes Ingress 的数据结构，数据结构基本一致。 关于这两者的差别，请看下面这张图：\n 可以看到，它们是多对多的关系。因此，apisix-ingress-controller 必须对 CRD 做一些转换，以适应不同的网关。\n规则比较 #  seven 模块内部保存了内存数据结构，目前与Apache APISIX资源对象非常相似。当 Kubernetes 资源对象有新变化时，seven 会比较内存对象，并根据比较结果进行增量更新。\n目前的比较规则是根据route/service/upstream资源对象的分组，分别进行比较，发现差异后做出相应的广播通知。\n 服务发现 #  根据 ApisixUpstream 中定义的 namespace name port 字段，apisix-ingress-controller 会在 Apache APISIX Upstream 中注册 处于 running 状态的 endpoints 节点，并且根据 kubernetes endpoints 状态进行实时同步。\n基于服务发现，apisix-ingress-controller 可以直接访问后端 pod，绕过 Kubernetes Service，可以实现自定义的负载均衡策略。\nAnnotation 实现 #  不像 Kubernetes Nginx Ingress Controller，apisix-ingress-controller 的 annotation 实现是基于 Apache APISIX 的插件机制的。\n比如，可以通过在ApisixRoute资源对象中设置k8s.apisix.apache.org/whitelist-source-rangeannotation来配置白名单。\napiVersion:apisix.apache.org/v2beta3kind:ApisixRoutemetadata:annotations:k8s.apisix.apache.org/whitelist-source-range:1.2.3.4,2.2.0.0/16name:httpserver-routespec:...黑/白名单功能是通过 ip-restriction插件来实现的。\n为方便的定义一些常用的配置，未来会有更多的 annotation 实现，比如CORS。\nApisixRoute 介绍 #  ApisixRoute 是一个 CRD 资源，它关注如何将流量发送到后端，它有很多 APISIX 支持的特性。相比 Ingress，功能实现的更原生，语意更强。\n基于路径的路由规则 #  URI 路径总是用于拆分流量，比如访问 foo.com 的请求， 含有 /foo 前缀请求路由到 foo 服务，访问 /bar 的请求要路由到 bar 服务。以 ApisixRoute 方式配置应该是这样的：\napiVersion:apisix.apache.org/v2beta3kind:ApisixRoutemetadata:name:foo-bar-routespec:http:- name:foomatch:hosts:- foo.compaths:- \u0026#34;/foo*\u0026#34;backends:- serviceName:fooservicePort:80- name:barmatch:paths:- \u0026#34;/bar\u0026#34;backends:- serviceName:barservicePort:80有prefix和exact两种路径类型可用 默认exact，当需要前缀匹配的时候，就在路径后加 * 比如 /id/* 能匹配所有带 /id/ 前缀的请求。\n高级路由特性 #  基于路径的路由是最普遍的，但这并不够， 再试一下其他路由方式，比如 methods 和 exprs\nmethods 通过 HTTP 动作来切分流量，下面例子会把所有 GET 请求路由到 foo 服务（kubernetes service）\napiVersion:apisix.apache.org/v2beta3kind:ApisixRoutemetadata:name:method-routespec:http:- name:methodmatch:paths:- /methods:- GETbackends:- serviceName:fooservicePort:80exprs允许用户使用 HTTP 中的任意字符串来配置匹配条件，例如query、HTTP Header、Cookie。它可以配置多个表达式，而这些表达式又由主题(subject)、运算符(operator)和值/集合(value/set)组成。比如：\napiVersion:apisix.apache.org/v2beta3kind:ApisixRoutemetadata:name:method-routespec:http:- name:methodmatch:paths:- /exprs:- subject:scope:Queryname:idop:Equalvalue:\u0026#34;2143\u0026#34;backends:- serviceName:fooservicePort:80上面是绝对匹配，匹配所有请求的 query 字符串中 id 的值必须等于 2143。\n服务解析粒度 #  默认，apisix-ingress-controller 会监听 service 的引用，所以最新的 endpoints 列表会被更新到 Apache APISIX。同样 apisix-ingress-controller 也可以直接使用 service 自身的 clusterIP。如果这正是你想要的，配置 resolveGranularity: service(默认endpoint). 如下：\napiVersion:apisix.apache.org/v2beta3kind:ApisixRoutemetadata:name:method-routespec:http:- name:methodmatch:paths:- /*methods:- GETbackends:- serviceName:fooservicePort:80resolveGranularity:service基于权重的流量切分 #  这是 APISIX Ingress Controller 一个非常棒的特性。一个路由规则中可以指定多个后端，当多个后端共存时，将应用基于权重的流量拆分（实际上是使用Apache APISIX中的流量拆分 traffic-split 插件）您可以为每个后端指定权重，默认权重为 100。比如：\napiVersion:apisix.apache.org/v2beta3kind:ApisixRoutemetadata:name:method-routespec:http:- name:methodmatch:paths:- /*methods:- GETexprs:- subject:scope:Headername:User-Agentop:RegexMatchvalue:\u0026#34;.*Chrome.*\u0026#34;backends:- serviceName:fooservicePort:80weight:100- serviceName:barservicePort:81weight:50上面有一个路由规则（1.所有GET /*请求 2.Header中有匹配 User-Agent: .*Chrome.* 的条目）它有两个后端服务 foo、bar，权重是100：50，意味着有2/3的流量会进入 foo，有1/3的流量会进入bar。\n插件 #  Apache APISIX 提供了 40 多个插件，可以在 APIsixRoute 中使用。所有配置项的名称与 APISIX 中的相同。\napiVersion:apisix.apache.org/v2beta3kind:ApisixRoutemetadata:name:httpbin-routespec:http:- name:httpbinmatch:hosts:- local.httpbin.orgpaths:- /*backends:- serviceName:fooservicePort:80plugins:- name:corsenable:true为到 local.httpbin.org 的请求都配置了 Cors 插件\nWebsocket 代理 #  创建一个 route，配置特定的 websocket 字段，就可以代理 websocket 服务。比如：\napiVersion:apisix.apache.org/v2beta3kind:ApisixRoutemetadata:name:ws-routespec:http:- name:websocketmatch:hosts:- ws.foo.orgpaths:- /*backends:- serviceName:websocket-serverservicePort:8080websocket:trueTCP 路由 #  apisix-ingress-controller 支持基于端口的 tcp 路由\napiVersion:apisix.apache.org/v2beta3kind:ApisixRoutemetadata:name:tcp-routespec:stream:- name:tcp-route-rule1protocol:TCPmatch:ingressPort:9100backend:serviceName:tcp-serverservicePort:8080进入 apisix-ingress-controller 9100 端口的 TCP 流量会路由到后端 tcp-server 服务。\n注意： APISIX不支持动态监听，所以需要在APISIX 配置文件中预先定义9100端口。\nUDP 路由 #  apisix-ingress-controller 支持基于端口的 udp 路由\napiVersion:apisix.apache.org/v2beta3kind:ApisixRoutemetadata:name:udp-routespec:stream:- name:udp-route-rule1protocol:UDPmatch:ingressPort:9200backend:serviceName:udp-serverservicePort:53进入 apisix-ingress-controller 9200 端口的 TCP 流量会路由到后端 udp-server 服务。\n注意： APISIX不支持动态监听，所以需要在APISIX 配置文件中预先定义9200端口。\nApisixUpstream 介绍 #  ApisixUpstream 是 kubernetes service 的装饰器。它设计成与其关联的 kubernetes service 的名字一致，将其变得更加强大，使该 kubernetes service 能够配置负载均衡策略、健康检查、重试、超时参数等。\n通过 ApisixUpstream 和 kubernetes service，apisix-ingress-controller 会生成 APISIX Upstream(s).\n配置负载均衡 #  需要适当的负载均衡算法来合理地分散 Kubernetes Service 的请求\napiVersion:apisix.apache.org/v1kind:ApisixUpstreammetadata:name:httpbinspec:loadbalancer:type:ewma---apiVersion:v1kind:Servicemetadata:name:httpbinspec:selector:app:httpbinports:- name:httpport:80targetPort:8080上面这个例子给 httpbin 服务配置了 ewma 负载均衡算法。有时候可能会需要会话保持，你可以配置一致性哈希负载均衡算法\napiVersion:apisix.apache.org/v1kind:ApisixUpstreammetadata:name:httpbinspec:loadbalancer:type:chashhashOn:headerkey:\u0026#34;user-agent\u0026#34;这样 apisix 就会根据 user-agent header 来分发流量。\n配置健康检查 #  尽管 kubelet 已经提供了检测 pod 健康的探针机制。你可能还需要更加丰富的健康检查机制，比如被动健康检查机制。\napiVersion:apisix.apache.org/v1kind:ApisixUpstreammetadata:name:httpbinspec:healthCheck:passive:unhealthy:httpCodes:- 500- 502- 503- 504httpFailures:3timeout:5sactive:type:httphttpPath:/healthztimeout:5shost:www.foo.comhealthy:successes:3interval:2shttpCodes:- 200- 206上面的yaml片段定义了被动健康检查器来检查endpoints的健康状况。一旦连续三次请求的响应状态码是错误（500 502 503 504 中的一个），这个endpoint就会被标记成不健康并不会再给它分配流量了，直到它再次健康。\n所以，主动健康检查器就出现了。endpoint 可能掉线一段时间又回复健康，主动健康检查器主动探测这些不健康的endpoints，一旦满足健康条件就将其恢复为健康（条件：连续三次请求响应状态码为200或206）\n 注意：主动健康检查器在某种程度上与 liveness/readiness 探针重复，但如果使用被动健康检查机制，则它是必需的。因此，一旦您使用了 ApisixUpstream 中的健康检查功能，主动健康检查器是强制性的。\n 配置重试和超时 #  当请求出现错误，比如网络问题或者服务不可用当时候，你可能想重试请求。默认重试次数是1，通过定义retries字段可以改变这个值。\n下面这个例子将retries定义为3，表明会对kubernetes service/httpbin的endpoints最多请求3次。\n 注意：只有在尚未向客户端响应任何内容的情况下，才有可能将请求重试传递到下一个端点。也就是说，如果在传输响应的过程中发生错误或超时，就不会重试了。\n apiVersion:apisix.apache.org/v1kind:ApisixUpstreammetadata:name:httpbinspec:retries:3默认，connect、send 和 read 的超时时间是60s，这可能对有些应用不合适，修改timeout字段来改变默认值。\napiVersion:apisix.apache.org/v1kind:ApisixUpstreammetadata:name:httpbinspec:timeout:connect:5sread:10ssend:10s上面例子将connect、read 和 send 分别设置为 5s、10s、10s。\n端口级别配置 #  有时，单个 kubernetes service 可能会暴露多个端口，这些端口提供不同的功能并且需要不同的上游配置。在这种情况下，您可以为单个端口创建配置。\napiVersion:apisix.apache.org/v1kind:ApisixUpstreammetadata:name:foospec:loadbalancer:type:roundrobinportLevelSettings:- port:7000scheme:http- port:7001scheme:grpc---apiVersion:v1kind:Servicemetadata:name:foospec:selector:app:fooportLevelSettings:- name:httpport:7000targetPort:7000- name:grpcport:7001targetPort:7001foo 服务暴露的两个端口，一个使用http协议，另一个使用grpc协议。同时，ApisixUpstream/foo 为7000端口配置http协议，为7001端口配置grpc协议（所有端口都是service端口），两个端口都共享使用同一个负载均衡算法。\n如果服务仅公开一个端口，则 PortLevelSettings 不是必需的，但在定义多个端口时很有用。\n用户故事 #   思必驰：为什么我们重新写了一个 k8s ingress controller？\n腾讯云：为什么选择 apisix 实现 kubernetes ingress controller\n"},{"id":20,"href":"/posts/2204/announcing-grafana-mimir/","title":"Grafana Mimir 发布 目前最具扩展性的开源时序数据库","section":"文章","content":"Mimir 简介 #  Grafana Mimir 是目前最具扩展性、性能最好的开源时序数据库，Mimir 允许你将指标扩展到 1 亿。它部署简单、高可用、多租户支持、持久存储、查询性能超高，比 Cortex 快 40 倍。 Mimir 托管在 https://github.com/grafana/mimir 并在 AGPLv3 下获得许可。\n B站：Grafana Mimir 发布 目前最具扩展性的开源时序数据库\nMimir 是指标领域的一个新项目，站在巨人的肩膀上。为了理解 Mimir，我们需要回顾一下 Cortex 的历史。\n源自 Prometheus #  2016 年在 Weaveworks 工作时，我与 Prometheus 的联合创始人兼维护者 Julius Volz 一起启动了 Cortex 项目。该项目的目标是构建一个可扩展的与 Prometheus 兼容的解决方案，旨在作为 SaaS 产品运行。在我加入 Grafana Labs 后，我们与 Weaveworks 合作，将 Cortex 转移到一个中立的地方，即云原生计算基金会。 Cortex 于 2018 年 9 月 20 日被接受为 CNCF 沙盒项目，两年后 晋升为孵化项目。CNCF 为两个公司在项目上提供了一个公平的竞争协作环境，这确实很棒，Grafana Labs 和 Weaveworks 都积极参与其中。Cortex 被 20 多个组织使用，并得到了 大约 100 名开发人员的贡献。 Grafana Labs 的员工无疑是 Cortex 项目的最大贡献者，在 2019 - 2021 年期间贡献了约 87% 的代码提交。\n 来源: cortex.devstats.cncf.io\n开源和商业 #  来看看这些产品 Cortex、Loki、Tempo 和 Grafana Enterprise Metrics\n过去，Cortex 已经成为很多项目的基础，包括 Grafana Loki（类似 Prometheus，用于日志）、Grafana Tempo（用于分布式追踪）、Grafana Enterprise Metrics（GEM）。Grafana Labs 于 2020 年发布该项目，让 Prometheus 能适应更大的组织、加入很多企业级特性（比如安全、访问控制、简化管理UI），旨在他们卖给那些不想自己构建但还想使用这类产品的企业。\n同时，云服务商和 ISVs（独立软件开发商）也推出了基于 Cortex 的产品，但是对项目却没啥贡献。一家公司，通过创造技术来降低其他公司的成本，但是却对开源技术不感兴趣。这是不可持续并且非常不好的。为了回应，我们后面更偏向于对 GEM 投资而不是 Cortex。作为一家热衷于开源的公司，这一点让大家很不舒服。我们认为，GEM 中一些可扩展性相关和性能相关的特性应该被开源。\n大家应该知道，去年我们 重新授权了一些开源项目，把 Grafana, Grafana Loki 和 Grafana Tempo, 从 Apache 2.0 调整到 AGPLv3（OSI 批准的许可证，保留了开源自由，同时鼓励第三方将代码贡献回社区）从 Grafana Labs 开创之初，我们的目标就是要围绕我们的开源项目构建可持续发展的商业，将商业产品的收入重新投入到开源技术和社区。AGPL 许可能平衡商业和开源之间的关系。\n介绍 Grafana Mimir #  Mimir 集合了 Cortex 中的最佳功能和为 GEM \u0026amp; Grafana Cloud 大规模运行而研发的功能，所有这些都在 AGPLv3 许可下。Mimir 包含以前的商业功能，包括无限制基数（使用水平可扩展的 “split” 压缩器实现）和快速、高基数查询（使用分片查询引擎实现）\n产品比较 #  Cortex、Grafana Mimir 和 Grafana Cloud \u0026amp; Grafana Enterprise Metrics 比较\n 在从 Cortex 开始构建 Mimir 的过程中，团队有机会消除五年来欠下的技术债务，删除未使用的功能，使项目更易于维护，简化配置并改进文档。希望通过这次投资，在 Mimir 上的努力会让其更加易用，从而帮助社区更好的发展。\n对于 Grafana Cloud 和 Grafana Enterprise Metrics 的用户来说，没有任何变化，因为这两种产品从几个月前就都基于 Grafana Mimir。对于正使用 Cortex 的组织，在一定程度的主版本升级限制内，Mimir 可以作为替代品。大多数情况下，从 Cortex 迁移到 Mimir只需不到 10 分钟。\n指标的未来 #  Mimir 的愿景不是成为“最具可扩展性的普罗米修斯”，而是“最具可扩展性的泛指标时序数据库”。用户无需更改代码即可将指标发送到 Mimir。今天，Mimir 可以原生使用 Prometheus 指标。很快 Influx、Graphite、OpenTelemetry 和 Datadog 将紧随其后。这是我们“大帐篷”理念的一部分：正如 Grafana 是可视化所有数据的一体化工具一样，Mimir 可以成为存储所有指标的一体化工具。\nMimir 发布以后，强大、全面、可插拔的开源观测工具栈已经形成：LGTM（Loki 用户日志, Grafana 用于可视化, Tempo 用于跟踪, Mimir 用于指标），快去体验吧。\n想了解更多，阅读 Q\u0026amp;A with our CEO, Raj Dutt，注册4月26日网络研讨会 介绍 Grafana Mimir，能扩展1亿指标的开源的时序数据库，不仅如此\n"},{"id":21,"href":"/posts/2203/execjava-versioninpod/","title":"在 kubernetes 中找出使用 jdk9 及以上版本的应用","section":"文章","content":"一个漏洞 #  近日，在 Spring Cloud (SPEL) 中发现 RCE 0-day 漏洞，发布在Ots安全中，公众号文章如下:\n  为了排查 kubernetes 中所有存在安全威胁的应用，特地开发了一个小工具来寻找。该工具基于 golang\u0026amp;client-go 开发, 程序会找出当前集群中所有 Running 的 pods, 然后逐个进入容器，执行 java -version 命令，将命令输出打印到文件中，使用编辑器进行查找检索即可。\n源代码 #  代码量不大，这里直接贴出main.go的代码：\npackage main import ( \u0026#34;bytes\u0026#34; \u0026#34;context\u0026#34; \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; corev1 \u0026#34;k8s.io/api/core/v1\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; \u0026#34;k8s.io/client-go/kubernetes\u0026#34; \u0026#34;k8s.io/client-go/kubernetes/scheme\u0026#34; \u0026#34;k8s.io/client-go/rest\u0026#34; \u0026#34;k8s.io/client-go/tools/clientcmd\u0026#34; \u0026#34;k8s.io/client-go/tools/remotecommand\u0026#34; \u0026#34;strings\u0026#34; ) func main() { kubeconfig := flag.String(\u0026#34;kubeconfig\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;absolute path to the kubeconfig file\u0026#34;) flag.Parse() config, err := clientcmd.BuildConfigFromFlags(\u0026#34;\u0026#34;, *kubeconfig) if err != nil { panic(err.Error()) } clientset, err := kubernetes.NewForConfig(config) if err != nil { panic(err.Error()) } pods, err := clientset.CoreV1().Pods(\u0026#34;\u0026#34;).List(context.TODO(), metav1.ListOptions{ FieldSelector: \u0026#34;status.phase=Running\u0026#34;, }) if err != nil { panic(err.Error()) } fmt.Printf(\u0026#34;There are %d running pods in the cluster\\n\u0026#34;, len(pods.Items)) for _, item := range pods.Items { stdout, stderr, err := execInPod(config, item.Namespace, item.Name, item.Spec.Containers[0].Name) if err != nil { continue } fmt.Printf(\u0026#34;--\u0026gt; %v/%v/%v\\n\u0026#34;, item.Namespace, item.Name, item.Spec.Containers[0].Name) fmt.Println(stderr) fmt.Println(stdout) } } func execInPod(config *rest.Config, namespace, podName, containerName string) (string, string, error) { clientset, err := kubernetes.NewForConfig(config) if err != nil { panic(err.Error()) } req := clientset.CoreV1().RESTClient().Post(). Resource(\u0026#34;pods\u0026#34;).Name(podName).Namespace(namespace). SubResource(\u0026#34;exec\u0026#34;). Param(\u0026#34;container\u0026#34;, containerName) req.VersionedParams( \u0026amp;corev1.PodExecOptions{ Command: []string{\u0026#34;java\u0026#34;, \u0026#34;-version\u0026#34;}, Stdin: false, Stdout: true, Stderr: true, TTY: false, }, scheme.ParameterCodec, ) var stdout, stderr bytes.Buffer exec, err := remotecommand.NewSPDYExecutor(config, \u0026#34;POST\u0026#34;, req.URL()) if err != nil { return \u0026#34;\u0026#34;, \u0026#34;\u0026#34;, err } err = exec.Stream(remotecommand.StreamOptions{ Stdin: nil, Stdout: \u0026amp;stdout, Stderr: \u0026amp;stderr, }) if err != nil { return \u0026#34;\u0026#34;, \u0026#34;\u0026#34;, err } return strings.TrimSpace(stdout.String()), strings.TrimSpace(stderr.String()), err } 使用方式 #  mkdir execjava-versioninpod cd execjava-versioninpod vi main.go # 将上述代码粘贴进去 :wq go mod init example.com/execjava-versioninpod go mod tidy go build . kubectl config use-context \u0026lt;context-name\u0026gt; \\ \u0026amp;\u0026amp; ./execjava-versioninpod \\  --kubeconfig=$HOME/.kube/config \u0026gt; java-version-outputs.txt 打印的文件java-version-outputs.txt的内容如下：\nThere are 11 running pods in the cluster --\u0026gt; default/tomcat-7cf47cf6b4-7bbqr/tomcat openjdk version \u0026#34;1.8.0_292\u0026#34; OpenJDK Runtime Environment (build 1.8.0_292-b10) OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode) --\u0026gt; default/tomcat-7cf47cf6b4-jcp7k/tomcat openjdk version \u0026#34;1.8.0_292\u0026#34; OpenJDK Runtime Environment (build 1.8.0_292-b10) OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode) ... 使用常规的文本编辑器打开该文件，检索 jdk9 及以上版本的 pod 即可，至此完成～\n"},{"id":22,"href":"/posts/2203/harbor-dual-master-replication-ha/","title":"Harbor 双主复制解决方案实践","section":"文章","content":" 本文已参与「开源摘星计划」，欢迎正在阅读的你加入。活动链接：https://github.com/weopenprojects/WeOpen-Star\n 方案的选择 #  分析了 官方 Github: Harbor 高可用方案讨论, 一开始我们选择了 Solution 1 (双激活共享存储方案), 在公司内部大概运行了一年多的时间, 架构图如下:\n 从图中可以看到, 这种方案基于外部共享存储、外部数据库和 Redis 服务, 构建其两个/以上的 harbor 实例. 既然使用了外部的服务, 那么高可用的压力自然而然的转移到了外部服务上. 我们一开始采用的外部的 NFS 共享存储服务, 由于我们团队实际情况, 我们暂时还不能保证外部存储的高可用. 同时, 鉴于我们对镜像服务高可用的迫切需求, 决定调研新的 Harbor 的高可用方案.\n选择了 Solution 4 (双主复制方案), 这个解决方案, 使用复制来实现高可用, 它不需要共享存储、外部数据库服务、外部 Redis 服务. 这种方案可以有效的解决镜像服务的单点故障. 架构图如下:\n 从图中可以看到, 这种方案仅需要在两个 harbor 实例之间建立全量复制机制. 这种方案特别适合异地办公的团队.\n环境 #  以下是服务器和各组件的详细情况:\n   服务器配置 值     虚拟机 2台   IP/内网 10.206.99.57, 10.206.99.58   配置 4核8G, 系统盘160G, 数据盘5T挂载到/data目录   操作系统 CentOS 7.9   用户 root     这里把数据磁盘挂到 /data 目录, 是因为 harbor 的数据卷配置默认就是它, 后面就不需要修改 harbor 这块的配置了.\n    组件 配置/版本 说明     docker-ce 20.10.14    docker-compose 1.29.2 最新稳定版   harbor v2.2.4 离线版    安装 docker #  参考 Install Docker Engine on CentOS 来安装, 因为我是全新的系统, 直接安装:\n安装 yum 仓库 #  安装 yum-utils 包, 它能提供 yum-config-manager 配置工具, 然后用工具来配置安装稳定的 yum 仓库.\nyum install -y yum-utils yum-config-manager \\  --add-repo \\  http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo  这里使用阿里云镜像替换 https://download.docker.com/linux/centos/docker-ce.repo\n 安装 docker 引擎 #  安装最新稳定版 Docker 引擎和 containerd\nyum install -y docker-ce docker-ce-cli containerd.io 启动 docker 实例并配置开机自动启动\nsystemctl start docker systemctl enable docker 优化 docker 配置 #  做一些 docker 相关的配置优化:\ncat \u0026lt;\u0026lt;EOF | tee /etc/docker/daemon.json { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;max-size\u0026#34;: \u0026#34;100m\u0026#34; }, \u0026#34;storage-driver\u0026#34;: \u0026#34;overlay2\u0026#34;, \u0026#34;storage-opts\u0026#34;: [ \u0026#34;overlay2.override_kernel_check=true\u0026#34; ] } EOF 重启启动 docker 实例\nsystemctl daemon-reload systemctl restart docker 安装 docker-compose #  harbor 使用 docker-compose 进行部署, 当前最新稳定版本是 1.29.2, 使用下面命令进行安装:\ncurl -L \u0026#34;https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose # 如果你的服务器也是 Linux-x86_64, 可以用这个国内的地址下载 curl -L \u0026#34;https://rutron.oss-cn-beijing.aliyuncs.com/tools/docker-compose-Linux-x86_64\u0026#34; -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose 安装 Harbor 实例 #  打开 Harbor 下载页面, 下载离线安装器. 因为之前使用的是 v2.2.0 版本, 有不少应用已经对接了 harbor 的 api, 为了兼容性, 我选择了 v2.2.4.\n# 使用 root 用户 ~ 目录 cd /root curl -O https://rutron.oss-cn-beijing.aliyuncs.com/harbor/harbor-offline-installer-v2.2.4.tgz tar xzvf harbor-offline-installer-v2.2.4.tgz cd harbor  由于 github-releases 下载页面速度很慢, 我将下载好的包放在了 aliyun-oss 上\n 配置文件 #  拷贝示例配置文件, 进行修改:\ncp harbor.yml.tmpl harbor.yml vi harbor.yml 因为我打算采用默认安装, 所以需要修改的配置项不多, 仅有几个地方需要修改:\n hostname: 访问 harbor admin ui 和镜像服务的 hostname 或者 ip https:  certificate: 线上服务基本都需要要开通 https, 为 https 配置证书路径 private_key: 为 https 配置私钥路径   external_url: 如果要把 harbor 放在代理的后面, 比如请求会通过 nginx/f5 的代理转发才会到 harbor, 就需要配置该项. 如配置了该项, 上面的 hostname 配置就会失效. database.paasword: 数据库密码, 线上环境必须修改 data_volume: 这是 harbor 的数据目录, 默认是 /data, 因为我服务器的数据盘就挂的 /data 目录, 这里就不需要修改了.  下面是默认配置文件, 重点配置我都做了翻译。别看配置文件这么长，重要的都在前 50 行:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206  # Harbor 配置文件# 访问管理端 UI 和容器镜像服务使用的 IP 地址或者 hostname# 禁止使用 localhost 或 127.0.0.1, 因为 Harbor 需要被外部客户端访问hostname:reg.mydomain.com# http related confighttp:# port for http, default is 80. If https enabled, this port will redirect to https portport:80# https 相关配置https:# harbor https 端口, 默认 443port:443# nginx 证书和私钥路径certificate:/your/certificate/pathprivate_key:/your/private/key/path# # Uncomment following will enable tls communication between all harbor components# internal_tls:# # set enabled to true means internal tls is enabled# enabled: true# # put your cert and key files on dir# dir: /etc/harbor/tls/internal# 取消注释会开启外部代理# 如开启了该配置, 就不会使用 hostname 了# external_url: https://reg.mydomain.com:8433# Harbor 管理后台初始密码# 仅第一次安装 harbor 时有用# 登录 harbor 管理后台之后, 记得修改 admin 密码harbor_admin_password:Harbor12345# Harbor 数据库配置database:# Harbor 数据库 root 用户的密码# 上生产环境, 必须要修改password:root123# 空闲连接池中的最大连接数量# 如果 \u0026lt;=0 表示不保留任何空闲连接max_idle_conns:50# 数据库开启的最大连接数# 如果 \u0026lt;=0, 表示不限制打开连接数# 注意: harbor 使用的 postgres 该配置默认是 1024max_open_conns:1000# 默认数据卷data_volume:/data# Harbor Storage settings by default is using /data dir on local filesystem# Uncomment storage_service setting If you want to using external storage# storage_service:# # ca_bundle is the path to the custom root ca certificate, which will be injected into the truststore# # of registry\u0026#39;s and chart repository\u0026#39;s containers. This is usually needed when the user hosts a internal storage with self signed certificate.# ca_bundle:# # storage backend, default is filesystem, options include filesystem, azure, gcs, s3, swift and oss# # for more info about this configuration please refer https://docs.docker.com/registry/configuration/# filesystem:# maxthreads: 100# # set disable to true when you want to disable registry redirect# redirect:# disabled: false# Trivy configuration## Trivy DB contains vulnerability information from NVD, Red Hat, and many other upstream vulnerability databases.# It is downloaded by Trivy from the GitHub release page https://github.com/aquasecurity/trivy-db/releases and cached# in the local file system. In addition, the database contains the update timestamp so Trivy can detect whether it# should download a newer version from the Internet or use the cached one. Currently, the database is updated every# 12 hours and published as a new release to GitHub.trivy:# ignoreUnfixed The flag to display only fixed vulnerabilitiesignore_unfixed:false# skipUpdate The flag to enable or disable Trivy DB downloads from GitHub## You might want to enable this flag in test or CI/CD environments to avoid GitHub rate limiting issues.# If the flag is enabled you have to download the `trivy-offline.tar.gz` archive manually, extract `trivy.db` and# `metadata.json` files and mount them in the `/home/scanner/.cache/trivy/db` path.skip_update:false## insecure The flag to skip verifying registry certificateinsecure:false# github_token The GitHub access token to download Trivy DB## Anonymous downloads from GitHub are subject to the limit of 60 requests per hour. Normally such rate limit is enough# for production operations. If, for any reason, it\u0026#39;s not enough, you could increase the rate limit to 5000# requests per hour by specifying the GitHub access token. For more details on GitHub rate limiting please consult# https://developer.github.com/v3/#rate-limiting## You can create a GitHub token by following the instructions in# https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line## github_token: xxxjobservice:# Maximum number of job workers in job servicemax_job_workers:10notification:# Maximum retry count for webhook jobwebhook_job_max_retry:10chart:# Change the value of absolute_url to enabled can enable absolute url in chartabsolute_url:disabled# 日志配置log:# 可选项 debug, info, warning, error, fatallevel:info# 使用 local 存储的日志相关配置local:# 日志轮转文件数量# 日志文件在被删除之前会轮转 rotate_count 次# 如果 0 则删除旧版本而不是轮换。rotate_count:50# 当日志文件大于 rotate_size 个字节 bytes 时会轮换# 如果 size 后跟 k 则表示以 kb 为单位, 也可以跟 M/G# 所以 100/100k/100M/200G 都是合法的rotate_size:200M# 存储日志的主机目录location:/var/log/harbor# Uncomment following lines to enable external syslog endpoint.# external_endpoint:# # protocol used to transmit log to external endpoint, options is tcp or udp# protocol: tcp# # The host of external endpoint# host: localhost# # Port of external endpoint# port: 5140#This attribute is for migrator to detect the version of the .cfg file, DO NOT MODIFY!_version:2.2.0# Uncomment external_database if using external database.# external_database:# harbor:# host: harbor_db_host# port: harbor_db_port# db_name: harbor_db_name# username: harbor_db_username# password: harbor_db_password# ssl_mode: disable# max_idle_conns: 2# max_open_conns: 0# notary_signer:# host: notary_signer_db_host# port: notary_signer_db_port# db_name: notary_signer_db_name# username: notary_signer_db_username# password: notary_signer_db_password# ssl_mode: disable# notary_server:# host: notary_server_db_host# port: notary_server_db_port# db_name: notary_server_db_name# username: notary_server_db_username# password: notary_server_db_password# ssl_mode: disable# Uncomment external_redis if using external Redis server# external_redis:# # support redis, redis+sentinel# # host for redis: \u0026lt;host_redis\u0026gt;:\u0026lt;port_redis\u0026gt;# # host for redis+sentinel:# # \u0026lt;host_sentinel1\u0026gt;:\u0026lt;port_sentinel1\u0026gt;,\u0026lt;host_sentinel2\u0026gt;:\u0026lt;port_sentinel2\u0026gt;,\u0026lt;host_sentinel3\u0026gt;:\u0026lt;port_sentinel3\u0026gt;# host: redis:6379# password:# # sentinel_master_set must be set to support redis+sentinel# #sentinel_master_set:# # db_index 0 is for core, it\u0026#39;s unchangeable# registry_db_index: 1# jobservice_db_index: 2# chartmuseum_db_index: 3# trivy_db_index: 5# idle_timeout_seconds: 30# Uncomment uaa for trusting the certificate of uaa instance that is hosted via self-signed cert.# uaa:# ca_file: /path/to/ca# Global proxy# Config http proxy for components, e.g. http://my.proxy.com:3128# Components doesn\u0026#39;t need to connect to each others via http proxy.# Remove component from `components` array if want disable proxy# for it. If you want use proxy for replication, MUST enable proxy# for core and jobservice, and set `http_proxy` and `https_proxy`.# Add domain to the `no_proxy` field, when you want disable proxy# for some special registry.proxy:http_proxy:https_proxy:no_proxy:components:- core- jobservice- trivy# metric:# enabled: false# port: 9090# path: /metrics  默认安装 #  默认安装不含 Notary, Trivy, 或者 Chart 仓库服务, 执行下面的命令:\n./install.sh 查看安装状态:\ndocker ps 如果所有的容器的状态 STATUS 都为 Up About a minute (healthy) 说明安装成功~\n打开 harbor admin ui 验证下吧! 别忘了修改 admin 的密码. 使用同样的方式将两台虚拟机的 docker、docker-compose 和 harbor 都安装好.\n更改配置 #  如果需要更改 harbor 的配置, 请按照如下步骤操作:\n 停止 harbor  # 首先进入工作目录 cd ~/harbor/ docker-compose down -v 更新配置文件  vim harbor.yml 运行脚本生成最终配置  ./prepare 重新启动 harbor 实例  docker-compose up -d 其他命令  # 重装前清理历史数据 rm -rf /data/database rm -rf /data/registry rm -rf /data/redis 配置双主复制 #  在其中一台 harbor 实例上配置，我以 10.206.99.58 为例，另一实例同理，首先需要创建仓库，点击系统管理\u0026gt;仓库管理\u0026gt;新建目标，按照如下填写：\n 然后，创建复制规则，点击系统管理\u0026gt;复制管理\u0026gt;新建规则，按照如下填写：\n 这样，当用户往 10.206.99.58 中推送/删除镜像时，10.206.99.57 也会同步发生变化。\n增加反向代理 #  现在两个 harbor 实例都已经配置好了。用户看到的是两个完全独立的 harbor，他们的用户独立，访问地址不同。当然有些场景下这样已经可以满足需求了，比如异地办公的团队（可以按照地域区分使用访问地址）。如果我们想统一访问地址，可以在前面增加一个反向代理。而且可以将 ssl 证书部署在代理上。还是比较推荐的。所以我希望这个代理能实现：\n 统一的访问入口： 将两个 harbor 地址统一为一个。 卸载 ssl 证书： 这将简化 harbor 实例的配置，更易于证书的管理。 会话保持： 因为 harbor 之间复制是有时间差的，用户往一个实例中推送镜像之后不可能立即在另一实例中拉取到，所以要将客户端的请求固定到一个实例上。   但是很遗憾，harbor 实例之间用户和相关权限是无法同步的。这可能需要需要一些外在的机制实现了。\n 我假设提供给用户的域名是：registry.example.com，我使用 nginx 作为这个反向代理，它的配置文件/etc/nginx/conf.d/registry.example.com.conf是这样的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50  upstream harbor{ ip_hash; server 10.206.99.57; server 10.206.99.58; } server { listen 80; server_name registry.example.com; rewrite ^(.*)$ https://$host$1; } server { listen 443 ssl; server_name registry.example.com; charset utf-8; client_max_body_size 0; client_header_timeout 180; client_body_timeout 180; send_timeout 180; ssl_certificate /etc/nginx/conf.d/cert/registry_example_com.pem; ssl_certificate_key /etc/nginx/conf.d/cert/registry_example_com.key; ssl_session_timeout 5m; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4:!DH:!DHE; ssl_protocols TLSv1 TLSv1.1 TLSv1.2 TLSv1.3; ssl_prefer_server_ciphers on; proxy_http_version 1.1; proxy_connect_timeout 900; proxy_send_timeout 900; proxy_read_timeout 900; proxy_buffering off; proxy_request_buffering off; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # 如果harbor实例仅配置了ip类型的hostname这里就不用配置了  # 如果配置了可解析的hostname/external_url需要打开注释  # proxy_set_header Host $host;  # 如果external_url中使用https但是代理访问harbor使用http需要打开注释  # 同时去掉harbor实例内部的nginx相关的$scheme配置  # proxy_set_header X-Forwarded-Proto $scheme;  location / { proxy_pass http://harbor; } }   运行 nginx 反向代理：\n# 将证书和配置文件都放在 /etc/nginx/conf.d 路径下 docker run -d --restart=always \\  --name=nginx \\  -p 80:80 -p 443:443 \\  -v /etc/nginx/conf.d:/etc/nginx/conf.d \\  nginx 测试对 registry.example.com 进行login/push/pull镜像均正常，检查两个 harbor 实例也同步正常。至此，完成～\n总结 #  至此，所有的安装/配置就结束了，通过体验测试我发现：\n 用户是独立的\n两个实例之间的项目、镜像、标签相关资源是可以同步的，但是用户不可以。如果用户要在两个实例直接切换使用的话，需要分别登录两个 harbor admin ui 为用户创建两个相同的账号。所以说该方案比较适合异地办公团队，仅做镜像数据的同步。 镜像同步有一定的时间差\n我的两个实例是所在虚拟机在一个网段内的，测试了一个约 900M 的镜像，从开始同步到结束大概是10秒种。如果用户在一台实例上推送之后，立马去另一台实例上拉去是不行的。所以如果两个实例前面要增加 http 代理的话，需要使用 ip_hash 负载均衡策略，将用户请求固定到其中一台实例上。 实例 url 地址不一致\n这个问题不严重，因为是两个实例，如果我们在他们前面再部署 http 代理的话，就是三个地址。所以，两个实例对应 admin ui 上的 url 地址和用户使用的（如果有代理）url 地址都不一样。 比如：   "},{"id":23,"href":"/posts/2203/find-ns-that-exceed-resource-limits/","title":"在 kubernetes 中找出过度使用资源的 namespaces","section":"文章","content":"我们知道, 在 kubernetes 中, namespace 的资源限制在 ResourceQuota 中定义, 比如我们控制 default 名称空间使用 64核80G 的资源:\n$ kubectl get resourcequota not-best-effort -oyamlapiVersion:v1kind:ResourceQuotametadata:name:not-best-effortspec:hard:limits.cpu:\u0026#34;64\u0026#34;limits.memory:80Gstatus:hard:limits.cpu:\u0026#34;64\u0026#34;limits.memory:80Gused:limits.cpu:30500mlimits.memory:59G通常来讲, 由于 kubernetes 的资源控制机制, .status.used 中资源的值会小于 .status.hard 中相应资源的值. 但也有特例.\n特殊情况 #  当我们开始定义了一个较大的资源限制, 待应用部署完毕, 资源占用了很多之后, 这时调低资源限制. 此时就会出现 .status.used 中的值超过 .status.hard 中相应值的情况, 尤其是内存的限制. 比如下面个:\n...spec:hard:limits.cpu:\u0026#34;1\u0026#34;limits.memory:1Gstatus:hard:limits.cpu:\u0026#34;1\u0026#34;limits.memory:1Gused:limits.cpu:15600mlimits.memory:26044M找出这些名称空间 #  在集群管理的过程中, 往往我们需要找出这些过度使用资源名称空间, 这可能是因为用户会为资源付费. 所以, 我尝试使用 golang 开发一个小工具, 找出这些名称空间. 现在代码已经写好了, 这个工具有很少的代码, 直接将全部代码贴出来:\npackage main import ( \u0026#34;bufio\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;strings\u0026#34; ) func main() { fi, err := os.Open(\u0026#34;./resourcequotas.txt\u0026#34;) if err != nil { fmt.Printf(\u0026#34;Error: %s\\n\u0026#34;, err) return } defer fi.Close() br := bufio.NewReader(fi) for { l, _, c := br.ReadLine() if c == io.EOF { break } fmt.Print(string(l)) s := strings.Split(string(l), \u0026#34;limits.memory:\u0026#34;) mem := strings.Trim(s[1], \u0026#34; \u0026#34;) s = strings.Split(mem, \u0026#34;/\u0026#34;) mused := formatUnits(s[0]) mhard := formatUnits(s[1]) fmt.Printf(\u0026#34;(%v/%v)\u0026#34;, mused, mhard) musedf, _ := strconv.ParseFloat(mused, 64) mhardf, _ := strconv.ParseFloat(mhard, 64) if musedf \u0026gt; mhardf { fmt.Println(\u0026#34; \u0026lt;== this line\u0026#34;) } else { fmt.Println(\u0026#34;\u0026#34;) } } } func formatUnits(s string) string { if strings.Index(s, \u0026#34;G\u0026#34;) \u0026gt;= 0 { s = strings.Trim(s, \u0026#34;G\u0026#34;) s = strings.Trim(s, \u0026#34;Gi\u0026#34;) return s } if strings.Index(s, \u0026#34;M\u0026#34;) \u0026gt;= 0 { s = strings.Trim(s, \u0026#34;M\u0026#34;) s = strings.Trim(s, \u0026#34;Mi\u0026#34;) i, _ := strconv.Atoi(s) s = fmt.Sprintf(\u0026#34;%.2f\u0026#34;, float64(i)/1024) return s } if s != \u0026#34;0\u0026#34; { // byte  i, _ := strconv.Atoi(s) s = fmt.Sprintf(\u0026#34;%.2f\u0026#34;, float64(i)/1024/1024/1024) return s } return s }  该代码仅实现了查找内存占用超过限制的逻辑\n 首先, 我们通过命令, 将集群中所有的 ResourceQuota 资源导出到 resourcequotas.txt 文件中:\nkubectl get resourcequota -A \u0026gt; resourcequotas.txt # 文件内容如下: NAMESPACE NAME AGE REQUEST LIMIT namespace-001 not-best-effort 129d limits.cpu: 30500m/64, limits.memory: 59G/80G namespace-002 not-best-effort 125d limits.cpu: 3300m/10, limits.memory: 3564M/10G namespace-003 not-best-effort 4d5h limits.cpu: 1/4, limits.memory: 1G/8G 将生成的 txt 文件和代码放在一个目录中, 接下来执行代码, 会看到下面的输出:\n$ go run main.go ... namespace-102 not-best-effort 349d limits.cpu: 10100m/13, limits.memory: 20218M/26G(19.74/26) namespace-103 not-best-effort 349d limits.cpu: 15600m/1, limits.memory: 26044M/1G(25.43/1) \u0026lt;== this line namespace-104 not-best-effort 349d limits.cpu: 5200m/8, limits.memory: 5460M/16G(5.33/16) ... 可以看到 \u0026lt;== this line, 名称空间 namespace-103 定义的内存限制是 1G, 但是实际使用了 25.43G, 很明显超过了资源限制. 我们的目的也就达到了.\n"},{"id":24,"href":"/posts/2203/what-is-ebpf/","title":"[译]什么是 eBPF?","section":"文章","content":"点击查看 原文\n什么是 eBPF? #  eBPF 是革命性技术, 起源于 linux 内核, 能够在操作系统内核中执行沙盒程序. 旨在不改变内核源码或加载内核模块的前提下安全便捷的扩展内核能力.\n历史上, 由于内核拥有全局查看并控制整个操作系统的特权, 操作系统一直被认为是实现可观察性, 安全, 网络功能的理想地方. 同时, 由于其核心角色和对于稳定和安全的高要求, 操作系统很难演进. 因此, 传统上与在操作系统之外实现的功能相比, 操作系统级别的创新率较低.\n eBPF 从根本上改变了这种一成不变的状态. 通过允许在操作系统中执行沙盒程序, 开发者可以通过执行 eBPF 程序, 来给运行中的操作系统添加额外的能力. 就像在本地使用即时编译器(JIT)和验证引擎一样, 操作系统可以保证安全性和执行效率. 这催生了不少基于 eBPF 的项目, 涵盖了广泛的用例, 包括下一代网络、可观察性和安全功能.\n今天, eBPF 被广泛用于各种用例: 在现代化的数据中心和云原生环境中提供高性能网络和负载均衡, 以较低的开销提取细粒度的可观察性安全数据, 帮助应用程序开发者追踪应用, 并能够在性能故障分析、预防性应用和容器运行时安全执法等方面提供帮助. 它的可能性是无限的, 关于 eBPF 的创新才刚开始.\n什么是 eBPF.io? #  eBPF.io 是以 eBPF 为主题, 每个人学习和协作的地方. eBPF 是一个开源社区, 每个人可以实践或者分享. 不论你是想阅读 eBPF 第一篇介绍文章, 还是发现更多阅读素材, 抑或是为变成 eBPF 主项目贡献者迈出第一步, eBPF.io 会一直陪伴你帮助你.\n介绍 eBPF #  下面的章节是关于 eBPF 的快速介绍. 如果你想了解更多, 查看 eBPF \u0026amp; XDP Reference Guide. 不管你是一名从事 eBPF 的开发者, 或是有兴趣使用 eBPF 作为解决方案, 理解基础概念和架构都是很有用的.\nHook 概览 #  eBPF 程序是事件驱动的, 能在内核或应用程序执行到一个特定的 hook 点时执行. 预定义的 hooks 包含系统调用, 函数出/入口, 内核追踪点, 网络事件等等.\n 如果预定义 hook 不能满足需求, 也可以创建内核探针(kprobe)或者用户探针(uprobe), 在内核/用户应用程序的任何位置, 把探针附加到 eBPF 程序上.\neBPF 程序怎么写? #  在很多场景中, 用户不需要直接使用 eBPF, 而是通过一些项目, 比如 cilium, bcc 或 bpftrace, 它们是 eBPF 上层的抽象, 提供了使用 eBPF 实现的特定功能, 用户无需直接编写 eBPF 程序.\n 如果没有高级抽象, 就需要直接编写 eBPF 程序. Linux 内核要器加载字节码形式的 eBPF 程序. 虽然可以直接编写字节码, 但是更普遍的开发实践是借用像 LLVM 这样的编译器, 把伪 C 代码编译成字节码.\n加载器 \u0026amp; 验证架构 #  当所需的钩子被识别后, 可以使用 bpf 系统调用将 eBPF 程序加载到 Linux 内核中. 这通常使用一个可用的 eBPF 工具库来完成. 下一节将介绍一些可用的开发工具链.\n 当程序加载到 Linux 内核中时, 它在附加到请求的钩子之前要经过两个步骤:\n验证 #  这一步是为了确保 eBPF 程序安全执行. 它验证程序是否满足一些条件, 比如:\n 加载 eBPF 程序的进程拥有所需的能力(特权). 除非启用非特权 eBPF, 否则只有特权进程才能加载 eBPF 程序. 该程序不能崩溃或者以其他方式伤害操作系统. 该程序必须总是能执行完(即程序不会死循环, 阻止后面的处理).  即时编译 (JIT) #  该步骤将通用字节码翻译成机器特定的指令集, 以优化程序的执行速度. 这使 eBPF 程序像原生编译的内核代码或者像已加载的内核模块代码一样高效运行.\nMaps #  eBPF 程序一个重要能力是: 能够共享收集的信息, 能够存储状态. 为了实现该能力, eBPF 程序借用 Maps 来存储/获取数据, 它支持丰富的数据结构. 通过系统调用, 可以从 eBPF 程序或者用户空间应用访问 maps.\n 为了解 map 类型的多样性, 下面是不完整的 map 类型列表. 这些类型的变量同时是 共享变量 和 per-CPU 变量.\n Hash tables, Arrays 哈希表, 数组 LRU (Least Recently Used) 最近最少使用 Ring Buffer 环形缓冲区 Stack Trace 堆栈跟踪 LPM (Longest Prefix Match) 最长前缀匹配 \u0026hellip;  帮助函数 #  eBPF 程序不能随意调用内核函数. 如果允许的话, 将会把 eBPF 程序绑定到特定的内核版本, 这会使程序的兼容性复杂化. 所以, eBPF 程序转而使用帮助函数, 它是内核提供的大家熟知的稳定的 API.\n 可用的帮助函数还在持续发展中, 例如:\n 生成随机数 获取当前时间和日期 访问 eBPF map 获取 process/cgroup 上下文 网络数据包处理和转发逻辑  尾调用 \u0026amp; 函数调用 #  eBPF 程序可以组合使用尾调用和函数调用(tail \u0026amp; function calls). 函数调用允许在 eBPF 程序中定义和调用函数. 尾调用可以调用执行其他 eBPF 程序, 并替换执行上下文, 类似于 execve() 系统调用对常规进程的操作方式.\n eBPF 安全 #  权利越大, 责任越大\neBPF 是一项伟大的技术, 当下在很多关键软件中都扮演了核心的角色. 在 eBPF 程序开发过程中, 当 eBPF 进入 Linux 内核时, eBPF 的安全性就变得异常重要. eBPF 的安全性通过下面几点来保证:\n要求特权 #  除非开启非特权 eBPF, 所有企图加载 eBPF 程序到内核的进程必须在特权模式（root）下运行，或者必须获得 CAP_BPF 能力. 这意味着非授信的程序不能加载 eBPF 程序.\n如果开启非特权 eBPF, 非特权进程可以加载特定的 eBPF 程序, 它们仅能使用被缩减的功能集合, 并且将受限制的访问内核.\n验证器 #  如果进程允许加载 eBPF 程序, 所有的程序都要经过 eBPF 验证器, 验证器来确保程序本身的安全性. 这意味着:\n 通过验证的程序一定会执行完, 比如, eBPF 程序不会卡住或死循环. eBPF 程序可以包含有边界的循环, 但是验证器要求, 循环必须具有可以被执行到的退出条件. 程序不能使用任何未初始化的变量或者越界访问内存. 程序必须在系统要求的大小范围内. 随意大的 eBPF 程序是无法加载的. 程序必须具备有限的复杂性. 验证器会评估所有可能的执行路径, 并且必须在配置的复杂度范围内完成分析.  加固 #  完成验证之后, 根据 eBPF 程序是从特权进程还是非特权进程加载, 来决定是否加固的 eBPF 程序. 这包括:\n 程序执行保护: 存有 eBPF 程序的内核内存是被保护的并且是只读的. 不管是内核 bug 或者是被恶意操纵, 内核都将崩溃, 而不是允许它继续执行损坏/被操纵的程序. Mitigation against Spectre: Under speculation CPUs may mispredict branches and leave observable side effects that could be extracted through a side channel. 举几个例子: eBPF programs mask memory access in order to redirect access under transient instructions to controlled areas, the verifier also follows program paths accessible only under speculative execution and the JIT compiler emits Retpolines in case tail calls cannot be converted to direct calls. 常量 blinding: 代码中的所有常量都被 blinded, 以防止 JIT spraying 攻击. 这可以避免: 当存在某种内核 bug 的情况下, 攻击者可以把可执行代码作为常量注入, 从而让攻击者跳转到 eBPF 程序的内存区域来执行代码.  抽象的运行时上下文 #  eBPF 程序不能直接访问任意内核内存. 必须通过 eBPF 助手函数访问位于程序上下文之外的数据和数据结构. 这保证了一致性的数据访问, 并使任何此类访问均受制于 eBPF 程序的权限, 例如如果可以保证修改是安全的, 则允许运行的 eBPF 程序修改某些数据结构的数据. eBPF 程序不能随机修改内核中的数据结构.\n为什么使用 eBPF? #  可编程的力量 #  还记得 GeoCities 吗? 20年前, 网页几乎全都是用静态标记语言(HTML)写的, 网页基本上是一种应用程序(浏览器)能打开的文件. 再看今天, 网页已经变成了非常成熟的应用, 并且 WEB 已经取代了绝大部分编译语言写的应用. 是什么成就了这次革命?\n 简单来说, 就是引入 JavaScript 之后的可编程性. 它开启了一场大规模的革命, 几乎将浏览器变成了独立的操作系统.\n为什么呢? 程序员不再受限于特定的浏览器版本. 没有去说服标准机构去定义更多需要的 HTML 标签, 相反, 而是提供了一些必要的构建模块, 将浏览器底层的演进和运行在其上层的应用进行分离. 这样说可能过于简单, 因为 HTML 的确做了不小的贡献, 也的确有所发展, 但是 HTML 本身的变革还不够.\n在举这个例子并将其应用到 eBPF 之前, 让我们看一下对引入 JavaScript 至关重要的几个关键方面:\n 安全性: 不受信任的代码在用户的浏览器中运行. 这是通过沙盒 JavaScript 程序和抽象对浏览器数据的访问来解决的. 持续交付: 在不需要浏览器发新版本的情况下, 程序要能不断更新. 这得益于浏览器低级的(low-level)构建模块, 它能构建任意的逻辑. 性能: 必须以最小的开销提供可编程性. 这得益于即时编译器(JIT).  上面说的所有内容, 在 eBPF 中都能找到:\neBPF 对 Linux 内核的影响 #  现在我们回到 eBPF. 为了理解 eBPF 可编程性在 Linux 内核上的影响, 我们来看张图片, 它有助于我们对 Linux 内核的架构进行理解, 并且能了解它是如何与应用程序和硬件进行交互的.\n Linux 内核的主要目的是抽象硬件或虚拟硬件, 并提供一致的 API(系统调用), 允许应用程序运行和共享资源. 为了实现这一点, 维护了大量的子系统和层来分配这些职责. 每个子系统通常允许某种级别的配置来满足不同的用户需求. 如果没办法通过配置满足某种需求, 则需要更改内核. 从历史上看, 有两种选择:\n   原生支持 内核模块     1. 更改内核源代码并说服 Linux 内核社区 1. 写一个新的内核模块   2. 等几年新内核版本上市 2. 定期修复它, 因为每个内核版本都可能破坏它    3. 由于缺乏安全边界, 有损坏 Linux 内核的风险    在不需要改变内核源码或者加载内核模块的情况下, eBPF 为重新编程内核行为提供了一种新的选择. 在很多地方, 这很像 JavaScript 和其他脚本语言, 它们让那些改变难度大, 成本高的系统开始演进.\n开发工具链 #  有几个开发工具链来能够协助 eBPF 程序的开发和管理. 它们能满足用户的不同需求:\nbcc #  BCC 是一个框架, 能够让用户编写嵌入了 eBPF 程序的 python 程序. 该框架主要用来分析和跟踪应用/系统, eBPF 在其中主要负责收集统计数据或生成事件, 然后, 对应的用户空间程序会收集这些数据并以易读的方式进行展示. 运行 python 程序会生成 eBPF 字节码并将其加载进内核.\n bpftrace #  bpftrace 是 Linux eBPF 的高级跟踪语言, 可用于最新的 Linux 内核(4.x). bpftrace 使用 LLVM 作为后端将脚本编译为 eBPF 字节码，并利用 BCC 与 Linux eBPF 子系统以及现有的 Linux 跟踪功能进行交互: 内核动态跟踪(kprobes)、用户级动态跟踪(uprobes)和跟踪点(tracepoints). bpftrace 语言的灵感来自 awk、C 和以前的跟踪器(如 DTrace 和 SystemTap).\n eBPF Go 类库 #  eBPF Go 库提供了一个通用的 eBPF 库, 它将获取 eBPF 字节码的过程与 eBPF 程序的加载和管理分离. eBPF 程序通常是通过编写高级语言创建的, 然后使用 clang/LLVM 编译器编译为 eBPF 字节码.\n libbpf C/C++ 类库 #  libbpf 库是一个基于 C/C++ 的通用 eBPF 库. 它提供给应用程序一种易用的 API 来抽象化 BPF 系统调用, 并将 eBPF 字节码(clang/LLVM 编译器生成)加载到内核的过程与之分离.\n 阅读更多 #  如果你想学习更多的 eBPF 知识, 阅读下面的材料:\n文档 #    BPF \u0026amp; XDP Reference Guide\nCilium 文档, 2020年8月  BPF Documentation\nLinux 内核中的 BPF 介绍文档  BPF Design Q\u0026amp;A\n内核相关的 eBPF 问答  教程 #    Learn eBPF Tracing: Tutorial and Examples\nBrendan Gregg 的博客, 2019年1月  XDP Hands-On Tutorials\n很多作者, 2019年  BCC, libbpf and BPF CO-RE Tutorials\nFacebook 的 BPF 博客, 2020年  发言 #  基础 #    eBPF and Kubernetes: Little Helper Minions for Scaling Microservices (Slides)\nDaniel Borkmann, KubeCon EU, Aug 2020  eBPF - Rethinking the Linux Kernel (Slides)\nThomas Graf, QCon London, April 2020  BPF as a revolutionary technology for the container landscape (Slides)\nDaniel Borkmann, FOSDEM, Feb 2020  BPF at Facebook\nAlexei Starovoitov, Performance Summit, Dec 2019  BPF: A New Type of Software (Slides)\nBrendan Gregg, Ubuntu Masters, Oct 2019  The ubiquity but also the necessity of eBPF as a technology\nDavid S. Miller, Kernel Recipes, Oct 2019  深入 #    BPF and Spectre: Mitigating transient execution attacks (Slides)\nDaniel Borkmann, eBPF Summit, Aug 2021  BPF Internals (Slides)\nBrendan Gregg, USENIX LISA, Jun 2021  Cilium #    Advanced BPF Kernel Features for the Container Age (Slides)\nDaniel Borkmann, FOSDEM, Feb 2021  Kubernetes Service Load-Balancing at Scale with BPF \u0026amp; XDP (Slides)\nDaniel Borkmann \u0026amp; Martynas Pumputis, Linux Plumbers, Aug 2020  Liberating Kubernetes from kube-proxy and iptables (Slides)\nMartynas Pumputis, KubeCon US 2019  Understanding and Troubleshooting the eBPF Datapath in Cilium (Slides)\nNathan Sweet, KubeCon US 2019  Transparent Chaos Testing with Envoy, Cilium and BPF (Slides)\nThomas Graf, KubeCon EU 2019  Cilium - Bringing the BPF Revolution to Kubernetes Networking and Security (Slides)\nThomas Graf, All Systems Go!, Berlin, Sep 2018  How to Make Linux Microservice-Aware with eBPF (Slides)\nThomas Graf, QCon San Francisco, 2018  Accelerating Envoy with the Linux Kernel\nThomas Graf, KubeCon EU 2018  Cilium - Network and Application Security with BPF and XDP (Slides)\nThomas Graf, DockerCon Austin, Apr 2017  Hubble #    Hubble - eBPF Based Observability for Kubernetes\nSebastian Wicki, KubeCon EU, Aug 2020  图书 #    Systems Performance: Enterprise and the Cloud, 2nd Edition\nBrendan Gregg, Addison-Wesley Professional Computing Series, 2020  BPF Performance Tools\nBrendan Gregg, Addison-Wesley Professional Computing Series, Dec 2019  Linux Observability with BPF\nDavid Calavera, Lorenzo Fontana, O\u0026rsquo;Reilly, Nov 2019  文章 \u0026amp; 博客 #    BPF for security - and chaos - in Kubernetes\nSean Kerner, LWN, Jun 2019  Linux Technology for the New Year: eBPF\nJoab Jackson, Dec 2018  A thorough introduction to eBPF\nMatt Fleming, LWN, Dec 2017  Cilium, BPF and XDP\nGoogle Open Source Blog, Nov 2016  Archive of various articles on BPF\nLWN, since Apr 2011  Various articles on BPF by Cloudflare\nCloudflare, since March 2018  Various articles on BPF by Facebook\nFacebook, since August 2018  "},{"id":25,"href":"/posts/2203/kubectl-usefull-command/","title":"比较冷门但有用的 kubectl 命令","section":"文章","content":"以下冷门命令能实现某种具体的功能, 都是在实际工作中摸索总结的经验, 获取到相关的资源名称之后, 就可以配合常用的 kubectl 命令获取其他详细信息.\n列出所有被 OOMKilled 过的 POD 列表 #  使用 jq (一个轻量级的灵活的命令行JSON解析器) 获取上一个状态\nkubectl get pods --all-namespaces -ojson | jq -c \u0026#39;.items[] | { name: .metadata.name, reasons: [{reason: .status.containerStatuses[]?.lastState.terminated.reason, finishedAt: .status.containerStatuses[]?.lastState.terminated.finishedAt}] }\u0026#39; | grep OOMKilled 信息以行为单位显示, 方便进行筛选:\n{\u0026#34;name\u0026#34;:\u0026#34;nginx-699f949679-8kkth\u0026#34;,\u0026#34;reasons\u0026#34;:[{\u0026#34;reason\u0026#34;:\u0026#34;OOMKilled\u0026#34;,\u0026#34;fini shedAt\u0026#34;:\u0026#34;2021-09-08T01:34:05Z\u0026#34;}]} ... 列出含有 UID 的 POD 列表 #  kubelet 大量使用了 pod-uid 来为 pod 创建相关的资源, 获取 pod-uid 来辅助运维是很有必要的.\nkubectl get pods -A -o custom-columns=NAME:.metadata.name,UID:.metadata.uid 列出集群所有 ContainerID #  集群中所有的 containerID，方便排查问题\nkubectl get pods --all-namespaces -ojson | jq -c \u0026#39;.items[] | { name:.metadata.name, c1: [.status.containerStatuses[]?.containerID], c2: [.status.containerStatuses[]?.lastState.terminated.containerID] }\u0026#39; 列出集群中所有镜像 #  有时候为了统计分析镜像，可能需要导出所有在用的镜像\nkubectl get pods --all-namespaces -ojson | jq -c \u0026#39;.items[] | { name:.metadata.name,images: [{image: .spec.containers[]?.image}]}\u0026#39; 按行输出，部分结果如下：\n{\u0026#34;name\u0026#34;:\u0026#34;fz-wms-fz-wms-78657df596-s9zbr\u0026#34;,\u0026#34;images\u0026#34;:[{\u0026#34;image\u0026#34;: \u0026#34;registry .example.com/18678868396/wms:1.1.55\u0026#34;}]} 删除集群中所有被驱逐到Pods记录 #  kubectl get pods --all-namespaces -owide | grep Evicted | awk \u0026#39;{print \u0026#34;kubectl -n \u0026#34;$1\u0026#34; delete pods \u0026#34;$2}\u0026#39; | xargs -I {} sh -c \u0026#34;{}\u0026#34; "},{"id":26,"href":"/posts/2203/pod-cannot-be-assumed/","title":"[解决] FailedScheduling pod/\u003cpod-name\u003e pod is \u003cuid\u003e in the cache so can't be assumed","section":"文章","content":"现象\u0026amp;分析 #  之前由于 prometheus 所在节点 nfs 异常, 更换 prometheus 存储类型 (local-path) 并重新调度 prometheus 之后, 发现集群出现异常事件:\n$ kubectl -n monitoring get events LAST SEEN TYPE REASON OBJECT MESSAGE 60m Warning FailedScheduling pod/prometheus-k8s-1 pod 19b50126-c636-4d3c-842e-768e76e3357b is in the cache, so can\u0026#39;t be assumed 58m Warning FailedScheduling pod/prometheus-k8s-1 pod 19b50126-c636-4d3c-842e-768e76e3357b is in the cache, so can\u0026#39;t be assumed 57m Warning FailedScheduling pod/prometheus-k8s-1 pod 19b50126-c636-4d3c-842e-768e76e3357b is in the cache, so can\u0026#39;t be assumed 55m Warning FailedScheduling pod/prometheus-k8s-1 pod 19b50126-c636-4d3c-842e-768e76e3357b is in the cache, so can\u0026#39;t be assumed 54m Warning FailedScheduling pod/prometheus-k8s-1 pod 19b50126-c636-4d3c-842e-768e76e3357b is in the cache, so can\u0026#39;t be assumed 52m Warning FailedScheduling pod/prometheus-k8s-1 pod 19b50126-c636-4d3c-842e-768e76e3357b is in the cache, so can\u0026#39;t be assumed 51m Warning FailedScheduling pod/prometheus-k8s-1 pod 19b50126-c636-4d3c-842e-768e76e3357b is in the cache, so can\u0026#39;t be assumed 参考 #56682, 这是调度器 scheduler 缓存失效导致的异常事件, 大致原因是 pod 已经调度, 并绑定到指定节点, 由于该节点异常导致启动失败.\n解决办法 #  重新启动 prometheus statefulset, 让集群重新调度, 其实就是将现有到 prometheus pod 副本数将至 0, 再恢复正常即可:\n# prometheus crd 是 prometheus-operator 的自定义资源 # 用来定义部署配置 kubectl -n monitoring edit prometheus k8s # 先 replicas =\u0026gt; 0 :wq # 再 replicas =\u0026gt; 2 :wq 变更操作影响 #  届时 prometheus 会短暂(几分钟)停止服务, 旧数据无丢失.\n"},{"id":27,"href":"/posts/2203/calico-node-readiness-probe-failed/","title":"[解决] Warning pod/calico-node-\u003chash\u003e Readiness probe failed","section":"文章","content":"现象 #  开始发现 calico-system 名称空间下一直有一个 warning 的事件:\n$ kubectl -n calico-system get events LAST SEEN TYPE REASON OBJECT MESSAGE 6m1s Warning Unhealthy pod/calico-node-4fpgp Readiness probe failed: 随后发现 kubelet 日志, 一直在报有两个孤立的 pod.\n$ tail -f /var/log/messages Mar 20 13:20:37 pcosmo-hda-ceno-06 kubelet: E0320 13:20:37.754807 20121 kubelet_volumes.go:154] orphaned pod \u0026#34;47e20a85-99ba-4c77-9cac-36d1aa56b6d3\u0026#34; found, but volume paths are still present on disk : There were a total of 2 errors similar to this. Turn up verbosity to see them. 解决办法 #  手动清理主机上遗留的 pod 文件夹:\n$ rm -rf /var/lib/kubelet/pods/ce3bccb8-560d-4cda-b054-bf138291aa42/ 原因分析 #  孤立的 pod 没有正确清理和 calico-node 就绪检查失败应该没有直接的关联, 大概是 kubelet 就绪检查受到 kubelet_volumes.go 影响.\n"},{"id":28,"href":"/posts/2203/bcc-opensnoop-usage/","title":"bcc 之 opensnoop 工具的使用","section":"文章","content":"这篇文档主要演示了 opensnoop(Linux eBPF/bcc) 工具的使用.\n示例 #  opensnoop 在系统范围内跟踪 open() 系统调用，并打印各种详细信息.\n示例输出:\n# ./opensnoop PID COMM FD ERR PATH 17326 \u0026lt;...\u0026gt; 7 0 /sys/kernel/debug/tracing/trace_pipe 1576 snmpd 9 0 /proc/net/dev 1576 snmpd 11 0 /proc/net/if_inet6 1576 snmpd 11 0 /proc/sys/net/ipv4/neigh/eth0/retrans_time_ms 1576 snmpd 11 0 /proc/sys/net/ipv6/neigh/eth0/retrans_time_ms 1576 snmpd 11 0 /proc/sys/net/ipv6/conf/eth0/forwarding 1576 snmpd 11 0 /proc/sys/net/ipv6/neigh/eth0/base_reachable_time_ms 1576 snmpd 11 0 /proc/sys/net/ipv4/neigh/lo/retrans_time_ms 1576 snmpd 11 0 /proc/sys/net/ipv6/neigh/lo/retrans_time_ms 1576 snmpd 11 0 /proc/sys/net/ipv6/conf/lo/forwarding 1576 snmpd 11 0 /proc/sys/net/ipv6/neigh/lo/base_reachable_time_ms 1576 snmpd 9 0 /proc/diskstats 1576 snmpd 9 0 /proc/stat 1576 snmpd 9 0 /proc/vmstat 1956 supervise 9 0 supervise/status.new 1956 supervise 9 0 supervise/status.new 17358 run 3 0 /etc/ld.so.cache 17358 run 3 0 /lib/x86_64-linux-gnu/libtinfo.so.5 17358 run 3 0 /lib/x86_64-linux-gnu/libdl.so.2 17358 run 3 0 /lib/x86_64-linux-gnu/libc.so.6 17358 run -1 6 /dev/tty 17358 run 3 0 /proc/meminfo 17358 run 3 0 /etc/nsswitch.conf 17358 run 3 0 /etc/ld.so.cache 17358 run 3 0 /lib/x86_64-linux-gnu/libnss_compat.so.2 17358 run 3 0 /lib/x86_64-linux-gnu/libnsl.so.1 17358 run 3 0 /etc/ld.so.cache 17358 run 3 0 /lib/x86_64-linux-gnu/libnss_nis.so.2 17358 run 3 0 /lib/x86_64-linux-gnu/libnss_files.so.2 17358 run 3 0 /etc/passwd 17358 run 3 0 ./run ^C 在跟踪时，snmpd 进程打开了各种 /proc 文件(读取指标). 另外, 一个 “run” 进程读取各种库和配置文件(看起来像 正在启动: 一个新进程).\n如果在应用程序启动期间使用, opensnoop 可用于发现配置和日志文件.\n过滤 PID #  -p 选项可用于在内核中过滤 PID. 这里我将它与 -T 一起使用来打印时间戳:\n$ ./opensnoop -Tp 1956 TIME(s) PID COMM FD ERR PATH 0.000000000 1956 supervise 9 0 supervise/status.new 0.000289999 1956 supervise 9 0 supervise/status.new 1.023068000 1956 supervise 9 0 supervise/status.new 1.023381997 1956 supervise 9 0 supervise/status.new 2.046030000 1956 supervise 9 0 supervise/status.new 2.046363000 1956 supervise 9 0 supervise/status.new 3.068203997 1956 supervise 9 0 supervise/status.new 3.068544999 1956 supervise 9 0 supervise/status.new 这表明 supervise 进程每秒打开2次 status.new 文件.\n包含/过滤 UID #  -U 选项在输出中包含 UID:\n# ./opensnoop -U UID PID COMM FD ERR PATH 0 27063 vminfo 5 0 /var/run/utmp 103 628 dbus-daemon -1 2 /usr/local/share/dbus-1/system-services 103 628 dbus-daemon 18 0 /usr/share/dbus-1/system-services 103 628 dbus-daemon -1 2 /lib/dbus-1/system-services -u 选项过滤 UID:\n# ./opensnoop -Uu 1000 UID PID COMM FD ERR PATH 1000 30240 ls 3 0 /etc/ld.so.cache 1000 30240 ls 3 0 /lib/x86_64-linux-gnu/libselinux.so.1 1000 30240 ls 3 0 /lib/x86_64-linux-gnu/libc.so.6 1000 30240 ls 3 0 /lib/x86_64-linux-gnu/libpcre.so.3 1000 30240 ls 3 0 /lib/x86_64-linux-gnu/libdl.so.2 1000 30240 ls 3 0 /lib/x86_64-linux-gnu/libpthread.so.0 过滤失败的 opens #  -x 选项仅打印失败的 opens:\n# ./opensnoop -x PID COMM FD ERR PATH 18372 run -1 6 /dev/tty 18373 run -1 6 /dev/tty 18373 multilog -1 13 lock 18372 multilog -1 13 lock 18384 df -1 2 /usr/share/locale/en_US.UTF-8/LC_MESSAGES/coreutils.mo 18384 df -1 2 /usr/share/locale/en_US.utf8/LC_MESSAGES/coreutils.mo 18384 df -1 2 /usr/share/locale/en_US/LC_MESSAGES/coreutils.mo 18384 df -1 2 /usr/share/locale/en.UTF-8/LC_MESSAGES/coreutils.mo 18384 df -1 2 /usr/share/locale/en.utf8/LC_MESSAGES/coreutils.mo 18384 df -1 2 /usr/share/locale/en/LC_MESSAGES/coreutils.mo 18385 run -1 6 /dev/tty 18386 run -1 6 /dev/tty 这里捕获了一个 df 命令无法打开 coreutils.mo 文件, 并尝试从其他目录打开.\n列 ERR 表示系统错误码, 2 代表 ENOENT: no such file or directory.\n可以使用 -d 选项设置最长跟踪持续时间. 例如, 要跟踪 2秒:\n# ./opensnoop -d 2 PID COMM FD ERR PATH 2191 indicator-multi 11 0 /sys/block 2191 indicator-multi 11 0 /sys/block 2191 indicator-multi 11 0 /sys/block 2191 indicator-multi 11 0 /sys/block 2191 indicator-multi 11 0 /sys/block 过滤进程名称 #  -n 选项可用于过滤进程名称(部分匹配):\n# ./opensnoop -n ed PID COMM FD ERR PATH 2679 sed 3 0 /etc/ld.so.cache 2679 sed 3 0 /lib/x86_64-linux-gnu/libselinux.so.1 2679 sed 3 0 /lib/x86_64-linux-gnu/libc.so.6 2679 sed 3 0 /lib/x86_64-linux-gnu/libpcre.so.3 2679 sed 3 0 /lib/x86_64-linux-gnu/libdl.so.2 2679 sed 3 0 /lib/x86_64-linux-gnu/libpthread.so.0 2679 sed 3 0 /proc/filesystems 2679 sed 3 0 /usr/lib/locale/locale-archive 2679 sed -1 2 2679 sed 3 0 /usr/lib/x86_64-linux-gnu/gconv/gconv-modules.cache 2679 sed 3 0 /dev/null 2680 sed 3 0 /etc/ld.so.cache 2680 sed 3 0 /lib/x86_64-linux-gnu/libselinux.so.1 2680 sed 3 0 /lib/x86_64-linux-gnu/libc.so.6 2680 sed 3 0 /lib/x86_64-linux-gnu/libpcre.so.3 2680 sed 3 0 /lib/x86_64-linux-gnu/libdl.so.2 2680 sed 3 0 /lib/x86_64-linux-gnu/libpthread.so.0 2680 sed 3 0 /proc/filesystems 2680 sed 3 0 /usr/lib/locale/locale-archive 2680 sed -1 2 ^C 这里捕获了 “sed” 命令，是因为命令中使用了命令名称部分匹配 “-n ed”\n过滤标志 #  -e 选项能打印出额外的列; 例如，以下输出包含传递给 open(2) 的标志(以八进制表示):\n# ./opensnoop -e PID COMM FD ERR FLAGS PATH 28512 sshd 10 0 00101101 /proc/self/oom_score_adj 28512 sshd 3 0 02100000 /etc/ld.so.cache 28512 sshd 3 0 02100000 /lib/x86_64-linux-gnu/libwrap.so.0 28512 sshd 3 0 02100000 /lib/x86_64-linux-gnu/libaudit.so.1 28512 sshd 3 0 02100000 /lib/x86_64-linux-gnu/libpam.so.0 28512 sshd 3 0 02100000 /lib/x86_64-linux-gnu/libselinux.so.1 28512 sshd 3 0 02100000 /lib/x86_64-linux-gnu/libsystemd.so.0 28512 sshd 3 0 02100000 /usr/lib/x86_64-linux-gnu/libcrypto.so.1.0.2 28512 sshd 3 0 02100000 /lib/x86_64-linux-gnu/libutil.so.1 -f 选项能基于 open(2) 调用的标志进行过滤, 比如:\n# ./opensnoop -e -f O_WRONLY -f O_RDWR PID COMM FD ERR FLAGS PATH 28084 clear_console 3 0 00100002 /dev/tty 28084 clear_console -1 13 00100002 /dev/tty0 28084 clear_console -1 13 00100001 /dev/tty0 28084 clear_console -1 13 00100002 /dev/console 28084 clear_console -1 13 00100001 /dev/console 28051 sshd 8 0 02100002 /var/run/utmp 28051 sshd 7 0 00100001 /var/log/wtmp 基于 cgroup 集进行过滤 #  \u0026ndash;cgroupmap 选项基于 cgroup 集进行过滤, 它用于使用外部创建的映射.\n# ./opensnoop --cgroupmap /sys/fs/bpf/test01 更多信息, 查看 docs/special_filtering.md\nUSAGE 说明 #  # ./opensnoop -h usage: opensnoop.py [-h] [-T] [-U] [-x] [-p PID] [-t TID] [--cgroupmap CGROUPMAP] [--mntnsmap MNTNSMAP] [-u UID] [-d DURATION] [-n NAME] [-e] [-f FLAG_FILTER] 跟踪 open() 系统调用 optional arguments: -h, --help show this help message and exit -T, --timestamp include timestamp on output -U, --print-uid include UID on output -x, --failed only show failed opens -p PID, --pid PID trace this PID only -t TID, --tid TID trace this TID only --cgroupmap CGROUPMAP trace cgroups in this BPF map only --mntnsmap MNTNSMAP trace mount namespaces in this BPF map on -u UID, --uid UID trace this UID only -d DURATION, --duration DURATION total duration of trace in seconds -n NAME, --name NAME only print process names containing this name -e, --extended_fields show extended fields -f FLAG_FILTER, --flag_filter FLAG_FILTER filter on flags argument (e.g., O_WRONLY) examples: ./opensnoop # trace all open() syscalls ./opensnoop -T # include timestamps ./opensnoop -U # include UID ./opensnoop -x # only show failed opens ./opensnoop -p 181 # only trace PID 181 ./opensnoop -t 123 # only trace TID 123 ./opensnoop -u 1000 # only trace UID 1000 ./opensnoop -d 10 # trace for 10 seconds only ./opensnoop -n main # only print process names containing \u0026#34;main\u0026#34; ./opensnoop -e # show extended fields ./opensnoop -f O_WRONLY -f O_RDWR # only print calls for writing ./opensnoop --cgroupmap mappath # only trace cgroups in this BPF map ./opensnoop --mntnsmap mappath # only trace mount namespaces in the map "},{"id":29,"href":"/posts/2203/bcc-tcplife-usage/","title":"bcc 之 tcplife 工具的使用","section":"文章","content":"这篇文档主要演示了 tcplife(Linux eBPF/bcc) 工具的使用.\n示例 #  tcplife 总结了在跟踪期间打开和关闭的 TCP 会话. 比如:\n# ./tcplife PID COMM LADDR LPORT RADDR RPORT TX_KB RX_KB MS 22597 recordProg 127.0.0.1 46644 127.0.0.1 28527 0 0 0.23 3277 redis-serv 127.0.0.1 28527 127.0.0.1 46644 0 0 0.28 22598 curl 100.66.3.172 61620 52.205.89.26 80 0 1 91.79 22604 curl 100.66.3.172 44400 52.204.43.121 80 0 1 121.38 22624 recordProg 127.0.0.1 46648 127.0.0.1 28527 0 0 0.22 3277 redis-serv 127.0.0.1 28527 127.0.0.1 46648 0 0 0.27 22647 recordProg 127.0.0.1 46650 127.0.0.1 28527 0 0 0.21 3277 redis-serv 127.0.0.1 28527 127.0.0.1 46650 0 0 0.26 [...] 这捕获了一个程序 “recordProg”, 它建立了一些到 “redis-serv” 的短暂的 TCP 连接, 每个连接持续大约 0.25 毫秒. 还有几个 “curl” 会话也被跟踪, 连接到端口 80, 持续了 91 和 121 毫秒.\n此工具对于工作负载表征和流量统计很有用: 识别正在发生的连接以及传输的字节.\n在这个例子中, 我上传了一个 10 Mbyte 的文件到服务器, 然后再次使用 scp 下载:\n# ./tcplife PID COMM LADDR LPORT RADDR RPORT TX_KB RX_KB MS 7715 recordProg 127.0.0.1 50894 127.0.0.1 28527 0 0 0.25 3277 redis-serv 127.0.0.1 28527 127.0.0.1 50894 0 0 0.30 7619 sshd 100.66.3.172 22 100.127.64.230 63033 5 10255 3066.79 7770 recordProg 127.0.0.1 50896 127.0.0.1 28527 0 0 0.20 3277 redis-serv 127.0.0.1 28527 127.0.0.1 50896 0 0 0.24 7793 recordProg 127.0.0.1 50898 127.0.0.1 28527 0 0 0.23 3277 redis-serv 127.0.0.1 28527 127.0.0.1 50898 0 0 0.27 7847 recordProg 127.0.0.1 50900 127.0.0.1 28527 0 0 0.24 3277 redis-serv 127.0.0.1 28527 127.0.0.1 50900 0 0 0.29 7870 recordProg 127.0.0.1 50902 127.0.0.1 28527 0 0 0.29 3277 redis-serv 127.0.0.1 28527 127.0.0.1 50902 0 0 0.30 7798 sshd 100.66.3.172 22 100.127.64.230 64925 10265 6 2176.15 [...] 可以看到 sshd 接收了 10 MB, 然后再传输出去. 看起来, 接收(3.07 秒)比传输(2.18 秒)慢.\n宽显 #  进程名称被截断为 10 个字符. 通过使用宽选项 -w, 列宽变为 16 个字符. IP 地址栏也更宽, 以适合 IPv6 地址:\n# ./tcplife -w PID COMM IP LADDR LPORT RADDR RPORT TX_KB RX_KB MS 26315 recordProgramSt 4 127.0.0.1 44188 127.0.0.1 28527 0 0 0.21 3277 redis-server 4 127.0.0.1 28527 127.0.0.1 44188 0 0 0.26 26320 ssh 6 fe80::8a3:9dff:fed5:6b19 22440 fe80::8a3:9dff:fed5:6b19 22 1 1 457.52 26321 sshd 6 fe80::8a3:9dff:fed5:6b19 22 fe80::8a3:9dff:fed5:6b19 22440 1 1 458.69 26341 recordProgramSt 4 127.0.0.1 44192 127.0.0.1 28527 0 0 0.27 3277 redis-server 4 127.0.0.1 28527 127.0.0.1 44192 0 0 0.32 添加时间戳 #  可以使用 -t 添加时间戳:\n# ./tcplife -t TIME(s) PID COMM LADDR LPORT RADDR RPORT TX_KB RX_KB MS 0.000000 5973 recordProg 127.0.0.1 47986 127.0.0.1 28527 0 0 0.25 0.000059 3277 redis-serv 127.0.0.1 28527 127.0.0.1 47986 0 0 0.29 1.022454 5996 recordProg 127.0.0.1 47988 127.0.0.1 28527 0 0 0.23 1.022513 3277 redis-serv 127.0.0.1 28527 127.0.0.1 47988 0 0 0.27 2.044868 6019 recordProg 127.0.0.1 47990 127.0.0.1 28527 0 0 0.24 2.044924 3277 redis-serv 127.0.0.1 28527 127.0.0.1 47990 0 0 0.28 3.069136 6042 recordProg 127.0.0.1 47992 127.0.0.1 28527 0 0 0.22 3.069204 3277 redis-serv 127.0.0.1 28527 127.0.0.1 47992 0 0 0.28 这表明 recordProg 进程每秒连接一次.\n另外 -T 选项可以使用 HH:MM:SS 格式时间戳.\n逗号分隔列表 #  -s 选项可以指定逗号分隔的列表模式. 这里同时使用了 -t 和 -T 类型的时间戳:\n# ./tcplife -stT TIME,TIME(s),PID,COMM,IP,LADDR,LPORT,RADDR,RPORT,TX_KB,RX_KB,MS 23:39:38,0.000000,7335,recordProgramSt,4,127.0.0.1,48098,127.0.0.1,28527,0,0,0.26 23:39:38,0.000064,3277,redis-server,4,127.0.0.1,28527,127.0.0.1,48098,0,0,0.32 23:39:39,1.025078,7358,recordProgramSt,4,127.0.0.1,48100,127.0.0.1,28527,0,0,0.25 23:39:39,1.025141,3277,redis-server,4,127.0.0.1,28527,127.0.0.1,48100,0,0,0.30 23:39:41,2.040949,7381,recordProgramSt,4,127.0.0.1,48102,127.0.0.1,28527,0,0,0.24 23:39:41,2.041011,3277,redis-server,4,127.0.0.1,28527,127.0.0.1,48102,0,0,0.29 23:39:42,3.067848,7404,recordProgramSt,4,127.0.0.1,48104,127.0.0.1,28527,0,0,0.30 23:39:42,3.067914,3277,redis-server,4,127.0.0.1,28527,127.0.0.1,48104,0,0,0.35 [...] 端口过滤 #  还有过滤本地/远端端口的选项. 这里就过滤了本地 22 和 80 端口.\n# ./tcplife.py -L 22,80 PID COMM LADDR LPORT RADDR RPORT TX_KB RX_KB MS 8301 sshd 100.66.3.172 22 100.127.64.230 58671 3 3 1448.52 [...] USAGE 说明 #  # ./tcplife.py -h usage: tcplife.py [-h] [-T] [-t] [-w] [-s] [-p PID] [-L LOCALPORT] [-D REMOTEPORT] [-4 | -6] 跟踪 TCP 会话的生命周期并进行总结 optional arguments: -h, --help show this help message and exit -T, --time include time column on output (HH:MM:SS) -t, --timestamp include timestamp on output (seconds) -w, --wide wide column output (fits IPv6 addresses) -s, --csv comma separated values output -p PID, --pid PID trace this PID only -L LOCALPORT, --localport LOCALPORT comma-separated list of local ports to trace. -D REMOTEPORT, --remoteport REMOTEPORT comma-separated list of remote ports to trace. -4, --ipv4 trace IPv4 family only -6, --ipv6 trace IPv6 family only examples: ./tcplife # trace all TCP connect()s ./tcplife -t # include time column (HH:MM:SS) ./tcplife -w # wider columns (fit IPv6) ./tcplife -stT # csv output, with times \u0026amp; timestamps ./tcplife -p 181 # only trace PID 181 ./tcplife -L 80 # only trace local port 80 ./tcplife -L 80,81 # only trace local ports 80 and 81 ./tcplife -D 80 # only trace remote port 80 ./tcplife -4 # only trace IPv4 family ./tcplife -6 # only trace IPv6 family "},{"id":30,"href":"/posts/2203/linux-error-code/","title":"Linux 常见错误码","section":"文章","content":"下表是 Linux 操作系统一些常见的错误代码和对应的错误描述:\n   数字 错误码 描述     1 EPERM Operation not permitted   2 ENOENT No such file or directory   3 ESRCH No such process   4 EINTR Interrupted system call   5 EIO I/O error   6 ENXIO No such device or address   7 E2BIG Argument list too long   8 ENOEXEC Exec format error   9 EBADF Bad file number   10 ECHILD No child processes   11 EAGAIN Try again   12 ENOMEM Out of memory   13 EACCES Permission denied   14 EFAULT Bad address   15 ENOTBLK Block device required   16 EBUSY Device or resource busy   17 EEXIST File exists   18 EXDEV Cross-device link   19 ENODEV No such device   20 ENOTDIR Not a directory   21 EISDIR Is a directory   22 EINVAL Invalid argument   23 ENFILE File table overflow   24 EMFILE Too many open files   25 ENOTTY Not a typewriter   26 ETXTBSY Text file busy   27 EFBIG File too large   28 ENOSPC No space left on device   29 ESPIPE Illegal seek   30 EROFS Read-only file system   31 EMLINK Too many links   32 EPIPE Broken pipe   33 EDOM Math argument out of domain of func   34 ERANGE Math result not representable   35 EDEADLK Resource deadlock would occur   36 ENAMETOOLONG File name too long   37 ENOLCK No record locks available   38 ENOSYS Function not implemented   39 ENOTEMPTY Directory not empty   40 ELOOP Too many symbolic links encountered   42 ENOMSG No message of desired type   43 EIDRM Identifier removed   44 ECHRNG Channel number out of range   45 EL2NSYNC Level 2 not synchronized   46 EL3HLT Level 3 halted   47 EL3RST Level 3 reset   48 ELNRNG Link number out of range   49 EUNATCH Protocol driver not attached   50 ENOCSI No CSI structure available   51 EL2HLT Level 2 halted   52 EBADE Invalid exchange   53 EBADR Invalid request descriptor   54 EXFULL Exchange full   55 ENOANO No anode   56 EBADRQC Invalid request code   57 EBADSLT Invalid slot   59 EBFONT Bad font file format   60 ENOSTR Device not a stream   61 ENODATA No data available   62 ETIME Timer expired   63 ENOSR Out of streams resources   64 ENONET Machine is not on the network   65 ENOPKG Package not installed   66 EREMOTE Object is remote   67 ENOLINK Link has been severed   68 EADV Advertise error   69 ESRMNT Srmount error   70 ECOMM Communication error on send   71 EPROTO Protocol error   72 EMULTIHOP Multihop attempted   73 EDOTDOT RFS specific error   74 EBADMSG Not a data message   75 EOVERFLOW Value too large for defined data type   76 ENOTUNIQ Name not unique on network   77 EBADFD File descriptor in bad state   78 EREMCHG Remote address changed   79 ELIBACC Can not access a needed shared library   80 ELIBBAD Accessing a corrupted shared library   81 ELIBSCN .lib section in a.out corrupted   82 ELIBMAX Attempting to link in too many shared libraries   83 ELIBEXEC Cannot exec a shared library directly   84 EILSEQ Illegal byte sequence   85 ERESTART Interrupted system call should be restarted   86 ESTRPIPE Streams pipe error   87 EUSERS Too many users   88 ENOTSOCK Socket operation on non-socket   89 EDESTADDRREQ Destination address required   90 EMSGSIZE Message too long   91 EPROTOTYPE Protocol wrong type for socket   92 ENOPROTOOPT Protocol not available   93 EPROTONOSUPPORT Protocol not supported   94 ESOCKTNOSUPPORT Socket type not supported   95 EOPNOTSUPP Operation not supported on transport endpoint   96 EPFNOSUPPORT Protocol family not supported   97 EAFNOSUPPORT Address family not supported by protocol   98 EADDRINUSE Address already in use   99 EADDRNOTAVAIL Cannot assign requested address   100 ENETDOWN Network is down   101 ENETUNREACH Network is unreachable   102 ENETRESET Network dropped connection because of reset   103 ECONNABORTED Software caused connection abort   104 ECONNRESET Connection reset by peer   105 ENOBUFS No buffer space available   106 EISCONN Transport endpoint is already connected   107 ENOTCONN Transport endpoint is not connected   108 ESHUTDOWN Cannot send after transport endpoint shutdown   109 ETOOMANYREFS Too many references: cannot splice   110 ETIMEDOUT Connection timed out   111 ECONNREFUSED Connection refused   112 EHOSTDOWN Host is down   113 EHOSTUNREACH No route to host   114 EALREADY Operation already in progress   115 EINPROGRESS Operation now in progress   116 ESTALE Stale NFS file handle   117 EUCLEAN Structure needs cleaning   118 ENOTNAM Not a XENIX named type file   119 ENAVAIL No XENIX semaphores available   120 EISNAM Is a named type file   121 EREMOTEIO Remote I/O error   122 EDQUOT Quota exceeded   123 ENOMEDIUM No medium found   124 EMEDIUMTYPE Wrong medium type   125 ECANCELED Operation Canceled   126 ENOKEY Required key not available   127 EKEYEXPIRED Key has expired   128 EKEYREVOKED Key has been revoked   129 EKEYREJECTED Key was rejected by service   130 EOWNERDEAD Owner died   131 ENOTRECOVERABLE State not recoverable      https://mariadb.com/kb/en/operating-system-error-codes/\n "},{"id":31,"href":"/posts/2203/nfs-options/","title":"load average 过高, mount nfs 问题处理","section":"文章","content":"周末, 有一台服务器告警: 系统负载过高, 最高的时候都已经到 100 +, 以下是排查\u0026amp;处理的具体过程.\n发现的问题/现象 #  uptime 显示 load average 都在70+ #  因为服务器是40核心, 原则上负载40是满负荷, 现在明显存在大量等待的任务. 继续往下分析进程, 看具体那个进程一直在堵塞.\nps -ef 执行到某一个进程就卡住了 #  命令执行如下:\n$ ps -ef ... root 40004 2912 0 Mar08 ? 00:00:33 containerd-shim -namespace moby -workdir /data/docker/containerd/daemon/ io.containerd.runtime.v1.linux/moby/\u0026lt;container-hash\u0026gt; 卡住了 根据命令中的  找到对应的 pod, 将其从当前节点移除. 移除之后, ps 命令以及其他系统命令可以成功执行. 被移除的 pod 分别是: 2个 prometheus、 1个 mysql.\n无法执行 umount 卸载 #  测试 mount 挂载正常, 但是 umount 失败, 解决办法:\n 先强制 umount  $ umount -f -l /mount-point # 命令解释 $ umount [options] \u0026lt;source\u0026gt; | \u0026lt;directory\u0026gt; Options: -f Force unmount (in case of an unreachable NFS system). (Requires kernel 2.1.116 or later.) -l Lazy unmount. Detach the filesystem from the filesystem hierarchy now, and cleanup all references to the filesystem as soon as it is not busy anymore. (Requires kernel 2.4.11 or later.)   https://linux.die.net/man/8/umount\n  kill占用进程  $ fuser –m –v /mount-point USER PID ACCESS COMMAND /mount-point: ... user1 21691 .rce. ls ... # 必须 -9 $ kill -9 21691 继续分析 #  通过阅读其他文档发现:\n网络原因导致某连接断开, 该连接进入持续的等待中 #  这种情况会提高负载, 这里涉及一个 nfs 挂载参数, 通过 mount | grep nfs 看到 kubelet 默认的挂载配置如下:\n$ mount | grep nfs 10.***.***.6:/3PAR_d11_Node1_FPG/3PAR_d11_Node1_VFS/paas_share_FS/***/fz***c on /var/lib/kubelet/pods/c6c26172-1030-4dad-8611-102636803a58/volumes/kubernetes.io ~nfs/pvc-a7e8bf54-c787-44a4-94d5-b849ec2d24bb type nfs4 (rw,relatime,vers=4.0,rsize=1048576,wsize=1048576, namlen=255,hard,proto=tcp,timeo=600,retrans=2, sec=sys,clientaddr=10.***.***.140,local_lock=none, addr=10.***.***.6) ... 括号中可以看到: rw, nfs4.0, hard, tcp 等配置信息, 简单说一下 mount 配置:\nmount -F nfs [-o mount-options] server:/directory /mount-point # 比如: mount -F nfs -o hard 192.168.0.10:/nfs /nfs # 同时使用多个参数使用逗号分隔： mount -t nfs -o timeo=3,udp,hard 192.168.0.30:/tmp /nfs -o mount-options 指定可以用来挂载 NFS 文件系统的挂载选项。有关常用的 mount 选项的列表，请参见 表 19–2\n-o hard/soft 如果服务器没有响应，有 hard 和 soft 两种处理方式, soft 选项表示返回了错误, hard 选项表示继续重试请求, 直到服务器响应为止. 缺省情况下使用 hard.\n详细解释如下:\nsoft / hard Determines the recovery behavior of the NFS client after an NFS request times out. If neither option is specified (or if the hard option is specified), NFS requests are retried indefinitely. If the soft option is specified, then the NFS client fails an NFS request after retrans retransmissions have been sent, causing the NFS client to return an error to the calling application. NB: A so-called \u0026#34;soft\u0026#34; timeout can cause silent data corruption in certain cases. As such, use the soft option only when client responsiveness is more important than data integrity. Using NFS over TCP or increasing the value of the retrans option may mitigate some of the risks of using the soft option.   https://linux.die.net/man/5/nfs\n 在某些情况下, soft 模式可能会导致静默数据损坏. 因此，仅当客户端响应比数据完整性更重要时才使用 soft. 考虑到 nfs 作为 kubernetes 持久卷使用的话, 数据完整性肯定是比较重要的, 所以还是沿用官方默认配置 hard. 并未做修改.\n官方不建议使用 nfs 作为 prometheus 后端存储 #  阅读prometheus 文档发现:\nCAUTION: Non-POSIX compliant filesystems are not supported for Prometheus\u0026#39; local storage as unrecoverable corruptions may happen. NFS filesystems (including AWS\u0026#39;s EFS) are not supported. NFS could be POSIX-compliant, but most implementations are not. It is strongly recommended to use a local filesystem for reliability.   https://prometheus.io/docs/prometheus/latest/storage/\n 听人劝吃饱饭, 文档中建议用本地文件系统, 所以使用 rancher/local-path-provisioner 替换了 prometheus 原来的 nfs 后端存储.\n"},{"id":32,"href":"/posts/2203/openldap-docker/","title":"容器化部署 openldap","section":"文章","content":"使用容器化安装非常便捷, 参考 osixia/openldap仓库使用说明安装即可, 如下:\ndocker stop openldap \u0026amp;\u0026amp; docker rm openldap \u0026amp;\u0026amp; \\ docker run --name openldap --detach \\  -p 389:389 \\  -p 636:636 \\  --env LDAP_ORGANISATION=\u0026#34;Rutron Net\u0026#34; \\  --env LDAP_DOMAIN=\u0026#34;rutron.net\u0026#34; \\  --env LDAP_ADMIN_PASSWORD=\u0026#34;your-password\u0026#34; \\  --env LDAP_READONLY_USER=true \\  --env LDAP_TLS_VERIFY_CLIENT=try \\  --volume /data/openldap/data:/var/lib/ldap \\  --volume /data/openldap/slapd.d:/etc/ldap/slapd.d \\  --hostname ldap.rutron.net \\  osixia/openldap:1.5.0 好了, 现在该服务同时支持 ldap 和 ldaps 协议, 有一个初始化的账号 readonly/readonly, 可以使用了~\n"},{"id":33,"href":"/posts/2202/cannot-allocate-memory/","title":"解决 kubelet cannot allocate memory 错误","section":"文章","content":"问题描述 #  查看 pod 相关 events 如下：\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 18m default-scheduler Successfully assigned container-186002196200947712/itms-5f6d7798-wrpjj to 10.206.65.144 Warning FailedCreatePodContainer 3m31s (x71 over 18m) kubelet unable to ensure pod container exists: failed to create container for [kubepods burstable pod31f4c93c-c3a1-49ad-b091-0802c5f1d396] : mkdir /sys/fs/cgroup/memory/kubepods/burstable/pod31f4c93c-c3a1-49ad-b091-0802c5f1d396: cannot allocate memory 这是内核bug，建议升级内核\n"},{"id":34,"href":"/posts/2202/Kubespray-kubernetes-setup/","title":"使用Kubespray安装kubernetes的教程","section":"文章","content":"本文使用 kubespray 容器部署 kubernetes v1.22, 提供了从国外搬运的离线软件包/容器镜像. 仅需要几步即可部署高可用集群. 所有离线文件都来自官方下载 kubespray 安装过程会进行软件包验证, 放心使用.\n前提 #   禁用防火墙 重要: 本文使用 kubespray 的容器环境部署, 为避免影响节点部署(特别是 Runtime 部署), 所以需要一台独立于集群外的服务器执行下面的命令, 这台服务器安装 docker 19.03+ 并到所有节点SSH免密进入. 目标服务器要允许 IPv4 转发, 如果要给 pods 和 services 用 IPv6, 目标服务器要允许 IPv6 转发.  注意: 下面配置是适合 kubespray 的配置, 实际配置取决于集群规模.\n Master  Memory: 1500 MB   Node  Memory: 1024 MB    文件和镜像搬运 #  由于国内网络的限制, 直接使用Kubespray是无法成功安装集群的. 所以我写了一个脚本, 将文件上传至阿里云OSS, 镜像上传至阿里云ACR. 下面是脚本的内容, 请在国外服务器上运行, 比如GitHub Actions、Google云控制台等.\n#!/bin/bash  set -x KUBSPRAY_VERSION=v2.18.1 OSS_ENDPOINT=\u0026lt;OSS ENDPOINT\u0026gt; OSS_ACCESS_KEY_ID=\u0026lt;OSS ACCESS KEY ID\u0026gt; OSS_ACCESS_KEY=\u0026lt;OSS ACCESS KEY\u0026gt; OSS_CLOUD_URL=\u0026lt;Bucket和文件路径\u0026gt; ACR_REPO=\u0026lt;ACR地址\u0026gt; ACR_USERNAME=\u0026lt;ACR用户名\u0026gt; ACR_PASSWORD=\u0026lt;ACR密码\u0026gt; MY_IMAGE_REPO=${ACR_REPO}/\u0026lt;ACR命名空间\u0026gt; wget -O kubespray-src.zip https://github.com/kubernetes-sigs/kubespray/archive/refs/tags/${KUBSPRAY_VERSION}.zip unzip -q kubespray-src.zip SRC_PATH=$(unzip -l kubespray-src.zip | sed -n \u0026#39;5p\u0026#39; | awk \u0026#39;{print $4}\u0026#39;) cd ${SRC_PATH}contrib/offline ./generate_list.sh -i inventory/sample/inventory.ini cat temp/files.list cat temp/images.list echo \u0026#34;Download files and upload to OSS\u0026#34; wget -qx -P temp/files -i temp/files.list tree temp/ wget -q https://gosspublic.alicdn.com/ossutil/1.7.13/ossutil64 \u0026amp;\u0026amp; chmod 755 ossutil64 ./ossutil64 \\  -e $OSS_ENDPOINT \\  -i $OSS_ACCESS_KEY_ID \\  -k $OSS_ACCESS_KEY \\  cp temp/files/ oss://${OSS_CLOUD_URL} -ruf --acl=public-read echo \u0026#34;Copy images to ACR\u0026#34; cat \u0026gt;\u0026gt; temp/images.list \u0026lt;\u0026lt;EOF quay.io/metallb/speaker:v0.10.3 quay.io/metallb/controller:v0.10.3 quay.io/kubespray/kubespray:v2.18.1 EOF skopeo login -u $ACR_USERNAME -p $ACR_PASSWORD $ACR_REPO for image in $(cat temp/images.list) do myimage=${image#*/} myimage=${MY_IMAGE_REPO}/${myimage/\\//_} echo $myimage \u0026gt;\u0026gt; temp/myimages.list skopeo copy docker://${image} docker://${myimage} done cat temp/myimages.list 运行kubespray容器 #  直接使用 kubespray 提供的镜像能让我们避免处理各依赖包的复杂的版本问题\ndocker run --rm -it \\  -v ${PWD}/inventory/mycluster:/kubespray/inventory/mycluster \\  -v ${HOME}/.ssh/id_rsa:/root/.ssh/id_rsa \\  registry.cn-beijing.aliyuncs.com/llaoj/kubespray_kubespray:v2.18.1 bash # 后面的命令均在容器内部执行 cp -r inventory/sample inventory/mycluster 编辑inventory文件 #  Ansible 可同时操作属于一个组的多台主机, 组和主机之间的关系通过 inventory 文件配置. 修改inventory/mycluster/inventory.ini即可, 该文件已经配置好了供你修改的模版:\n# ## Configure \u0026#39;ip\u0026#39; variable to bind kubernetes services on a # ## different ip than the default iface # ## We should set etcd_member_name for etcd cluster. The node that is not a etcd member do not need to set the value, or can set the empty string value. [all] # node1 ansible_host=95.54.0.12 # ip=10.3.0.1 etcd_member_name=etcd1 # node2 ansible_host=95.54.0.13 # ip=10.3.0.2 etcd_member_name=etcd2 # node3 ansible_host=95.54.0.14 # ip=10.3.0.3 etcd_member_name=etcd3 # node4 ansible_host=95.54.0.15 # ip=10.3.0.4 etcd_member_name=etcd4 # node5 ansible_host=95.54.0.16 # ip=10.3.0.5 etcd_member_name=etcd5 # node6 ansible_host=95.54.0.17 # ip=10.3.0.6 etcd_member_name=etcd6 # ## configure a bastion host if your nodes are not directly reachable # [bastion] # bastion ansible_host=x.x.x.x ansible_user=some_user [kube_control_plane] # node1 # node2 # node3 [etcd] # node1 # node2 # node3 [kube_node] # node2 # node3 # node4 # node5 # node6 [calico_rr] [k8s_cluster:children] kube_control_plane kube_node calico_rr 自定义部署 #  按照集群的规划, 按说明修改下面两个文件中的配置:\nvi inventory/mycluster/group_vars/all/*.yml vi inventory/mycluster/group_vars/k8s_cluster/*.yml 请认真阅读每一个配置, 其中有个关于 CIDR 的配置如下(非常重要), 贴出来供参考:\n# CNI 插件配置, 可选 cilium, calico, weave 或 flannelkube_network_plugin:calico# sevice CIDR 配置# 不能与其他网络重叠kube_service_addresses:10.233.0.0/18# pod CIDR 配置# 不能与其他网络重叠kube_pods_subnet:10.233.64.0/18# 配置内部网络节点大小# 配置每个节点可分配的 ip 个数# 注意: 每节点最大 pods 数也受 kubelet_max_pods 限制, 默认 110# 例子1:# 最高64个节点, 每节点最高 254 或 kubelet_max_pods(两个取最小的) 个 pods # - kube_pods_subnet: 10.233.64.0/18# - kube_network_node_prefix: 24# - kubelet_max_pods: 110# 例子2:# 最高128个节点, 每节点最高 126 或 kubelet_max_pods(两个取最小的) 个 pods # - kube_pods_subnet: 10.233.64.0/18# - kube_network_node_prefix: 25# - kubelet_max_pods: 110kube_network_node_prefix:24 所有配置文件基本不需要修改, 如非必须, 采用默认值即可. 但要了解每个配置项作用.\n 更换文件/镜像地址 #  将我们上面搬运到国内的文件包/镜像的地址配置上, 并放在inventory/mycluster/group_vars/all/文件夹下:\nvi inventory/mycluster/group_vars/all/files-images.yaml 将下面的内容是我搬运之后的地址, 你可以直接使用. 或者使用你的地址替换oss_files_repo和acr_image_repo两个地址\n# filesoss_files_repo:\u0026#34;https://rutron.oss-cn-beijing.aliyuncs.com/kubernetes\u0026#34;kubelet_download_url:\u0026#34;{{ oss_files_repo }}/storage.googleapis.com/kubernetes-release/release/{{ kube_version }}/bin/linux/{{ image_arch }}/kubelet\u0026#34;kubectl_download_url:\u0026#34;{{ oss_files_repo }}/storage.googleapis.com/kubernetes-release/release/{{ kube_version }}/bin/linux/{{ image_arch }}/kubectl\u0026#34;kubeadm_download_url:\u0026#34;{{ oss_files_repo }}/storage.googleapis.com/kubernetes-release/release/{{ kubeadm_version }}/bin/linux/{{ image_arch }}/kubeadm\u0026#34;etcd_download_url:\u0026#34;{{ oss_files_repo }}/github.com/coreos/etcd/releases/download/{{ etcd_version }}/etcd-{{ etcd_version }}-linux-{{ image_arch }}.tar.gz\u0026#34;flannel_cni_download_url:\u0026#34;{{ oss_files_repo }}/github.com/flannel-io/cni-plugin/releases/download/{{ flannel_cni_version }}/flannel-{{ image_arch }}\u0026#34;cni_download_url:\u0026#34;{{ oss_files_repo }}/github.com/containernetworking/plugins/releases/download/{{ cni_version }}/cni-plugins-linux-{{ image_arch }}-{{ cni_version }}.tgz\u0026#34;calicoctl_download_url:\u0026#34;{{ oss_files_repo }}/github.com/projectcalico/calicoctl/releases/download/{{ calico_ctl_version }}/calicoctl-linux-{{ image_arch }}\u0026#34;calico_crds_download_url:\u0026#34;{{ oss_files_repo }}/github.com/projectcalico/calico/archive/{{ calico_version }}.tar.gz\u0026#34;crictl_download_url:\u0026#34;{{ oss_files_repo }}/github.com/kubernetes-sigs/cri-tools/releases/download/{{ crictl_version }}/crictl-{{ crictl_version }}-{{ ansible_system | lower }}-{{ image_arch }}.tar.gz\u0026#34;helm_download_url:\u0026#34;{{ oss_files_repo }}/get.helm.sh/helm-{{ helm_version }}-linux-{{ image_arch }}.tar.gz\u0026#34;runc_download_url:\u0026#34;{{ oss_files_repo }}/github.com/opencontainers/runc/releases/download/{{ runc_version }}/runc.{{ image_arch }}\u0026#34;crun_download_url:\u0026#34;{{ oss_files_repo }}/github.com/containers/crun/releases/download/{{ crun_version }}/crun-{{ crun_version }}-linux-{{ image_arch }}\u0026#34;kata_containers_download_url:\u0026#34;{{ oss_files_repo }}/github.com/kata-containers/kata-containers/releases/download/{{ kata_containers_version }}/kata-static-{{ kata_containers_version }}-{{ ansible_architecture }}.tar.xz\u0026#34;gvisor_runsc_download_url:\u0026#34;{{ oss_files_repo }}/storage.googleapis.com/gvisor/releases/release/{{ gvisor_version }}/{{ ansible_architecture }}/runsc\u0026#34;gvisor_containerd_shim_runsc_download_url:\u0026#34;{{ oss_files_repo }}/storage.googleapis.com/gvisor/releases/release/{{ gvisor_version }}/{{ ansible_architecture }}/containerd-shim-runsc-v1\u0026#34;nerdctl_download_url:\u0026#34;{{ oss_files_repo }}/github.com/containerd/nerdctl/releases/download/v{{ nerdctl_version }}/nerdctl-{{ nerdctl_version }}-{{ ansible_system | lower }}-{{ image_arch }}.tar.gz\u0026#34;krew_download_url:\u0026#34;{{ oss_files_repo }}/github.com/kubernetes-sigs/krew/releases/download/{{ krew_version }}/krew-{{ host_os }}_{{ image_arch }}.tar.gz\u0026#34;containerd_download_url:\u0026#34;{{ oss_files_repo }}/github.com/containerd/containerd/releases/download/v{{ containerd_version }}/containerd-{{ containerd_version }}-linux-{{ image_arch }}.tar.gz\u0026#34;# imagesacr_image_repo:\u0026#34;registry.cn-beijing.aliyuncs.com/llaoj\u0026#34;kube_image_repo:\u0026#34;{{ acr_image_repo }}\u0026#34;netcheck_server_image_repo:\u0026#34;{{ acr_image_repo }}/mirantis_k8s-netchecker-server\u0026#34;netcheck_agent_image_repo:\u0026#34;{{ acr_image_repo }}/mirantis_k8s-netchecker-agent\u0026#34;etcd_image_repo:\u0026#34;{{ acr_image_repo }}/coreos_etcd\u0026#34;cilium_image_repo:\u0026#34;{{ acr_image_repo }}/cilium_cilium\u0026#34;cilium_init_image_repo:\u0026#34;{{ acr_image_repo }}/cilium_cilium-init\u0026#34;cilium_operator_image_repo:\u0026#34;{{ acr_image_repo }}/cilium_operator\u0026#34;multus_image_repo:\u0026#34;{{ acr_image_repo }}/k8snetworkplumbingwg_multus-cni\u0026#34;flannel_image_repo:\u0026#34;{{ acr_image_repo }}/coreos_flannel\u0026#34;calico_node_image_repo:\u0026#34;{{ acr_image_repo }}/calico_node\u0026#34;calico_cni_image_repo:\u0026#34;{{ acr_image_repo }}/calico_cni\u0026#34;calico_flexvol_image_repo:\u0026#34;{{ acr_image_repo }}/calico_pod2daemon-flexvol\u0026#34;calico_policy_image_repo:\u0026#34;{{ acr_image_repo }}/calico_kube-controllers\u0026#34;calico_typha_image_repo:\u0026#34;{{ acr_image_repo }}/calico_typha\u0026#34;weave_kube_image_repo:\u0026#34;{{ acr_image_repo }}/weaveworks_weave-kube\u0026#34;weave_npc_image_repo:\u0026#34;{{ acr_image_repo }}/weaveworks_weave-npc\u0026#34;kube_ovn_container_image_repo:\u0026#34;{{ acr_image_repo }}/kubeovn_kube-ovn\u0026#34;kube_router_image_repo:\u0026#34;{{ acr_image_repo }}/cloudnativelabs_kube-router\u0026#34;pod_infra_image_repo:\u0026#34;{{ acr_image_repo }}/pause\u0026#34;install_socat_image_repo:\u0026#34;{{ acr_image_repo }}/xueshanf_install-socat\u0026#34;nginx_image_repo:\u0026#34;{{ acr_image_repo }}/library_nginx\u0026#34;haproxy_image_repo:\u0026#34;{{ acr_image_repo }}/library_haproxy\u0026#34;coredns_image_repo:\u0026#34;{{ acr_image_repo }}/coredns_coredns\u0026#34;nodelocaldns_image_repo:\u0026#34;{{ acr_image_repo }}/dns_k8s-dns-node-cache\u0026#34;dnsautoscaler_image_repo:\u0026#34;{{ acr_image_repo }}/cpa_cluster-proportional-autoscaler-amd64\u0026#34;registry_image_repo:\u0026#34;{{ acr_image_repo }}/library_registry\u0026#34;metrics_server_image_repo:\u0026#34;{{ acr_image_repo }}/metrics-server_metrics-server\u0026#34;addon_resizer_image_repo:\u0026#34;{{ acr_image_repo }}/addon-resizer\u0026#34;local_volume_provisioner_image_repo:\u0026#34;{{ acr_image_repo }}/sig-storage_local-volume-provisioner\u0026#34;cephfs_provisioner_image_repo:\u0026#34;{{ acr_image_repo }}/external_storage_cephfs-provisioner\u0026#34;rbd_provisioner_image_repo:\u0026#34;{{ acr_image_repo }}/external_storage_rbd-provisioner\u0026#34;local_path_provisioner_image_repo:\u0026#34;{{ acr_image_repo }}/rancher_local-path-provisioner\u0026#34;ingress_nginx_controller_image_repo:\u0026#34;{{ acr_image_repo }}/ingress-nginx_controller\u0026#34;alb_ingress_image_repo:\u0026#34;{{ acr_image_repo }}/amazon_aws-alb-ingress-controller\u0026#34;cert_manager_controller_image_repo:\u0026#34;{{ acr_image_repo }}/jetstack_cert-manager-controller\u0026#34;cert_manager_cainjector_image_repo:\u0026#34;{{ acr_image_repo }}/jetstack_cert-manager-cainjector\u0026#34;cert_manager_webhook_image_repo:\u0026#34;{{ acr_image_repo }}/jetstack_cert-manager-webhook\u0026#34;csi_attacher_image_repo:\u0026#34;{{ acr_image_repo }}/sig-storage_csi-attacher\u0026#34;csi_provisioner_image_repo:\u0026#34;{{ acr_image_repo }}/sig-storage_csi-provisioner\u0026#34;csi_snapshotter_image_repo:\u0026#34;{{ acr_image_repo }}/sig-storage_csi-snapshotter\u0026#34;snapshot_controller_image_repo:\u0026#34;{{ acr_image_repo }}/sig-storage_snapshot-controller\u0026#34;csi_resizer_image_repo:\u0026#34;{{ acr_image_repo }}/sig-storage_csi-resizer\u0026#34;csi_node_driver_registrar_image_repo:\u0026#34;{{ acr_image_repo }}/sig-storage_csi-node-driver-registrar\u0026#34;cinder_csi_plugin_image_repo:\u0026#34;{{ acr_image_repo }}/k8scloudprovider_cinder-csi-plugin\u0026#34;aws_ebs_csi_plugin_image_repo:\u0026#34;{{ acr_image_repo }}/amazon_aws-ebs-csi-driver\u0026#34;dashboard_image_repo:\u0026#34;{{ acr_image_repo }}/kubernetesui_dashboard-amd64\u0026#34;dashboard_metrics_scraper_repo:\u0026#34;{{ acr_image_repo }}/kubernetesui_metrics-scraper\u0026#34;metallb_speaker_image_repo:\u0026#34;{{ acr_image_repo }}/metallb_speaker\u0026#34;metallb_controller_image_repo:\u0026#34;{{ acr_image_repo }}/metallb_controller\u0026#34;执行部署 #  在容器内执行命令:\n安装集群 #  ansible-playbook -i inventory/mycluster/inventory.ini cluster.yml 扩充节点 #  修改inventory/mycluster/inventory.ini文件, 添加好节点并执行:\nansible-playbook -i inventory/mycluster/inventory.ini scale.yml 至此, 完成~\n问题列表 #   报错: No package matching \u0026lsquo;container-selinux\u0026rsquo; found available, installed or updated  fatal: [hde-ceno1]: FAILED! =\u0026gt; {\u0026#34;attempts\u0026#34;: 4, \u0026#34;changed\u0026#34;: false, \u0026#34;msg\u0026#34;: \u0026#34;No package matching \u0026#39;container-selinux\u0026#39; found available, installed or updated\u0026#34;, \u0026#34;rc\u0026#34;: 126, \u0026#34;results\u0026#34;: [\u0026#34;libselinux-python-2.5-15.el7.x86_64 providing libselinux-python is already installed\u0026#34;, \u0026#34;7:device-mapper-libs-1.02.170-6.el7.x86_64 providing device-mapper-libs is already installed\u0026#34;, \u0026#34;nss-3.44.0-7.el7_7.x86_64 providing nss is already installed\u0026#34;, \u0026#34;No package matching \u0026#39;container-selinux\u0026#39; found available, installed or updated\u0026#34;]} 解决办法: 打开链接找到服务器所需要的 container-selinux 包, 选择最新的包复制链接地址, 在问题机器上执行:\nyum install -y http://mirror.centos.org/centos/7/extras/x86_64/Packages/container-selinux-2.119.2-1.911c772.el7_8.noarch.rpm  替换命令中的 centos 包地址即可, 执行完毕之后再次执行部署.\n "},{"id":35,"href":"/posts/2202/intro-kubernetes/","title":"[PPT] 实践中总结 Kubernetes 必须了解的核心内容","section":"文章","content":"PPT 分享 #  以下是 \u0026lt;实践中总结 Kubernetes 必须了解的核心内容\u0026gt; 主题分享 PPT\n                                                                            完~\n"},{"id":36,"href":"/posts/2202/what-are-kubernetes-pods-anyway/","title":"kubernetes 中的 pod 究竟是什么","section":"文章","content":"前言 #  kubernetes 中 pod 的设计是一个伟大的发明, 今天我很有必要去聊一下 pod 和 container, 探究一下它们究竟是什么? kubernetes 官方文档中关于 pod 概念介绍提供了一个完整的解释, 但写的不够详细, 表达过于专业, 但还是很推荐大家阅读一下. 当然这篇文档应该更接地气.\n容器真的存在吗? #  linux 中是没有容器这个概念的, 容器就是 linux 中的普通进程, 它使用了 linux 内核提供的两个重要的特性: namespace \u0026amp; cgroups.\nnamespace 提供了一种隔离的特性, 让它之外的内容隐藏, 给它下面的进程一个不被干扰的运行环境(其实不完全,下面说) .\nnamespace 包含:\n hostname Process IDs File System Network Interface Inter-Process Communication(IPC)  接上面, 其实 namespace 内部的进程并不是完全不和外面的进程产生影响的. 进程可以不受限制的使用物理机上的所有资源, 这样就会导致其他进程无资源可用. 所以, 为了限制进程资源使用, linux 提供了另一种特性 cgroups. 进程可以像在 namespace 中运行, 但是 cgroups 限制了进程的可以使用的资源. 这些资源包括:\n CPU RAM block I/O network I/O etc.  CPU 通常按照毫核来限制(单位:m), 1000m=1C; 内存按照RAM的字节数来限制. 进程可以在 cgroups 设置的资源限制范围内运行, 不允许超限使用, 比如, 超过内存限制就会报 OOM(out of memory) 的错误.\n需要特别说明的是, 上面提到的 namespace \u0026amp; cgroup 都是 Linux 独立的特性, 你可以使用上面提到的 namespace 中的一个或者多个. namespace \u0026amp; cgroup 作用到一组或者一个进程上. 你可以把多个进程放在一个 namespace 中, 这样它们就可以彼此交互, 或者 把他们放在一个 cgroups 中, 这样他们就可以共享一个CPU \u0026amp; Mem 资源限制.\n组合容器 #  我们都用过 docker, 当我们启动一个容器的时候, docker 会帮我们给每一个容器创建它们自己的 namespace \u0026amp; cgroups. 这应该就是我们理解的容器.\n 如图, 容器本身还是比较独立的, 他们可能会有映射到主机的端口和卷, 这样就可以和外面通信. 但是我们也可以通过一些命令将多个容器组合到一组namespace中, 下面我们举个例子说明:\n首先, 创建一个 nginx 容器:\n# cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; nginx.conf \u0026gt; error_log stderr; \u0026gt; events { worker_connections 1024; } \u0026gt; http { \u0026gt; access_log /dev/stdout combined; \u0026gt; server { \u0026gt; listen 80 default_server; \u0026gt; server_name example.com www.example.com; \u0026gt; location / { \u0026gt; proxy_pass http://127.0.0.1:2368; \u0026gt; } \u0026gt; } \u0026gt; } \u0026gt; EOF # docker run -d --name nginx -v `pwd`/nginx.conf:/etc/nginx/nginx.conf -p 8080:80 --ipc=shareable nginx  接着, 我们再启动一个 ghost 容器, ghost 是一个开源的博客系统, 同时我们添加几个额外的命令到 nginx 容器上.\n# docker run -d --name ghost --net=container:nginx --ipc=container:nginx --pid=container:nginx ghost 好了, 现在 nginx 容器可以通过 localhost 将请求代理到 ghost 容器, 访问 http://localhost:8080试试, 你可以通过 nginx 反向代理看到一个 ghost 博客. 上面的命令就把一组容器组合到里同一组 namespace 中, 容器彼此之间可以互相发现/通信.\n就像这样:\n 某种意义上, pod 就是一组容器 #  现在我们已经知道, 我们可以把一组进程组合到一个 namespace \u0026amp; cgroups 中, 这就是 kubernetes 中的 pod. pod 允许你定义你要运行的容器, 然后 kubernetes 会帮正确的配置 namespace \u0026amp; cgroups. 它稍微复杂的一点是, 网络这块它没用 docker network, 而是用到了 CNI(通用网络接口), 但原理都差不多.\n 按照上述方式创建的 pod, 更像是运行在同一台机器上, 他们之间可以通过 localhost 通信, 可以共享存储卷. 甚至他们可以使用 IPC 或者互相发送 HUP / TERM 这类信号.\n我们再举个例子, 如下图, 我们运行一个 nginx 反向代理 app, 再运行一个 confd, 当 app 实例增加或减少的时候去动态配置 nginx.conf 并重启 nginx, etcd 中存储了 app 的 ip 地址. 当 ip 列表发生变化, confd 会收到 etcd 发的通知, 并更新 nginx.conf 并给 nginx 发送一个 HUP 信号, nginx 收到 HUP 信号会重启.\n 如果用 docker, 你大概会把 nginx 和 confd 放在一个容器中. 由于 docker 只有一个 entrypoint, 所以你要启动一个类似 supervisord 一样的进程管理器 来让 nginx 和 confd 都运行起来. 你每启动一个 nginx 副本就要启动一个 supervisord, 这不好吧. 更重要的是, docker 只知道 supervisord 的状态, 因为它只有一个 entrypoint. 它看不到里面的所有进程, 这就意味着, 你用 docker 提供的工具获取不到他们的信息. 一旦 nginx Crash-Restart Loop, docker 一点办法没有.\n 通过 pod , kubernetes 能管理每一个进程, 看到他们的状态, 它可以通过 api 将进程状态信息暴露给用户, 或者提供进程崩溃时重启/记录日志等服务.\n 把容器当作接口 #  使用 pod 这种组织容器的方式, 可以把容器当作提供各种功能的 \u0026ldquo;接口\u0026rdquo;. 它不同于传统意义上的 web 接口. 更像是可以被容器所使用的某种抽象意义的接口.\n我们拿上面 nginx+confd 的例子来说, confd 不需要知道任何 nginx 进程的东西, 它就只需要去 watch etcd 然后给 nginx 进程发送 HUP 信号或者执行个命令. 而且你可以把 nginx 替换成其他任何类型的应用, 以这样的模式来使用 confd 的这种能力. 这种模式下, confd 通常被称作 \u0026ldquo;sidecar container\u0026rdquo; 边车容器, 下面这图就很形象.\n 像 istio 这样的服务网格项目, 也是, 给应用程序容器放置一个边车容器来提供服务路由, 遥测, 网络策略等功能, 但是对应用程序并没有做任何侵略性更改. 你也可以使用多个边车容器来组织 pod, 比如在一个 pod 中同时放置 confd \u0026amp; istio 边车容器. 用这样的方式, 可以构建更加复杂可靠的系统, 同时还能保持每个应用的独立性和简单性.\n参考 #   What are Kubernetes Pods Anyway?\n What even is a container: namespaces and cgroups\nvideo: Cgroups, namespaces, and beyond: what are containers made from?\n"},{"id":37,"href":"/posts/2202/prometheus-operator-design/","title":"Prometheus Operator 设计思路","section":"文章","content":"设计 #  这篇文章介绍了 Prometheus Operator 的几种自定义资源 (CRD):\n Prometheus Alertmanager ThanosRuler ServiceMonitor PodMonitor Probe PrometheusRule AlertmanagerConfig  Prometheus #  它定义了在 Kubernetes 集群中安装 Prometheus 的方式. 它提供了一些配置项, 比如副本数、持久卷还有接收告警的 Alertmanagers.\n对于每一个 Prometheus 资源, Operator 会在同一 namespace 中部署一个经过正确配置的 StatefulSet. 其配置文件 secret/prometheus-name 会被挂载到它的 pod 上.\n它明确了 Prometheus 实例选择 ServiceMonitors 所用的标签. 任何时候, 一旦 ServiceMonitors 和 Prometheus 资源有更改, Operator 会根据他们生成重新配置文件, 然后更新到上述 Secret 配置文件中.\n如果没有提供选择 ServiceMonitors 所用标签, 用户也可以自己管理 Secret, 同时该 Secret 也享受 Operator 配置安装 Prometheus 的能力.\nAlertmanager #  该资源定义了在 Kubernetes 中配置安装 Alertmanager 的方式. 它提供了配置副本数和持久卷的选项.\n对于每一个 Alertmanager 资源, Operator 会在同一个 namespace 中启动一个正确配置的 StatefulSet. Alertmanager pods 会挂载一个 secret/alertmanager-name, 它包含一个键为 alertmanager.yaml 的配置文件.\n当配置了两个以上的副本, Operator 会启动 Alertmanager 高可用模式.\nThanosRuler #  该资源定义了 Kubernetes 中如何配置安装 Thanos Ruler. 借助 Thanos Ruler, 记录和告警规则可以被多个 Prometheus 实例处理.\n一个 ThanosRuler 实例要求至少一个 queryEndpoint, 它指向 Thanos 查询器 或者 Prometheus 实例的位置. queryEndpoints 会配置 Thanos 运行时的 --query 参数. 了解 Thanos.\nServiceMonitor #  该资源允许以声明方式定义应如何监视一组动态 Service, 使用标签来选择哪些 Service 需要监控. 这是一种约定, 来定义服务如何公开指标, 按照该约定 Prometheus 会自动发现新服务, 而不需要重新配置.\n对于 Prometheus 监控的任何程序在 Kubernetes 中都要求有对应的 Endpoints 对象. Endpoints 对象本质上是 IP 地址列表. 通常，Endpoints 对象由 Service 对象填充。 Service 对象通过标签选择器发现 Pod 并将其添加到 Endpoints 对象中。\n一个 Service 可能会暴露一个或者多个端口, 通常，它会生成一个 endpoints 列表, 指向一个具体的 Pod, 这和 kubernetes 中的 Endpoints 一样. ServiceMonitor 会自动发现 Endpoints 对象并配置 Prometheus 来监控这些 Pods.\nServiceMonitorSpec 中的 endpoints 部分, 是用来配置 Endpoints 被刮取指标数据时所需要的端口和参数. 对于一些高级用例, 要监控的后端 Pods 端口, 可能不是 service 中定义的. 因此在 endpoints 中定义 endpoint 时, 它必填.\n 注意: endpoints (小写) 是 ServiceMonitor 中的一个字段, 但 Endpoints (大写) 是 Kubernetes 中的对象.\n ServiceMonitors 和发现的 targets 一样都可以来自任何 namespace. 这对于跨 namespace 的监控是很重要的, 比如 meta-monitoring (元监控). 使用 PrometheusSpec/ServiceMonitorNamespaceSelector 字段, 可以限制选择 ServiceMonitors 的命名空间. 使用 ServiceMonitorSpec/namespaceSelector 字段, 可以限制发现 Endpoints 对象的命名空间. 要发现所有 namespaces 的 targets 需要把 namespaceSelector 设置为空:\nspec:namespaceSelector:any:truePodMonitor #  该资源定义了需要监控的一组动态的 pods. 它使用配置的标签选择要被监控的 pod. 它定义了一种指标暴露的方式, 使用这种方式, 新产生的 pod 就可以被自动发现, 不需要重新配置 prometheus.\nPod 是一个或多个容器的集合, 它可以在一个或多个端口上暴露 Prometheus 指标. PodMonitor 对象可以用来发现这些 pods, 并且为了监控他们, 会生成相应的 Prometheus 配置. PodMonitorSpec/PodMetricsEndpoints 定义了获取指标所用的端口和参数.\nPodMonitors 和发现的 targets 可以来自任何 namespaces. 这对于跨 namespace 的监控是很重要的, 比如 meta-monitoring (元监控). 使用 PodMonitorSpec/namespaceSelector, 可以限制发现 Pods 对象的命名空间. 要发现所有 namespaces 的 targets 需要把 namespaceSelector 设置为空:\nspec:namespaceSelector:any:trueProbe #  该资源允许以声明的方式定义如何监控的 ingresses 和静态 targets. 除了 target, 它还要求一个探针, 这个探针是一个监控服务, 能监控 target 而且还能给 Prometheus 提供指标. 例如, 可以通过 Blackbox Exporter 来实现.\nPrometheusRule #  该资源以声明方式定义一个或多个 Prometheus 实例使用的所需的 Prometheus 规则. 告警和记录规则可以以 YAML 的形式被保存和应用, 而且是热加载的, 不需要重启.\nAlertmanagerConfig #  该资源定义了 Alertmanager 配置文件的子配置, 可以配置告警的接受者以及抑制策略, 它是 namespace 级别的. 这里是个例子.\n 注意: 该资源目前还不是 stable\n "},{"id":38,"href":"/posts/2202/velero-backup-k8s/","title":"使用 velero 备份 kubernetes 指引","section":"文章","content":"要求 #   kubernetes 版本 1.7+，velero 的每个主版本对 kuberetes 的版本要求不同，详情请参考官方文档说明。 官方文档通道 velero 所在服务器有 kubectl 命令, 且能连上集群  我们先从最简单的体验开始\n1. 安装 velero 客户端 #  下载二进制安装包, 点击 latest release, 下载 velero-v1.7.0-linux-amd64.tag.gz (以 release 页面为准), 解压\ntar -xvf \u0026lt;RELEASE-TARBALL-NAME\u0026gt;.tar.gz 然后将二进制文件 velero 移动到 $PATH 中的一个目录, 如 /usr/local/bin\n2. 创建 credentials #  备份文件保存在对象存储中, 在当前目录下创建 credentials-velero 文件, 声明连接对象存储所用的账号密码\n[default] aws_access_key_id = \u0026lt;your key_id\u0026gt; aws_secret_access_key = \u0026lt;your secret\u0026gt; 3. 安装 velero 服务端 #  通过 velero 客户端在 kubernetes 中安装 deployment/velero，velero 提供了很多 stroage provider, 能将备份文件存储到比如 aws, aliyun-oss 中, 他们大都是支持 s3 接口的. 下面这个例子使用 s3 接口兼容的对象存储:\nBUCKET=\u0026lt;your bucket\u0026gt; REGION=\u0026lt;your region\u0026gt; S3URL=\u0026lt;your s3url\u0026gt; velero install \\  --provider aws \\  --plugins velero/velero-plugin-for-aws:v1.3.0 \\  --bucket $BUCKET \\  --secret-file ./credentials-velero \\  --use-volume-snapshots=false \\  --backup-location-config region=$REGION,s3ForcePathStyle=\u0026#34;true\u0026#34;,s3Url=$S3URL 4. 进行一次备份 #  velero backup create first-all-ns 查看所有的备份\nvelero backup get 5. 恢复指定的 namespace #  velero restore create --from-backup \u0026lt;backup name\u0026gt; \\ --include-namespaces \u0026lt;namespace name\u0026gt; \\ --restore-volumes=false \\  恢复时忽略存储卷，默认是true。根据自己实际情况配置。\n 6. 每日定时更新整个集群 #  velero schedule create all-daily --schedule=\u0026#34;@daily\u0026#34; 至此体验结束, 下面是一些拓展内容\n 拓展内容 #  备份文件存在哪里? #   BackupStorageLocations : 用来存储 kubernetes 原数据, 包括各种资源的配置清单等。这个命令可以看到上面安装 velero 时自动创建的 BackupStorageLocations 资源  kubectl -n velero get BackupStorageLocations  VolumeSnapshotLocation : 用来存储存储卷的数据。这个命令可以看到上面安装 velero 时自动创建的 VolumeSnapshotLocation 资源  kubectl -n velero get VolumeSnapshotLocation BackupStorageLocations #  首先, 创建后端存储使用的密钥文件，在 velero namespace 下创建对接 BackupStorageLocations 使用的 secret\nkubectl create secret generic -n velero credentials --from-file=bsl=\u0026lt;/path/to/credentialsfile\u0026gt; 这里创建一个叫 credentials 的 secret, 键: bsl, 值: \u0026lt;/path/to/credentialsfile\u0026gt;, 后面 velero 和 BackupStorageLocations 通讯时候就用这个 credentials，下面使用这个 secret 创建 BackupStorageLocations。\nvelero backup-location create \u0026lt;bsl-name\u0026gt; \\  --provider \u0026lt;provider\u0026gt; \\  --bucket \u0026lt;bucket\u0026gt; \\  --config region=\u0026lt;region\u0026gt; \\  --credential=\u0026lt;secret-name\u0026gt;=\u0026lt;key-within-secret\u0026gt; 下面这个命令可以查看新创建的 BackupStorageLocations 是否可以使用, 如果有 Avaliable 表示创建成功\nvelero backup-location get 当我们使用这个 BackupStorageLocations 进行备份的时候, 可以使用 --storage-location 标志, 如下\nvelero backup create --storage-location \u0026lt;bsl-name\u0026gt; 或者不使用 --storage-location \u0026lt;bsl-name\u0026gt; 标志, 直接将它设置为默认 BackupStorageLocations, 这样\nvelero backup-location set --default \u0026lt;bsl-name\u0026gt; 当然, 如果想更改 credetial, 可以重新创建一个 secret 然后使用下面命令更换 secret 即可,\nvelero backup-location set \u0026lt;bsl-name\u0026gt; \\  --credential=\u0026lt;secret-name\u0026gt;=\u0026lt;key-within-secret\u0026gt;  2021.12.22日补充 #  问题1: 我在使用 velero v1.1.0 备份一个经过二开的 kubernetes 集群， 发现每次执行 schedule 都会报错。\nlevel=error msg=\u0026#34;backup failed\u0026#34; controller=backup error=\u0026#34;rpc error: code = Unknown desc = EOF,...\u0026#34; logSource=\u0026#34;pkg/controller/backup_controller.go:233\u0026#34; google 发现有人遇到了这个问题，大概是内存不够导致通讯失败。 参考 issue，按照 @skriss 所说， 提高 deployment/velero limits.memory 问题就解决了， 我的配置是 1024Mi\n"},{"id":39,"href":"/posts/2309/k8s-log/","title":"Index","section":"文章","content":"Kubernetes相关日志方案 #  目前的方案 #  目前的方案简单概括来说是: fluentd(daemonset)+es+kibana. 使用fluentd在集群每个节点运行一个fluentd实例采集日志通过调用k8s接口为每个日志添加kubernetes和container相关的标签, 然后直接发送到es存储.\n痛点 #   es集群的维护成本较高, 比如索引的管理和优化, 集群规模的维护等. 需要技术和经验的加持, 故障处理效率不高. es日志需要多副本, 并对整个文档进行索引, 占用资源较多. fluentd直接推送到es, 中间少了流量缓冲.(fluentd自带缓冲区,该问题可有所缓解). fluentd配置复杂  下面的方案会针对当前的方案提出, 分析与当前方案的优劣比较:\n1 升级现有方案 #   节点代理升级为FluentBit, flb使用golang开发速度比flentd快, 而且资源占用也较低. 也已经从cncf毕业. 增加Kafka流量削峰, 虽然flentbit有缓冲机制, 再增加kafka能进一步增加故障/流量高峰缓冲的能力.  增加kafka之后, 推送到es是有消费者来完成的. 日志的流计算、解析、索引的控制不再依赖fluentbit, 相对自由.   优化索引机制, 通过配置可以实现:  按照集群每日一个索引(当前策略) 按照租户每日一个索引    2 promtail+loki+grafana #  loki vs es #  二者最大的不同是索引, loki被设计成使用最小的索引, 但是es是全文索引, 而且每个字段都有数据结构. 所以, loki有好的性能和资源消耗. 但是富文本搜索的能力不及es.\nloki存储可以使用的后端存储有很多, 比如本地文件系统或者使用对象存储(s3兼容), 这也是它的优势, 更适合云环境, 这将进一步降低成本.\n查询语言 #  kibana自己有个KQL语言或者也可以使用lucene查询语法, 这个使用的时间比较长了, 网上有很多查询例子.\nloki使用logQL查询日志, 主要包含两个部分, 一个是标签选择器, 一个是日志过滤表达式. 查询性能与标签选择有关系, 标签选择的日志量大查询时间就长. 语法上相比上面两个相对简单. 这里是一个例子:\n{container=\u0026quot;query-frontend\u0026quot;,namespace=\u0026quot;loki-dev\u0026quot;} |= \u0026quot;metrics.go\u0026quot; | logfmt | duration \u0026gt; 10s and throughput_mb \u0026lt; 500\n{}中的是标签选择器, 选择loki-dev/query-frontend容器中的日志. 后面部分是过滤器, 过滤包含metrics.go关键词的日志, 并将其按照logfmt格式化, 并按照格式化之后的日志条件筛选.\n节点代理 promtail vs fluentd #  promtail与fluentd一样也是在节点上收集日志, 主要三个步骤: 发现目标文件、给日志附加标签、把日志推送到loki. fluentd/fluentbit相较promtail功能丰富一些. promtail和fluentbit使用golang开发, fluentd使用ruby开发, 相较fluentd资源占用较小性能更高.\n同时可见fluentbit可以作为fluentd的替代.\n用户交互 grafna vs kibana #  因为loki也是grafana labs公司的产品, grafana支持日志的查询, 相比kibana都差不多, 毕竟grafana之前只是kibana的一个分支.\ngrafana log\n kibana log\n 特点和优势 #  loki仅会索引标签和元数据, 日志数据被压缩为chuck存储(类似s3). 因为es支持全文索引, es会索引整个文档, 日志以json形式存储在磁盘中, 比loki占用更多的存储.\n缺点 #   因为只是对元数据和标签索引, loki虽然占用资源比es少, 但是查询能力不及es.  总结 #  loki的出现比较契合k8s pod日志存储查询的使用场景. 占用更少的存储空间, 虽然查询能力比es差, 但能满足pod日志查询的使用场景. es虽然在维护和资源消耗上处于劣势, 但是它对查询能力、查询速度、统计、分析上的支持还是要比loki好的.\n这是其他人对两个方案测试的结果, 可以参考:\n引用自: https://crashlaker.medium.com/which-logging-solution-4b96ad3e8d21\n 可以看到在摄取数据、磁盘占用上loki有一定优势, 但是在查询/分析上(里面涉及日志某系字段的计算, 比如nginx状态码、延迟)loki却逊色不少.\n参考 #  写本方案阅读了一些资料文献:\n Loki vs Elasticsearch - Which tool to choose for Log Analytics?: https://signoz.io/blog/loki-vs-elasticsearch/ Comparing Logging Solutions: https://crashlaker.medium.com/which-logging-solution-4b96ad3e8d21 日志分析下ES/ClickHouse/Loki比较与思考: https://zhuanlan.zhihu.com/p/396211457 Loki Filesystem Proc/Cons: https://grafana.com/docs/loki/latest/operations/storage/filesystem/  W3: 进一步测试并细化Loki方案 #  本文主要是从\n 维护性 高可用 使用体验 性能上 磁盘存储 几个方面展开. 并没有关注聚合计算、复杂的查询等方面.  高可用架构 #   Monolithic mode #  Monolithic mode is useful for getting started quickly to experiment with Loki, as well as for small read/write volumes of up to approximately 100GB per day.\n"}]