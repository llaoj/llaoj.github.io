---
layout: page
title: "读懂ceph"
categories: diary
---

#### Ceph 提供三种存储：

- 块存储

也叫 RBD, 服务提供了大小可调、精炼、支持快照和克隆的块设备。为提供高性能， Ceph 把块设备条带化到整个集群。 Ceph 同时支持内核对象（ KO ） 和 QEMU 管理程序直接使用`librbd` ——避免了内核对象在虚拟系统上的开销

- 文件存储

 CephFS, 服务提供了兼容 POSIX 的文件系统，可以直接 mount 或挂载为用户空间文件系统（ FUSE ）

- 对象存储

也叫 RGW, 服务提供了 `RESTful 风格` 的 API, 它与 Amazon S3 和 OpenStack Swift 兼容

> Ceph uniquely delivers object, block, and file storage in one unified system.

#### Ceph 集群有两种守护进程
1. Ceph Monitor

维护着集群运行图的主副本。一个监视器集群确保了当某个监视器失效时的高可用性。存储集群客户端向 Ceph 监视器索取集群运行图的最新副本。

2. Ceph OSD 守护进程

检查自身状态、以及其它 OSD 的状态，并报告给监视器们。


#### 数据存储

1. 数据来源

来自 Ceph 块设备, Ceph 对象存储, Ceph 文件系统, 还有基于 librados 的自定义实现

2. 处理过程

Ceph 从客户端接收数据并存储为**对象**. 每个对象是文件系统中的一个文件，它们存储在对象存储设备上。由 Ceph OSD 守护进程处理存储设备上的读/写操作。

#### 存储对象格式

1. Ceph OSD 在扁平的命名空间内把所有数据存储为对象: **没有目录层次**
2. 对象包含一个标识符 ID, 二进制数据 Binary Data, 和由名字/值对组成的元数据 Metadata
3. 对象 ID 不止在本地&集群内都是唯一的
4. 元数据语义完全取决于 Ceph 客户端. 如: CephFS 用元数据存储文件属性，如文件所有者、创建日期、最后修改日期等等

#### 去中心化：CRUSH

存储集群的客户端和各个 Ceph OSD 守护进程使用 **CRUSH 算法**高效地计算数据位置，而不是依赖于一个中心化的查询表。它的高级功能包括：基于 librados的原生存储接口、和多种基于 librados 的服务接口


